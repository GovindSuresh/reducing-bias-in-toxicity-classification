<!DOCTYPE html>
<html>
<head>
    <title> Greeting app | Welcome </title>
    <meta name="viewport" content="width=device-width">
    <link rel="stylesheet" href='css/style.css'/>
    <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        * { 
            font-size:30px;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div id="proj">
                <h1>Reducing Bias in Toxic Comment Classification</h1>
            </div>
            
        </div>
    </header>
    
    <section id="Project Overview">
        <div class="container">
            <h2>Problem Space</h2>
            <p>
                    As Machine Learning is increasingly used in our day to day lives, 
                    issues surrounding the reinforcement of existing biases against minority identities are increasingly important. 
                    We are exploring this from the scope of online toxic comment classification
            </p>
            <p> 
                <ul>
                    <li> Classic ML classification models, such as Logistic, perform very well in terms of accuracy</li>
                    <li> However they have a tendency to over-weight words that refer to identites, particulary those of
                        minorities (Black, Latino, LGBT, e.t.c).
                    </li>
                    <li> As a result comments involving these groups tend to be more likely to be classified as toxic, even
                        when the content and context of the comment would suggest otherwise.
                </ul>       
            </p>
            <h2>Our Solution</h2>
            <p>          
                Our aim is to train a neural network model, primarily an LSTM model, to classify toxic comments. 
                <ul>
                    <li>
                        RNN's such as LSTM are optimally suited to tackling this problem due to their ability
                        to remember information contained at different parts of a sequence
                    </li>
                    <li>
                        Our expectation is that the LSTM model will be able to decipher the underlying <span style="font-weight:bold; font-size:19px">context</span>
                        of a comment as opposed to just weighting individual words.
                    </li>
                </ul>
                
            </p> 
            <p>       
                    We also trained a handful of classic models, namely: Logistic, XGBoost, and Random Forest, to allow us to compare results
            </p>
        </div>
    </section>

    <section id="lime-viz">
        <div class="container">
            <h2>Visualising the issues with classic ML models</h2>
            <p>
                To highlight the bias issue with classic models we have run our Logistic Regression model through the ELI5 package.
            </p>
            <img src="img/missclass-lgbt.png" width="900" height="250" style="padding: 10px">
            <figcaption> A misclassified non-toxic comment post standard NLP processing and Logistic regression </figcaption>
            <p>
                Our model has highlighted the word homosexual repeatedly as a key contribution to its classification of Toxic.
                The original text mentions how the commenter felt the condemnation of young homosexual men was  
            
            </p>
        </div>
    </section>
    
    <section id="Model-Prediction">
        <div class="container">
            <h2>Try our LSTM Classifier for yourself!</h2>
            
            <input id="comment-input" type="text"/>
            <button id="predict-button" type="button">Submit</button>
            
            <p>Non-Toxic: <span id="Non-Toxic-prediction"></span></p>
            <p>Toxic: <span id="Toxic-prediction"></span></p>

            <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
            <script>
                $("#predict-button").click(function(event){
                    let message = {
                         comment: $("#comment-input").val()
                    }
                    console.log(message)
                    $.post("http://0.0.0.0:5000/predict", JSON.stringify(message), function(response){
                        $("#Non-Toxic-prediction").text(response.prediction.non_toxic.toFixed(3));
                        $("#Toxic-prediction").text(response.prediction.toxic.toFixed(3));
                        console.log(response);
                    });
                });    
            </script>
        </div>
    </section>
    <section>
        <div class="container">
            <h2> How do we measure bias?</h2>
            <p>
            For this problem we are using a specialized bias metric developed by <a href =https://arxiv.org/pdf/1903.04561.pdf>Jigsaw AI</a> 
            the metric effectively calculates false positive and false negative rates against specific identity subgroups and then calculates
            a weighted mean of the AUCs for each subgroup. The exact calculations can be found <a href=https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation> here.</a>
            </p>
            <p>
            As a summary we follow these steps:
            </p>
            <ul>
                <li>
                The dataset comes labelled against comments that mention a specific subgroup identity (black, latino, LGBT e.tc.)
                We split the predicted results for each subgroup and calculate the false positve and false negative rates. In other words
                we calculate the subgroup AUC.
                <li>
                We then calculate a generalized mean of the subgroup AUCs
                </li>
                <li>
                Finally we combine this mean with the overall AUC to give the final metric. Effectivley a low score here means that while the model
                may have been good at classifying toxic comments in general. It had a tendency to overweight comments mentioning a particular identity or
                underweight comments mentioning that identity. A strong model with minimised bias is one that can understand the context around words that refer to a particular identity.
                </li>
            </ul>


        </div>
    </section>
    <Section id="Results">
        <div class="container">
            <h2> Results </h2>
            <img src="img/results-table.png" width="550" height="180" class="center">
            <p>
                <ul>
                    <li>
                        As you can see, all the models performed well from a pure accuracy standpoint, however there were issues
                        with the other metrics, particularly recall. This is primarily due to a large class imbalance in the dataset.
                        We will explore the impact of addressing this imbalance on model performance later
                    </li>
                    <li>
                        Importantly from a final bias metric point of view. Our LSTM model performed appreciably better than the other models 
                        with a score of 0.920. This means the LSTM was able to avoid issues of miss-classifying non-toxic comments involving
                        the specified subgroups while also still capturing the toxic examples involving them. 
                    </li>
                    <li>
                        We therefore view this as a step in the right direction in building less biased toxic comment classifiers. There remains 
                        many further avenues to explore with the neural network architecture, but this is a promising first step!
                    </li>
                    </ul>
            </p>
        </div>
    </Section>
    <Section>
        <div class="container">
            <h2> Future Avenues of Exploration</h2>
            <p>
                <ul>
                    <li>
                        As mentioned above, we would like to assess the impact of addressing the class imbalance on model performance. Given the absolute
                        disparity between Toxic and Non-Toxic cases, this will have to be implemented carefully as simple over or undersampling would significantly
                        change the make-up of the dataset and lead to over/underfitting issues.
                    </li>
                    <li>
                        We would also like to explore further enhancements to our neural network. Due to the computing requirement to train the network, we were unable
                        to carry out detailed hyperparameter optimziation or the impact of adding additional layers or further dropout.
                    </li>
                </ul>
            </p>
        </div>
    </Section>
</body>
</html>