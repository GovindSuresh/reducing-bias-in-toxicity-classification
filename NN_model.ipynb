{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, MaxPooling1D, Input, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 200000 # there are 563693 words in the vocabulary\n",
    "MAX_LEN_SEQ = 300\n",
    "TRAIN_TEXT_COL = 'comment_text_clean2'\n",
    "TEST_TEXT_COL = 'comment_text_clean2'\n",
    "TRAIN_TARGET_COL = 'target'\n",
    "EMBED_DIM = 300\n",
    "EMBEDDING_FILE = 'embeds/glove.840B.300d.txt'\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "LSTM_UNITS = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 4\n",
    "CHECKPOINT_PATH = \"NN_models/cp.ckpt\"\n",
    "CHECKPOINT_DIR = os.path.dirname(CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "train_data = pd.read_csv('data/train_for_nn.csv', converters={\"comment_text_clean2\": literal_eval})\n",
    "test_data = pd.read_csv('data/test_for_nn.csv', converters={\"comment_text_clean2\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train val split, stratify on target\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.2, stratify=train_data['target'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fix tokenizer\n",
    "def train_tokenizer(train_data, vocab_size):\n",
    "    # Use Keras tokenizer to create vocabulary dictionary \n",
    "    # default arguments will filter punctuation and convert to lower, we do not want this given our use \n",
    "    # of pre-trained word embeddings\n",
    "    tokenizer = text.Tokenizer(num_words = vocab_size, filters='', lower=False)\n",
    "    tokenizer.fit_on_texts(train_data)\n",
    "    return tokenizer\n",
    "\n",
    "# pad tokenized sequences\n",
    "def text_padder(text, tokenizer):\n",
    "    return sequence.pad_sequences(tokenizer.texts_to_sequences(text), maxlen=MAX_LEN_SEQ)\n",
    "\n",
    "# Build embedding matrix\n",
    "def build_embedding_matrix(word_indexes, EMBEDDING_FILE):\n",
    "  \n",
    "    # Used to store words as key and vectors as value\n",
    "    embedding_dict = {}\n",
    "    with open(EMBEDDING_FILE) as file:\n",
    "        # file is formatted word {whitespace} vector\n",
    "        for line in file:\n",
    "            pairs = line.split(' ')\n",
    "           # word is 0 index of pairs\n",
    "            word = pairs[0]\n",
    "            vec = pairs[1:]\n",
    "           #convert vec into a numpy array\n",
    "            vec = np.asarray(vec, dtype=np.float32)\n",
    "            embedding_dict[word] = vec\n",
    "    \n",
    "    #create the embedding matrix which has dimensions:\n",
    "    # MAX_VOCAB_SIZE +1 for rows, this means there will be as many rows as words we allow to be part of the feature set.\n",
    "    # EMBED_DIM is the number of columns, this reflects the dimensions of the word embedding vectors we are using.\n",
    "    embedding_matrix = np.zeros((len(word_indexes)+1, EMBED_DIM))\n",
    "\n",
    "\n",
    "    word_count = 0\n",
    "    for word, i in word_indexes.items():\n",
    "        # gets the vector to the corresponding word from the previous dictionary and sets it to the variable\n",
    "        embedding_vector = embedding_dict.get(word)\n",
    "        # We check whether the embedding_vector is not none (i.e the word is in the embedding index)\n",
    "        if embedding_vector is not None:\n",
    "            word_count += 1\n",
    "            # Append the embedding vector to index i in the embedding matrix \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "            \n",
    "def build_embedding_matrix_restricted(word_indexes, EMBEDDING_FILE):\n",
    "  \n",
    "    # Used to store words as key and vectors as value\n",
    "    embedding_dict = {}\n",
    "    with open(EMBEDDING_FILE) as file:\n",
    "        # file is formatted word {whitespace} vector\n",
    "        for line in file:\n",
    "            pairs = line.split(' ')\n",
    "           # word is 0 index of pairs\n",
    "            word = pairs[0]\n",
    "            vec = pairs[1:]\n",
    "           #convert vec into a numpy array\n",
    "            vec = np.asarray(vec, dtype=np.float32)\n",
    "            embedding_dict[word] = vec\n",
    "    \n",
    "    #create the embedding matrix which has dimensions:\n",
    "    # MAX_VOCAB_SIZE +1 for rows, this means there will be as many rows as words we allow to be part of the feature set.\n",
    "    # EMBED_DIM is the number of columns, this reflects the dimensions of the word embedding vectors we are using.\n",
    "    embedding_matrix = np.zeros((MAX_VOCAB_SIZE+1, EMBED_DIM))\n",
    "\n",
    "\n",
    "    word_count = 0\n",
    "    for word, i in word_indexes.items():\n",
    "        # gets the vector to the corresponding word from the previous dictionary and sets it to the variable\n",
    "        embedding_vector = embedding_dict.get(word)\n",
    "        # We check whether the embedding_vector is not none (i.e the word is in the embedding index)\n",
    "        if embedding_vector is not None:\n",
    "            word_count += 1\n",
    "            # Append the embedding vector to index i in the embedding matrix \n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = train_tokenizer(train_df[TRAIN_TEXT_COL], MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494877"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "# NOTE: WITH TF2.0 CUDNNLSTM is active by default when there is a GPU available but you must use the default settings.\n",
    "# SEE https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM for more details\n",
    "\n",
    "def build_model(embedding_matrix):\n",
    "    # change to max word length \n",
    "    input_words = Input(shape=(MAX_LEN_SEQ,), dtype='int32')\n",
    "    embedding = Embedding(len(tokenizer.word_index)+1, EMBED_DIM,\n",
    "                          weights=[embedding_matrix],\n",
    "                          input_length = MAX_LEN_SEQ,\n",
    "                          #mask_zero = True\n",
    "                          trainable = False) (input_words)\n",
    "    x = Dropout(DROPOUT_RATE)(embedding)\n",
    "    x = Bidirectional(LSTM(128, activation='tanh', return_sequences=True))(x) #set return_sequence to false when passing to dense\n",
    "    #x = Bidirectional(LSTM(128, activation='tanh', return_sequences=True))(x)\n",
    "    \n",
    "    # Use GlobalMaxPooling\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    # Pass into DENSE layers \n",
    "    # Dense nodes total has been calculated as per \n",
    "    # https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
    "    # (300,000)/5*(128+2) = 462\n",
    "    x = Dense(462, activation='relu')(x)\n",
    "    prediction = Dense(2, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_words, outputs=prediction, name='baseline-LSTM')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', AUC()])\n",
    "    \n",
    "    return model\n",
    "                           \n",
    "def train_model(train_df, val_df, tokenizer):\n",
    "    # Create processed and padded train and targets\n",
    "    print('padding_text')\n",
    "    X_train = text_padder(train_df[TRAIN_TEXT_COL], tokenizer)\n",
    "    X_val = text_padder(val_df[TRAIN_TEXT_COL], tokenizer)\n",
    "    y_train = to_categorical(train_df[TRAIN_TARGET_COL])\n",
    "    y_val = to_categorical(val_df[TRAIN_TARGET_COL])\n",
    "    \n",
    "    print('building embedding matrix')\n",
    "    # build embedding matrix\n",
    "    embed_matrix = build_embedding_matrix(tokenizer.word_index, EMBEDDING_FILE)\n",
    "    \n",
    "    # build model\n",
    "    print('building model')\n",
    "    model = build_model(embed_mat)\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_PATH,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "    \n",
    "    # Connect to tensorboard\n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, write_image=True, write_graph=False\n",
    "                                                          write_grad=True)\n",
    "    # train model\n",
    "    print('training model')\n",
    "    fitted_model = model.fit(X_train, y_train,\n",
    "                             batch_size = BATCH_SIZE,\n",
    "                             epochs = NUM_EPOCHS,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             callbacks=[cp_callback, tensorboard_callback],\n",
    "                             verbose = 1)\n",
    "    #save full model \n",
    "    fitted_model.save('saved_model/baseline-LSTM') \n",
    "    #saves to h5\n",
    "    fitted_model.save('saved_model/baseline-LSTM.h5')\n",
    "    \n",
    "    #save weights\n",
    "    fitted_model.save_weights('saved_weights/baseline-LSTM')\n",
    "    fitted_model.save_weights('saved_weights/baseline-LSTM.h5')\n",
    "  \n",
    "    return fitted_model\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_vocab_restricted(embedding_matrix):\n",
    "    # change to max word length \n",
    "    input_words = Input(shape=(MAX_LEN_SEQ,), dtype='int32')\n",
    "    embedding = Embedding(MAX_VOCAB_SIZE+1, EMBED_DIM,\n",
    "                          weights=[embedding_matrix],\n",
    "                          input_length = MAX_LEN_SEQ,\n",
    "                          #mask_zero = True\n",
    "                          trainable = False) (input_words)\n",
    "    x = Dropout(DROPOUT_RATE)(embedding)\n",
    "    x = Bidirectional(LSTM(128, activation='tanh', return_sequences=True))(x) #set return_sequence to false when passing to dense\n",
    "    #x = Bidirectional(LSTM(128, activation='tanh', return_sequences=True))(x)\n",
    "    \n",
    "    # Use GlobalMaxPooling\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    # Pass into DENSE layers \n",
    "    # Dense nodes total has been calculated as per \n",
    "    # https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
    "    # (300,000)/5*(128+2) = 462\n",
    "    x = Dense(462, activation='relu')(x)\n",
    "    prediction = Dense(2, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_words, outputs=prediction, name='baseline-LSTM')\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', AUC()])\n",
    "    \n",
    "    return model\n",
    "                           \n",
    "def train_model_restricted(train_df, val_df, tokenizer):\n",
    "    # Create processed and padded train and targets\n",
    "    print('padding_text')\n",
    "    X_train = text_padder(train_df[TRAIN_TEXT_COL], tokenizer)\n",
    "    X_val = text_padder(val_df[TRAIN_TEXT_COL], tokenizer)\n",
    "    y_train = to_categorical(train_df[TRAIN_TARGET_COL])\n",
    "    y_val = to_categorical(val_df[TRAIN_TARGET_COL])\n",
    "    \n",
    "    print('building embedding matrix')\n",
    "    # build embedding matrix\n",
    "    embed_matrix = build_embedding_matrix_restricted(tokenizer.word_index, EMBEDDING_FILE)\n",
    "    \n",
    "    # build model\n",
    "    print('building model')\n",
    "    model = build_model(embed_mat)\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_PATH,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "    \n",
    "    # Connect to tensorboard\n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, write_image=True, write_graph=False\n",
    "                                                          write_grad=True)\n",
    "    # train model\n",
    "    print('training model')\n",
    "    fitted_model = model.fit(X_train, y_train,\n",
    "                             batch_size = BATCH_SIZE,\n",
    "                             epochs = NUM_EPOCHS,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             callbacks=[cp_callback, tensorboard_callback],\n",
    "                             verbose = 1)\n",
    "    \n",
    "    #save full model \n",
    "    fitted_model.save('saved_model/baseline-LSTM') \n",
    "    #saves to h5\n",
    "    fitted_model.save('saved_model/baseline-LSTM.h5')\n",
    "    \n",
    "    #save weights\n",
    "    fitted_model.save_weights('saved_weights/baseline-LSTM')\n",
    "    fitted_model.save_weights('saved_weights/baseline-LSTM.h5')\n",
    "    \n",
    "    return fitted_model\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
