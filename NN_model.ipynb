{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model\n",
    "\n",
    "In this notebook we have specified the build for our LSTM model.\n",
    "We have used tensorflow 2.0. If you are training the model, it is highly recommended to use a GPU and have at least ~32gb of RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general modules\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit learn modules\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow and keras modules. NOTE: WE ARE USING TENSORFLOW 2.0\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, MaxPooling1D, Input, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have defined the various parameters that will be used in the model,\n",
    "\n",
    "# General set-up parameters\n",
    "TRAIN_TEXT_COL = 'comment_text_clean2'\n",
    "TEST_TEXT_COL = 'comment_text_clean2'\n",
    "TRAIN_TARGET_COL = 'target'\n",
    "TEST_TARGET_COL = 'target'\n",
    "EMBEDDING_FILE = 'embeds/glove.840B.300d.txt' #change this if you are using a different embedding file\n",
    "\n",
    "# Model hyperparameters - the first 3 refer more to the data itself. Reducing the vocab size and max sequence length can\n",
    "# have negative impact on accuracy, but will likely lead to a model that trains faster and is less memory intensive. \n",
    "# There are similar effects to using lower dimension word embeddings. Changing the embedding dim requires you to have\n",
    "# the specific word-embedding file available. \n",
    "\n",
    "MAX_VOCAB_SIZE = 200000 # there are 563693 words in the vocabulary \n",
    "MAX_LEN_SEQ = 300\n",
    "EMBED_DIM = 300 #change this if you have chose different embedding dimensions\n",
    "DROPOUT_RATE = 0.2\n",
    "LSTM_UNITS = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "# Location to save training checkpoints\n",
    "CHECKPOINT_PATH = \"NN_models/cp.ckpt\"\n",
    "CHECKPOINT_DIR = os.path.dirname(CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data from S3 for cloud computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket, dataset_file_path_train, 'train_for_nn.csv')\n",
    "s3.download_file(bucket, dataset_file_path_test, 'test_for_nn.csv')\n",
    "\n",
    "# When the data set was saved as a CSV, tokenized column, which was a list was coverted to a string, \n",
    "# The converters option changes this back into its list form \n",
    "train_data = pd.read_csv('train_for_nn.csv', converters={\"comment_text_clean2\": literal_eval})\n",
    "test_data = pd.read_csv('test_for_nn.csv', converters={\"comment_text_clean2\": literal_eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data from local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train_for_nn.csv', converters={\"comment_text_clean2\": literal_eval})\n",
    "test_data = pd.read_csv('data/test_for_nn.csv', converters={\"comment_text_clean2\": literal_eval})#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnamed col \n",
    "train_data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "test_data.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train val split, stratify on target\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.2, stratify=train_data['target'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save down the train_df - we will need this in the future to fit the tokenizer to\n",
    "train_df.to_csv('data/nn_tokenizer_fit_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Methods\n",
    "Below are the various methods required to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fix tokenizer\n",
    "def train_tokenizer(train_data, vocab_size):\n",
    "    '''\n",
    "    Function to train the Keras tokenizer to create a vocabulary dictionary\n",
    "    \n",
    "    INPUT:\n",
    "    train_data - training data column\n",
    "    int vocab_size - When text_to_sequences is run, any word with index above this will be set to 0\n",
    "    \n",
    "    OUTPUT:\n",
    "    A fitted keras tokenizer\n",
    "    '''\n",
    "    # Use Keras tokenizer to create vocabulary dictionary \n",
    "    # default arguments will filter punctuation and convert to lower, we do not want this given our use \n",
    "    # of pre-trained word embeddings\n",
    "    tokenizer = text.Tokenizer(num_words = vocab_size, filters='', lower=False)\n",
    "    tokenizer.fit_on_texts(train_data)\n",
    "    return tokenizer\n",
    "\n",
    "# pad tokenized sequences\n",
    "def text_padder(text, tokenizer):\n",
    "    '''\n",
    "    Function to convert sequences to a sequence of word-indexes and also to pad these sequences to the max sequence length\n",
    "    \n",
    "    INPUT:\n",
    "    text - text to process\n",
    "    tokenizer - trained keras tokenizer.\n",
    "    \n",
    "    OUTPUT:\n",
    "    Returns a padded sequence of length MAX_LEN_SEQ\n",
    "    '''\n",
    "    \n",
    "    return sequence.pad_sequences(tokenizer.texts_to_sequences(text), maxlen=MAX_LEN_SEQ)\n",
    "\n",
    "# Build embedding matrix\n",
    "def build_embedding_matrix(word_indexes, EMBEDDING_FILE):\n",
    "    '''\n",
    "    Function to create the word-embedding matrix to feed into the model input layer\n",
    "    \n",
    "    INPUT:\n",
    "    text - word-indexes from keras tokenizer\n",
    "    EMBEDDING_FILE - path to embedding file\n",
    "    \n",
    "    OUTPUT:\n",
    "    numpy array with word indexes and embedding values\n",
    "    '''\n",
    "    # Used to store words as key and vectors as value\n",
    "    embedding_dict = {}\n",
    "    # read in embedding file\n",
    "    with open(EMBEDDING_FILE) as file:\n",
    "        # file is formatted word {whitespace} vector\n",
    "        for line in file:\n",
    "            pairs = line.split(' ')\n",
    "           # word is 0 index of pairs\n",
    "            word = pairs[0]\n",
    "            vec = pairs[1:]\n",
    "           #convert vec into a numpy array\n",
    "            vec = np.asarray(vec, dtype=np.float32)\n",
    "            embedding_dict[word] = vec\n",
    "    \n",
    "    #create the embedding matrix which has dimensions:\n",
    "    # MAX_VOCAB_SIZE +1 for rows, this means there will be as many rows as words we allow to be part of the feature set.\n",
    "    # EMBED_DIM is the number of columns, this reflects the dimensions of the word embedding vectors we are using.\n",
    "    embedding_matrix = np.zeros((len(word_indexes)+1, EMBED_DIM))\n",
    "\n",
    "\n",
    "    word_count = 0\n",
    "    for word, i in word_indexes.items():\n",
    "        \n",
    "        # checks if word index is outide max vocab size. If true we just continue\n",
    "        if i >= MAX_VOCAB_SIZE:\n",
    "            continue\n",
    "        \n",
    "        # gets the vector to the corresponding word from the previous dictionary and sets it to the variable\n",
    "        embedding_vector = embedding_dict.get(word)\n",
    "        # We check whether the embedding_vector is not none (i.e the word is in the embedding index)\n",
    "        if embedding_vector is not None:\n",
    "            word_count += 1\n",
    "            # Append the embedding vector to index i in the embedding matrix. If word is not in embeddings these will be 0.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "\n",
    "# NOTE: WITH TF2.0 CUDNNLSTM is active by default when there is a GPU available but you must use the default settings.\n",
    "# SEE https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM for more details\n",
    "\n",
    "def build_model(embedding_matrix):\n",
    "    '''\n",
    "    Function to build out the model structure. This function can be changed to edit the model architecture or a new\n",
    "    function can be created to seperate these. \n",
    "    \n",
    "    INPUT:\n",
    "    embedding_matrix - The embedding_matrix created by the build_embedding_matrix function\n",
    "    \n",
    "    OUTPUT:\n",
    "    tensorflow model\n",
    "    '''\n",
    "    \n",
    "    # Input layer, this has shape max_len_seq, which is the length all sequences will be padded to. \n",
    "    input_words = Input(shape=(MAX_LEN_SEQ,), dtype='int32')\n",
    "    \n",
    "    # Embedding layer. This is fixed layer with the weights being what we have stored in the embedding matrix\n",
    "    embedding = Embedding(len(tokenizer.word_index)+1, EMBED_DIM,\n",
    "                          weights=[embedding_matrix],\n",
    "                          input_length = MAX_LEN_SEQ,\n",
    "                          #mask_zero = True\n",
    "                          trainable = False) (input_words)\n",
    "    \n",
    "    # Dropout layer to reduce overfitting\n",
    "    x = Dropout(DROPOUT_RATE)(embedding)\n",
    "    \n",
    "    # Bidirectional LSTM layer. We go over each sequence forwards and backwards to extract as much\n",
    "    # contextual information as possible\n",
    "    # Note with TF2.0 to enable GPU training we cannot change certain paraemters such as the activation function.\n",
    "    # Second layer has been commented out for now\n",
    "    x = Bidirectional(LSTM(128, activation='tanh', return_sequences=True))(x) \n",
    "    #x = Bidirectional(LSTM(128, activation='tanh', return_sequences=True))(x)\n",
    "    \n",
    "    # Use GlobalMaxPooling\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    # Pass into DENSE layers \n",
    "    # Dense nodes total has been calculated as per \n",
    "    # https://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm\n",
    "    # (300,000)/5*(128+2) = 462\n",
    "    x = Dense(462, activation='relu')(x)\n",
    "    \n",
    "    # Final output layer using sigmoid for binary classification\n",
    "    prediction = Dense(2, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_words, outputs=prediction, name='baseline-LSTM')\n",
    "    # we use binary_crossentropy loss function given this is a binary classification problem\n",
    "    # adam has been selected given general performance\n",
    "    # We check both the classification accuracy and the AUC of the model.\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', AUC()])\n",
    "    \n",
    "    return model\n",
    "                           \n",
    "def train_model(train_df, val_df, tokenizer):\n",
    "    '''\n",
    "    Function to train a tensorflow model. This function runs a number of the previous functions\n",
    "    \n",
    "    INPUT:\n",
    "    train_df - training data\n",
    "    val_df - validation data\n",
    "    tokenizer - fitted keras tokenizer\n",
    "    \n",
    "    OUTPUT:\n",
    "    model - tensorflow model\n",
    "    fitted_model - fit history\n",
    "    '''\n",
    "    # Create processed and padded train and targets\n",
    "    print('padding_text')\n",
    "    X_train = text_padder(train_df[TRAIN_TEXT_COL], tokenizer)\n",
    "    X_val = text_padder(val_df[TRAIN_TEXT_COL], tokenizer)\n",
    "    y_train = to_categorical(train_df[TRAIN_TARGET_COL])\n",
    "    y_val = to_categorical(val_df[TRAIN_TARGET_COL])\n",
    "    \n",
    "    print('building embedding matrix')\n",
    "    # build embedding matrix\n",
    "    embed_matrix = build_embedding_matrix(tokenizer.word_index, EMBEDDING_FILE)\n",
    "    \n",
    "    # build model\n",
    "    print('building model')\n",
    "    model = build_model(embed_matrix)\n",
    "    \n",
    "    # set up checkpoint callbacks\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CHECKPOINT_PATH,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "    \n",
    "    # Connect to tensorboard\n",
    "    #logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, write_images=True, \n",
    "                                                          #write_graph=False\n",
    "                                                          #)\n",
    "   \n",
    "    # train model batch size and epochs were set up earlier.\n",
    "    print('training model')\n",
    "    fitted_model = model.fit(X_train, y_train,\n",
    "                             batch_size = BATCH_SIZE,\n",
    "                             epochs = NUM_EPOCHS,\n",
    "                             validation_data=(X_val, y_val),\n",
    "                             callbacks=[cp_callback],\n",
    "                             verbose = 1)\n",
    "\n",
    "    return model, fitted_model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 s, sys: 894 ms, total: 48.4 s\n",
      "Wall time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create fitted tokenizer\n",
    "tokenizer = train_tokenizer(train_df[TRAIN_TEXT_COL], MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check tensorflow can find GPU\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model training function\n",
    "model, fitted_model = train_model(train_df, val_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save full model  using TF saved model format\n",
    "model.save('NN_model/saved_model/baseline-LSTM') \n",
    "\n",
    "#saves full model using h5 format\n",
    "model.save('NN_model/saved_model_h5/baseline-LSTM.h5')\n",
    "    \n",
    "#save weights\n",
    "model.save_weights('NN_model/saved_weights/baseline-LSTM')\n",
    "model.save_weights('NN_model/saved_weights_h5/baseline-LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass trained tokenizer to convert test results to sequences\n",
    "X_test = text_padder(test_data[TEST_TEXT_COL], tokenizer)\n",
    "\n",
    "#convert target col to categorical \n",
    "y_test = to_categorical(test_data[TEST_TARGET_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set \n",
    "test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to add test set predictions\n",
    "test_pred_results = pd.DataFrame(test_data['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns to add the test predictions\n",
    "test_pred_results['prediction_prob_0'] = test_preds[:,0]\n",
    "test_pred_results['prediction_prob_1'] = test_preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv\n",
    "test_pred_results.to_csv('data/LSTM_pred_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: To predict new values, the string will have to be passed through the pre-processing functions (i.e text_padder()). In addition a tokenizer trained on the train data will need to be created**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in test predictions\n",
    "test_preds = pd.read_csv('data/LSTM_pred_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnamed column\n",
    "test_preds.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the predictions onto the test dataframe on id\n",
    "test_results = test_data.merge(test_preds, how='inner', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define identity columns\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "# convert identity and target columns to boolean\n",
    "for col in identity_columns + ['target']:\n",
    "    test_results[col] = np.where(test_results[col] >= 0.5, True, False)\n",
    "    \n",
    "# create a binary col for prediction of class 1 (toxic)\n",
    "test_results['prediction_binary'] = np.where(test_results['prediction_prob_1'] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.98      0.97    179192\n",
      "        True       0.75      0.63      0.68     15448\n",
      "\n",
      "    accuracy                           0.95    194640\n",
      "   macro avg       0.86      0.80      0.83    194640\n",
      "weighted avg       0.95      0.95      0.95    194640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store the precision, recall, and f1 score for later and print the classification report\n",
    "nn_precision = precision_score(test_results['target'], test_results['prediction_binary'])\n",
    "nn_recall = recall_score(test_results['target'], test_results['prediction_binary'])\n",
    "nn_f1 = f1_score(test_results['target'], test_results['prediction_binary'])\n",
    "\n",
    "print(classification_report(test_results['target'], test_results['prediction_binary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is very strong at predicting the false class, however not as adept at the cases where toxicity is the case. A recall score of 0.63 suggests we are letting through a number of cases of toxic commentary. This is most likely due to the very large class imbalance we noted during our EDA. The model only has a few cases of toxic comments to train on compared to non-toxic which impairs its ability to learn about what constitutes a toxic comment. \n",
    "\n",
    "In the future we will run all models once again with up-sampling and down-sampling applied and see whether this leads to a better preicsion and recall for the positive class. \n",
    "\n",
    "(see ML_models.ipynb for direct comparison to our other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subgroup metrics\n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "\n",
    "# These calculations have been provided by Jigsaw AI for scoring based on the metrics of the kaggle competition\n",
    "# https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation\n",
    "\n",
    "# They work by filtering the relevant dataframe into specific subgroups and using the roc_auc_score metric from sklearn.\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df.loc[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df.loc[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df.loc[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df.loc[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset.loc[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "\n",
    "MODEL_NAME = 'prediction_prob_1'\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "#log_bias_metrics_df_train = compute_bias_metrics_for_model(train_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "#log_final_metric_train = get_final_metric(log_bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))\n",
    "\n",
    "nn_bias_metrics_df_test = compute_bias_metrics_for_model(test_results, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "nn_final_metric_test = get_final_metric(nn_bias_metrics_df_test, calculate_overall_auc(test_results, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1065</td>\n",
       "      <td>0.814429</td>\n",
       "      <td>0.834683</td>\n",
       "      <td>0.963505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1519</td>\n",
       "      <td>0.825677</td>\n",
       "      <td>0.814967</td>\n",
       "      <td>0.973397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2040</td>\n",
       "      <td>0.845463</td>\n",
       "      <td>0.856942</td>\n",
       "      <td>0.966448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2452</td>\n",
       "      <td>0.849332</td>\n",
       "      <td>0.826127</td>\n",
       "      <td>0.975361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4386</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.916824</td>\n",
       "      <td>0.963620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>835</td>\n",
       "      <td>0.914738</td>\n",
       "      <td>0.898723</td>\n",
       "      <td>0.972254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>511</td>\n",
       "      <td>0.916247</td>\n",
       "      <td>0.896643</td>\n",
       "      <td>0.972160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5155</td>\n",
       "      <td>0.921562</td>\n",
       "      <td>0.928183</td>\n",
       "      <td>0.962616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4226</td>\n",
       "      <td>0.933272</td>\n",
       "      <td>0.943499</td>\n",
       "      <td>0.958967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian           1065      0.814429  0.834683   \n",
       "6                          black           1519      0.825677  0.814967   \n",
       "5                         muslim           2040      0.845463  0.856942   \n",
       "7                          white           2452      0.849332  0.826127   \n",
       "0                           male           4386      0.910400  0.916824   \n",
       "4                         jewish            835      0.914738  0.898723   \n",
       "8  psychiatric_or_mental_illness            511      0.916247  0.896643   \n",
       "1                         female           5155      0.921562  0.928183   \n",
       "3                      christian           4226      0.933272  0.943499   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.963505  \n",
       "6  0.973397  \n",
       "5  0.966448  \n",
       "7  0.975361  \n",
       "0  0.963620  \n",
       "4  0.972254  \n",
       "8  0.972160  \n",
       "1  0.962616  \n",
       "3  0.958967  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_bias_metrics_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9196725784887158"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_final_metric_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the final bias metric and overall accuracy the results of our LSTM model are very encouraging. In terms of the final weighted AUC, we can see that the model performed significantly better than our classical ML models. Looking at the specific bias subgroups we can see that the model did not particularly struggle with any particular identity group. \n",
    "\n",
    "While this is a good result in terms of our stated aim of reducing bias, we are interested to see the impact of adjusting for the existing class imbalance on our model performances. Especially in terms of precision and recall for toxic comments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
