{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "\n",
    "Preprocessing for NLP is a very task dependent process. The steps we take are dependent on the aims and methods we intend to apply at the modelling stage, as such there may be different paths to take.\n",
    "\n",
    "As a reminder, the task of our study is to be able to train models that are able to effectively classify text comments as Toxic, while minimising bias. Key to doing this will be to train a model that is able to understand contextual relationships between words, but also perhaps to understand the contextual impact of punctuation at a certain point. To help understand this, consider the impact of exclamation points to your view of the context of a sentence. \n",
    "\n",
    "Standard NLP preprocess workflows would generally remove punctuation given that classic methods of word-embedding such as TF-IDF do not gain meaning from these outside of their counts. However the use of pre-trained word-embeddings such as fasttext, GloVE, word2vec, and others, often contain vector representations of punctuation that has been learned by studying their relationships to other words. As a result, we could now skip the removal of punctuation.\n",
    "\n",
    "### Our Method:\n",
    "\n",
    "For this project we are following two paths. \n",
    "\n",
    "#### Traditional ML\n",
    "We will be training a series of classic ML classifiers such as SVM, Logistic Regression and Random Forest. We will embed the words using TF-IDF and therefore we will use a traditional NLP workflow.\n",
    "\n",
    " 1. Remove punctuation, convert to lower case\n",
    " 2. Tokenization\n",
    " 3. Removal of stop words\n",
    " 4. Stemming/Lemmatization\n",
    " \n",
    "We will use tools from NLTK/Spacy to achieve this, with adjustments for quirks in our data. \n",
    "\n",
    "#### Neural Networks with GloVe\n",
    "We will also be training a neural network model which takes advantage of glove word embeddings. As such we will not be carrying out the standard work flow. Instead, we will begin by comparing the number of tokens in our dataset vocabulary to the glove word embeddings. We will remove as many OOV words/symbols as we can. \n",
    "\n",
    "We will then attempt to correct issues such as misspellings that are causing words to not match with the word-embeddings. We will leave in punctuation that has a vector representation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gov/anaconda3/envs/capstone/lib/python3.7/site-packages/tqdm/std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import contractions\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the cleaned train and cleaned+concatenated test datasets. \n",
    "train_df = pd.read_csv('data/train_clean.csv')\n",
    "test_df = pd.read_csv('data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the first \n",
    "train_df = train_df.iloc[:,1:]\n",
    "test_df = test_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing:\n",
    "\n",
    "We will start with a baseline approach of:\n",
    "* Casing to lower\n",
    "* Expanding contractions\n",
    "* Tokenizing - Uses nltk.TweetTokenizer (with reduce_len = true)\n",
    "* Removing Stop words Uses nltk.English stop words\n",
    "* Lemmatizing - nltk. wordnet lemmatizer on adverbs, adjectives, verbs and nouns\n",
    "\n",
    "This is a standard workflow for pre-processing text. We will experiment with results if we remove some of these\n",
    "\n",
    "Ultimately this is a difficult task given these are online comments. We will likely see numerous misspellings that cause the tools we use to miss certain issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case.\n",
    "df['cleaned_comment'] = df['comment_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use contractions library to expand contractions\n",
    "# Contraction package code can be found here https://github.com/kootenpv/contractions, contractions list can be found below\n",
    "# https://github.com/kootenpv/contractions/blob/master/contractions/__init__.py\n",
    "# The package is very useful for expanding contractions and is good at doing so for slang \n",
    "\n",
    "# We could also implement this using a dictionary built from \n",
    "# https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "\n",
    "def contraction_expand(text):\n",
    "    return contractions.fix(text)\n",
    "#teststr = \"hi i'm the coolest cat yall, y'all, there's isn't\"\n",
    "\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: contraction_expand(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of some comments\n",
    "print(df['comment_text'].loc[1])\n",
    "print(df['cleaned_comment'].loc[1])\n",
    "print('')\n",
    "print(df['comment_text'].loc[204000])\n",
    "print('')\n",
    "print(df['cleaned_comment'].loc[204000])\n",
    "\n",
    "print('')\n",
    "print(df['comment_text'].loc[565934])\n",
    "print('')\n",
    "print(df['cleaned_comment'].loc[565934])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we can see that we have successfuly converted the strings to lower case and appear to have expanded out contractions, certainly we have captured common cases such as \"I'm\". We will now progress to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "\n",
    "# We will use the TweetTokenizer from nltk.\n",
    "# Our data has been taken from online comments, which likely share very similar traits as a Tweet, such as hashtags and\n",
    "# additional punctuation for emphasis. This tokenizer should more effectively deal with them.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(reduce_len=True) # Set reducelen to true to somewhat reduce lengths of words like 'waaayyy'\n",
    "\n",
    "#apply tokenizer using lambda function \n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# remove stop words via lambda function and list comprehension\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Punctuation\n",
    "# We are taking advantage of the translate functionality of the string function. \n",
    "\n",
    "# first we make a translation table - what we are doing here is actually making an empty table. The third argument is for\n",
    "# charachters which will be mapped to None if found. As such by putting in string.punctuation we are creating a translate\n",
    "# table which will set punctuation to none \n",
    "punc_table = str.maketrans('', '', string.punctuation)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: [item.translate(punc_table) for item in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check\n",
    "print(df.loc[1,['comment_text', 'cleaned_comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# lemmatize words using the WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Lemmatizer works on part of speech words, so we need to run this over the various pos,\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text_noun(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='n') for w in text]\n",
    "\n",
    "def lemmatize_text_verb(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='v') for w in text]\n",
    "def lemmatize_text_adj(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='a') for w in text]\n",
    "def lemmatize_text_adv(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='r') for w in text]\n",
    "\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_noun)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_verb)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_adj)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now the end of our standard pipeline, we have put all the above steps together in a function below which we can apply to the text. THis will also allow us to ensure we treat the test set in the exact same way as we have done for the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRE-PROCESS PIPELINE FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "def text_cleaner(df, col_name, clean_col_name):\n",
    "   \n",
    "\n",
    "    # Lemmatize functions to be called lated\n",
    "    # Lemmatize nouns\n",
    "    def lemmatize_text_noun(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='n') for w in text]\n",
    "    \n",
    "    # Lemmatize verbs\n",
    "    def lemmatize_text_verb(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='v') for w in text]\n",
    "    # Lemmatize adjectives\n",
    "    def lemmatize_text_adj(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='a') for w in text]\n",
    "\n",
    "    # Lemmatize adverbs\n",
    "    def lemmatize_text_adv(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='r') for w in text]\n",
    "    \n",
    "    # Expand contraction method\n",
    "    def contraction_expand(text):\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    # To lower case.\n",
    "    df[clean_col_name] = df[col_name].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Expand contractions\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: contraction_expand(x))\n",
    "    \n",
    "    #Tokenize:\n",
    "    tokenizer = TweetTokenizer(reduce_len=True)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: tokenizer.tokenize(x))\n",
    "   \n",
    "    \n",
    "    #Remove Stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    #Delete punctuation\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item.translate(punc_table) for item in x])\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_noun)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_verb)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adj)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adv)\n",
    "    \n",
    "    \n",
    "    return None\n",
    "\n",
    "def detokenizer(df, col_name):\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    df[col_name+'_detokenize'] = df[col_name].apply(lambda x: detokenizer.detokenize(x))\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run the cleaner func on train data\n",
    "text_cleaner(train_df, 'comment_text', 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# run text cleaner on testdf\n",
    "text_cleaner(test_df, 'comment_text', 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detokenize\n",
    "detokenizer(train_df,'comment_text_clean')\n",
    "detokenizer(test_df,'comment_text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Glove word embeddings\n",
    "\n",
    "For our Neural Network models we will be using Glove pre-trained word-embeddings. Key to the success of the model will therefore be processing our text to ensure we have as high a vocabulary coverage as possible. This means we will have to correct as many miss-spellings and contractions as we can. We will not use the cleaning pipeline defined above that we use for the classic ML models as the Glove word embeddings contain vector representation for many of the words that are removed by that pipeline. Instead we will tailor our approach to maximise the percentage of our vocabulary fromt he dataset that we can match to a word-embedding. \n",
    "\n",
    "We will use the Glove Common Crawl 840B 300dim vectors. We believe this will achieve superior accuracy over using the smaller 6B set which was trained on Wikipedia only. Remember, our comments come from various online sources so will likely use language differently than Wikipedia which is more formalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the cleaned train and cleaned+concatenated test datasets. \n",
    "train_df = pd.read_csv('data/train_clean.csv')\n",
    "test_df = pd.read_csv('data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the first col\n",
    "train_df = train_df.iloc[:,1:]\n",
    "test_df = test_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define functions used to read embeddings in and build vocab\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "import operator\n",
    "\n",
    "\n",
    "GLOVE_FILE = 'embeds/glove.840B.300d.txt'\n",
    "\n",
    "\n",
    "def load_embed(file):\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float16')[:1]\n",
    "    \n",
    "        \n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(file))\n",
    "        \n",
    "    return embedding_index\n",
    "\n",
    "def build_vocab(texts, verbose =  True):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "# this function checks our vocab against whats in the embedding matrix\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in vocab:\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print(f'Found embeddings for {len(a) / len(vocab):.2%} of vocab')\n",
    "    print(f'Found embeddings for  {k / (k + i):.2%} of all text')\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 16s, sys: 5.32 s, total: 2min 22s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove = load_embed(GLOVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Lets check the OOV for our pre-processed df and for df_train\n",
    "vocab = build_vocab(train_df['comment_text'])\n",
    "glove_oov = check_coverage(train_df, 'comment_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we were only able to find 15.82% of the words in our vocab within the embedding. Let us investigate the glove_oov dictionary to see what kind of words we are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Yes,', 19056),\n",
       " ('that,', 18289),\n",
       " ('(and', 16524),\n",
       " ('is,', 16305),\n",
       " ('\"The', 16292),\n",
       " ('Trump.', 15692),\n",
       " ('However,', 13721),\n",
       " ('So,', 13048),\n",
       " ('them,', 12684),\n",
       " ('people,', 12450),\n",
       " ('Well,', 12328),\n",
       " ('it?', 12276),\n",
       " ('No,', 12099),\n",
       " ('time,', 12042),\n",
       " ('\"I', 11621),\n",
       " ('course,', 11289),\n",
       " ('years,', 11245),\n",
       " ('Trump,', 10657),\n",
       " ('me,', 10600),\n",
       " ('said,', 10543),\n",
       " ('now,', 10281),\n",
       " ('way,', 10224),\n",
       " ('all,', 9671),\n",
       " ('this,', 9626),\n",
       " ('fact,', 9427),\n",
       " ('know,', 8904),\n",
       " ('again,', 8904),\n",
       " ('you?', 8787),\n",
       " ('(or', 8756),\n",
       " ('here,', 8750),\n",
       " ('\"the', 8747),\n",
       " ('yes,', 8678),\n",
       " ('not,', 8668),\n",
       " ('right?', 8385),\n",
       " ('up,', 8375),\n",
       " ('say,', 8273),\n",
       " ('Also,', 8200),\n",
       " ('Oh,', 8122),\n",
       " ('that?', 8051),\n",
       " ('well,', 7834),\n",
       " ('right,', 7649),\n",
       " ('out,', 7497),\n",
       " ('Canada,', 7374),\n",
       " ('so,', 7150),\n",
       " ('him,', 6864),\n",
       " ('year,', 6735),\n",
       " ('Yeah,', 6714),\n",
       " ('and,', 6645),\n",
       " ('But,', 6620),\n",
       " ('there,', 6582),\n",
       " ('100%', 6567),\n",
       " ('money,', 6541),\n",
       " ('country,', 6482),\n",
       " ('And,', 6475),\n",
       " ('do,', 6437),\n",
       " ('example,', 6196),\n",
       " ('case,', 6046),\n",
       " ('one,', 6018),\n",
       " ('then,', 5913),\n",
       " ('however,', 5883),\n",
       " ('world,', 5707),\n",
       " ('ago,', 5685),\n",
       " ('(which', 5639),\n",
       " ('government,', 5608),\n",
       " ('on,', 5603),\n",
       " ('us,', 5390),\n",
       " ('are,', 5362),\n",
       " ('Actually,', 5334),\n",
       " ('Now,', 5317),\n",
       " ('point,', 5235),\n",
       " ('work,', 5216),\n",
       " ('day,', 5215),\n",
       " ('law,', 5187),\n",
       " ('taxes,', 5171),\n",
       " ('don’t', 5140),\n",
       " ('(I', 5119),\n",
       " ('(the', 5029),\n",
       " ('life,', 4971),\n",
       " ('this?', 4943),\n",
       " ('what?', 4787),\n",
       " ('words,', 4746),\n",
       " ('too,', 4650),\n",
       " ('it!', 4554),\n",
       " ('no,', 4450),\n",
       " ('in,', 4448),\n",
       " ('article,', 4420),\n",
       " ('system,', 4411),\n",
       " ('Unfortunately,', 4336),\n",
       " ('(not', 4283),\n",
       " ('(as', 4274),\n",
       " ('wrong,', 4210),\n",
       " ('to,', 4207),\n",
       " ('them?', 4174),\n",
       " ('state,', 4138),\n",
       " ('\"We', 4130),\n",
       " ('50%', 4097),\n",
       " ('problem,', 4000),\n",
       " ('thing,', 3970),\n",
       " ('hand,', 3933),\n",
       " ('Really?', 3916),\n",
       " ('Again,', 3873),\n",
       " ('election,', 3842),\n",
       " ('true,', 3841),\n",
       " ('man,', 3829),\n",
       " ('down,', 3807),\n",
       " ('Sorry,', 3797),\n",
       " ('citizens.', 3787),\n",
       " ('good,', 3768),\n",
       " ('opinion,', 3766),\n",
       " ('others,', 3695),\n",
       " ('today,', 3671),\n",
       " ('tax,', 3649),\n",
       " ('care,', 3644),\n",
       " ('though,', 3642),\n",
       " ('more,', 3636),\n",
       " ('job,', 3629),\n",
       " ('women,', 3596),\n",
       " ('2)', 3588),\n",
       " ('be,', 3569),\n",
       " ('1)', 3541),\n",
       " ('US,', 3538),\n",
       " ('\"It', 3518),\n",
       " ('Wow,', 3501),\n",
       " ('jobs,', 3454),\n",
       " ('it’s', 3432),\n",
       " ('issue,', 3415),\n",
       " ('now?', 3409),\n",
       " ('Why?', 3393),\n",
       " ('America,', 3378),\n",
       " ('decades.', 3363),\n",
       " ('history,', 3350),\n",
       " ('children,', 3339),\n",
       " ('t,', 3334),\n",
       " ('education,', 3245),\n",
       " ('1%', 3206),\n",
       " ('(in', 3152),\n",
       " ('things,', 3110),\n",
       " ('did,', 3097),\n",
       " ('anything,', 3069),\n",
       " ('change,', 3033),\n",
       " ('power,', 3028),\n",
       " ('yet,', 3015),\n",
       " ('agenda.', 3014),\n",
       " ('Canadians.', 3009),\n",
       " ('business,', 2997),\n",
       " ('..', 2996),\n",
       " ('but,', 2975),\n",
       " ('\"If', 2968),\n",
       " ('\"a', 2963),\n",
       " ('\"You', 2915),\n",
       " ('God,', 2896),\n",
       " ('was,', 2893),\n",
       " ('China,', 2887),\n",
       " ('person,', 2865),\n",
       " ('20%', 2849),\n",
       " ('Alaska,', 2837),\n",
       " ('BTW,', 2814),\n",
       " ('home,', 2807),\n",
       " ('news,', 2786),\n",
       " ('do?', 2785),\n",
       " ('family,', 2781),\n",
       " ('mean,', 2776),\n",
       " ('for,', 2774),\n",
       " ('media,', 2769),\n",
       " ('here?', 2764),\n",
       " ('10%', 2763),\n",
       " ('issues,', 2749),\n",
       " ('Obama,', 2741),\n",
       " ('times,', 2698),\n",
       " ('you!', 2684),\n",
       " ('with,', 2674),\n",
       " ('Clinton.', 2666),\n",
       " ('far,', 2664),\n",
       " ('done,', 2660),\n",
       " ('comment,', 2659),\n",
       " ('place,', 2652),\n",
       " ('democracy.', 2626),\n",
       " ('back,', 2622),\n",
       " ('much,', 2616),\n",
       " ('vote,', 2601),\n",
       " ('rights,', 2595),\n",
       " ('agree,', 2594),\n",
       " ('not?', 2585),\n",
       " ('this:', 2584),\n",
       " ('elected.', 2579),\n",
       " ('Trump’s', 2565),\n",
       " ('story,', 2565),\n",
       " ('\"A', 2548),\n",
       " ('question,', 2538),\n",
       " ('violence.', 2534),\n",
       " ('have,', 2534),\n",
       " ('Meanwhile,', 2529),\n",
       " ('many,', 2529),\n",
       " ('want,', 2505),\n",
       " ('office,', 2495),\n",
       " ('It’s', 2486),\n",
       " ('schools,', 2484),\n",
       " ('Sure,', 2479),\n",
       " ('etc.,', 2476),\n",
       " ('bad,', 2475),\n",
       " ('war,', 2472),\n",
       " ('facts,', 2470),\n",
       " ('racist,', 2445),\n",
       " ('\"In', 2443),\n",
       " ('(a', 2438),\n",
       " ('economy,', 2436),\n",
       " ('First,', 2421),\n",
       " ('off,', 2402),\n",
       " ('days,', 2402),\n",
       " ('see,', 2392),\n",
       " ('voters.', 2386),\n",
       " ('90%', 2363),\n",
       " ('people?', 2356),\n",
       " ('racist.', 2355),\n",
       " ('better,', 2351),\n",
       " ('party,', 2349),\n",
       " ('(like', 2349),\n",
       " ('laws,', 2342),\n",
       " ('truth,', 2338),\n",
       " ('countries,', 2334),\n",
       " ('themselves,', 2328),\n",
       " ('president,', 2326),\n",
       " ('else,', 2324),\n",
       " ('school,', 2320),\n",
       " ('I,', 2318),\n",
       " ('enough,', 2317),\n",
       " ('like,', 2295),\n",
       " ('Russia,', 2288),\n",
       " ('Clinton,', 2285),\n",
       " ('first,', 2285),\n",
       " ('Sadly,', 2279),\n",
       " ('her,', 2252),\n",
       " ('$10', 2243),\n",
       " ('Hey,', 2239),\n",
       " ('about,', 2230),\n",
       " ('yeah,', 2220),\n",
       " ('\"What', 2218),\n",
       " ('eh?', 2211),\n",
       " ('what,', 2203),\n",
       " ('crime,', 2195),\n",
       " ('water,', 2191),\n",
       " ('(including', 2189),\n",
       " ('Or,', 2184),\n",
       " ('food,', 2177),\n",
       " ('comments,', 2176),\n",
       " ('matter,', 2175),\n",
       " ('says,', 2172),\n",
       " ('there?', 2172),\n",
       " ('think,', 2171),\n",
       " ('about?', 2164),\n",
       " ('Trump?', 2161),\n",
       " ('OK,', 2150),\n",
       " ('years?', 2139),\n",
       " ('of,', 2134),\n",
       " ('Instead,', 2126),\n",
       " ('society,', 2124),\n",
       " ('(if', 2120),\n",
       " ('him?', 2109),\n",
       " ('away,', 2106),\n",
       " ('\"Trump', 2097),\n",
       " ('end,', 2095),\n",
       " ('game,', 2094),\n",
       " ('tRump', 2092),\n",
       " ('go,', 2092),\n",
       " ('sure,', 2078),\n",
       " ('mind,', 2054),\n",
       " ('poor,', 2041),\n",
       " ('Hillary.', 2034),\n",
       " ('President,', 2032),\n",
       " ('I’m', 2029),\n",
       " ('reason,', 2028),\n",
       " ('men,', 2026),\n",
       " ('politics,', 2024),\n",
       " ('religion,', 2012),\n",
       " ('correct,', 2010),\n",
       " ('Constitution.', 1997),\n",
       " ('kids,', 1996),\n",
       " ('before,', 1992),\n",
       " ('Americans,', 1987),\n",
       " ('really,', 1986),\n",
       " ('(who', 1980),\n",
       " ('investigation.', 1971),\n",
       " ('post,', 1967),\n",
       " ('race,', 1956),\n",
       " ('states,', 1954),\n",
       " ('$1', 1953),\n",
       " ('doesn’t', 1949),\n",
       " ('...\"', 1946),\n",
       " ('reality,', 1940),\n",
       " ('other,', 1939),\n",
       " ('consequences.', 1936),\n",
       " ('too?', 1934),\n",
       " ('elections.', 1934),\n",
       " ('past,', 1929),\n",
       " ('will,', 1925),\n",
       " ('over,', 1921),\n",
       " ('spending.', 1909),\n",
       " ('costs,', 1900),\n",
       " ('3)', 1898),\n",
       " ('left,', 1893),\n",
       " ('policies.', 1888),\n",
       " ('Church,', 1884),\n",
       " ('\"we', 1881),\n",
       " ('30%', 1880),\n",
       " ('speech,', 1880),\n",
       " ('line,', 1875),\n",
       " ('up?', 1874),\n",
       " ('something,', 1867),\n",
       " ('5%', 1866),\n",
       " ('again?', 1859),\n",
       " ('(for', 1858),\n",
       " ('Trudeau,', 1856),\n",
       " ('Indeed,', 1851),\n",
       " ('workers.', 1849),\n",
       " ('sense,', 1843),\n",
       " ('80%', 1837),\n",
       " ('least,', 1834),\n",
       " ('\"This', 1827),\n",
       " ('up!', 1827),\n",
       " ('old,', 1825),\n",
       " ('does,', 1824),\n",
       " ('is:', 1819),\n",
       " ('(that', 1818),\n",
       " ('(with', 1815),\n",
       " ('plan,', 1809),\n",
       " ('can’t', 1809),\n",
       " ('saying,', 1807),\n",
       " ('$100', 1805),\n",
       " ('“The', 1805),\n",
       " ('house,', 1804),\n",
       " ('Trudeau.', 1797),\n",
       " ('(i.e.', 1797),\n",
       " ('“I', 1793),\n",
       " ('racism.', 1792),\n",
       " ('States,', 1790),\n",
       " ('etc,', 1788),\n",
       " ('worse,', 1785),\n",
       " ('experience,', 1783),\n",
       " ('is?', 1781),\n",
       " ('citizens,', 1778),\n",
       " ('week,', 1776),\n",
       " ('housing,', 1774),\n",
       " ('yourself,', 1765),\n",
       " ('nothing,', 1762),\n",
       " ('it.\"', 1760),\n",
       " ('name,', 1757),\n",
       " ('Ah,', 1757),\n",
       " ('guy,', 1756),\n",
       " ('folks,', 1753),\n",
       " ('violence,', 1752),\n",
       " ('who,', 1746),\n",
       " ('later,', 1744),\n",
       " ('community,', 1744),\n",
       " ('part,', 1739),\n",
       " ('oil,', 1730),\n",
       " ('40%', 1729),\n",
       " ('industry,', 1723),\n",
       " ('\"But', 1721),\n",
       " ('Finally,', 1717),\n",
       " ('Yep,', 1713),\n",
       " ('policy,', 1713),\n",
       " ('control,', 1708),\n",
       " ('out?', 1707),\n",
       " ('leadership.', 1703),\n",
       " ('car,', 1702),\n",
       " ('long,', 1701),\n",
       " ('side,', 1697),\n",
       " ('class,', 1696),\n",
       " ('\"There', 1691),\n",
       " ('Hawaii,', 1690),\n",
       " ('insurance,', 1690),\n",
       " ('public,', 1689),\n",
       " ('Democrats.', 1688),\n",
       " ('then?', 1686),\n",
       " ('\"...', 1685),\n",
       " ('hey,', 1682),\n",
       " ('supporters.', 1681),\n",
       " ('land,', 1678),\n",
       " ('immigrants.', 1674),\n",
       " ('area,', 1671),\n",
       " ('25%', 1670),\n",
       " ('Personally,', 1666),\n",
       " ('services,', 1663),\n",
       " ('Republicans.', 1659),\n",
       " ('2%', 1658),\n",
       " ('Second,', 1657),\n",
       " ('crimes.', 1655),\n",
       " ('military.', 1654),\n",
       " ('politicians.', 1652),\n",
       " ('idea,', 1652),\n",
       " ('cases,', 1650),\n",
       " ('to?', 1649),\n",
       " ('from?', 1640),\n",
       " ('problems,', 1636),\n",
       " ('Canadians,', 1628),\n",
       " ('candidate.', 1627),\n",
       " ('\"He', 1627),\n",
       " ('didn’t', 1626),\n",
       " ('income,', 1625),\n",
       " ('\"no', 1624),\n",
       " ('happen,', 1620),\n",
       " ('Really,', 1615),\n",
       " ('lies,', 1614),\n",
       " ('very,', 1610),\n",
       " ('2016.', 1610),\n",
       " ('Canada?', 1609),\n",
       " ('nation,', 1607),\n",
       " ('rail.', 1601),\n",
       " ('that!', 1601),\n",
       " ('service,', 1599),\n",
       " ('best,', 1597),\n",
       " ('racism,', 1595),\n",
       " ('Liberals.', 1593),\n",
       " ('15%', 1588),\n",
       " ('choice,', 1584),\n",
       " ('point?', 1582),\n",
       " ('Thanks,', 1578),\n",
       " ('me?', 1575),\n",
       " ('police,', 1569),\n",
       " ('evidence,', 1567),\n",
       " ('process,', 1563),\n",
       " ('friends,', 1562),\n",
       " ('Ontario.', 1561),\n",
       " ('why?', 1560),\n",
       " ('wait,', 1558),\n",
       " ('Otherwise,', 1554),\n",
       " ('companies,', 1549),\n",
       " ('own,', 1545),\n",
       " ('around,', 1544),\n",
       " ('Then,', 1544),\n",
       " ('future,', 1543),\n",
       " ('on?', 1539),\n",
       " ('market,', 1539),\n",
       " ('irrelevant.', 1536),\n",
       " ('white,', 1532),\n",
       " ('workers,', 1528),\n",
       " ('#1', 1525),\n",
       " ('they?', 1523),\n",
       " ('decades,', 1519),\n",
       " ('time?', 1518),\n",
       " ('Alaskans.', 1510),\n",
       " ('(at', 1506),\n",
       " ('Wow!', 1504),\n",
       " ('level,', 1503),\n",
       " ('guns,', 1503),\n",
       " ('view,', 1502),\n",
       " ('city,', 1495),\n",
       " ('beliefs.', 1493),\n",
       " ('USA,', 1492),\n",
       " ('housing.', 1491),\n",
       " ('himself,', 1484),\n",
       " ('can,', 1481),\n",
       " ('here:', 1480),\n",
       " ('great,', 1475),\n",
       " ('everyone,', 1474),\n",
       " ('taxpayers.', 1473),\n",
       " ('woman,', 1473),\n",
       " ('lives,', 1472),\n",
       " ('majority.', 1472),\n",
       " ('either,', 1467),\n",
       " ('situation,', 1459),\n",
       " ('clear,', 1458),\n",
       " ('or,', 1457),\n",
       " ('prison.', 1456),\n",
       " ('country?', 1455),\n",
       " ('Hillary,', 1454),\n",
       " ('too!', 1453),\n",
       " ('instance,', 1449),\n",
       " ('\"fake', 1448),\n",
       " ('Anchorage.', 1447),\n",
       " ('debt,', 1445),\n",
       " ('money?', 1445),\n",
       " ('victims.', 1443),\n",
       " ('much?', 1440),\n",
       " ('information,', 1438),\n",
       " ('for?', 1438),\n",
       " ('\"No', 1437),\n",
       " ('budget,', 1436),\n",
       " ('\"you', 1435),\n",
       " ('be?', 1434),\n",
       " ('read,', 1428),\n",
       " ('Besides,', 1426),\n",
       " ('cars,', 1424),\n",
       " ('culture,', 1418),\n",
       " ('2017.', 1406),\n",
       " ('responsibility.', 1405),\n",
       " ('same,', 1405),\n",
       " ('$20', 1404),\n",
       " ('drugs,', 1403),\n",
       " ('Yet,', 1401),\n",
       " ('group,', 1401),\n",
       " ('immigration.', 1399),\n",
       " ('anyone,', 1399),\n",
       " ('which,', 1395),\n",
       " ('Europe,', 1394),\n",
       " ('age,', 1393),\n",
       " ('\"it', 1386),\n",
       " ('church,', 1385),\n",
       " ('revenue.', 1383),\n",
       " ('means,', 1377),\n",
       " ('sorry,', 1375),\n",
       " ('possible,', 1373),\n",
       " ('Korea,', 1373),\n",
       " ('property,', 1372),\n",
       " ('propaganda.', 1371),\n",
       " ('record,', 1369),\n",
       " ('70%', 1368),\n",
       " ('military,', 1364),\n",
       " ('...and', 1360),\n",
       " ('2016,', 1357),\n",
       " ('rules,', 1357),\n",
       " ('(but', 1353),\n",
       " ('Seriously,', 1352),\n",
       " ('immigrants,', 1348),\n",
       " ('politicians,', 1347),\n",
       " ('\"good', 1345),\n",
       " ('people\"', 1344),\n",
       " ('fraud.', 1344),\n",
       " ('population,', 1343),\n",
       " ('\"not', 1342),\n",
       " ('help,', 1341),\n",
       " ('fine,', 1338),\n",
       " ('year?', 1338),\n",
       " ('please,', 1338),\n",
       " ('House,', 1338),\n",
       " ('\"And', 1337),\n",
       " ('funding.', 1337),\n",
       " ('itself,', 1336),\n",
       " ('(to', 1336),\n",
       " ('Furthermore,', 1334),\n",
       " ('campaign,', 1330),\n",
       " ('60%', 1319),\n",
       " ('sector.', 1319),\n",
       " ('billion.', 1318),\n",
       " ('families,', 1318),\n",
       " ('science,', 1317),\n",
       " ('us?', 1314),\n",
       " ('State,', 1314),\n",
       " ('alt-right', 1312),\n",
       " ('Congress,', 1312),\n",
       " ('them!', 1310),\n",
       " ('crisis.', 1309),\n",
       " ('Anyway,', 1309),\n",
       " ('in?', 1309),\n",
       " ('Ontario,', 1307),\n",
       " ('etc.)', 1305),\n",
       " ('indeed,', 1305),\n",
       " ('problem?', 1305),\n",
       " ('(even', 1304),\n",
       " ('term,', 1302),\n",
       " ('Democrats,', 1301),\n",
       " ('Republicans,', 1300),\n",
       " ('lying.', 1300),\n",
       " ('murder.', 1298),\n",
       " ('rate,', 1297),\n",
       " ('team,', 1295),\n",
       " ('months,', 1293),\n",
       " ('parents,', 1292),\n",
       " ('fired.', 1290),\n",
       " ('win,', 1289),\n",
       " ('one?', 1285),\n",
       " ('addition,', 1284),\n",
       " ('that’s', 1280),\n",
       " ('night,', 1280),\n",
       " ('communities.', 1279),\n",
       " ('pay,', 1278),\n",
       " ('Constitution,', 1277),\n",
       " ('rich,', 1273),\n",
       " ('immigration,', 1272),\n",
       " ('students,', 1271),\n",
       " ('myself,', 1271),\n",
       " ('usual,', 1270),\n",
       " ('lie,', 1266),\n",
       " ('Please,', 1266),\n",
       " ('free,', 1263),\n",
       " ('wrong?', 1262),\n",
       " ('alone,', 1262),\n",
       " ('anti-Trump', 1259),\n",
       " ('actions,', 1258),\n",
       " ('\"in', 1256),\n",
       " ('Syria,', 1255),\n",
       " ('benefits,', 1255),\n",
       " ('believe,', 1255),\n",
       " ('together,', 1253),\n",
       " ('bill,', 1251),\n",
       " ('article?', 1251),\n",
       " ('deal,', 1250),\n",
       " ('death,', 1249),\n",
       " ('Gee,', 1249),\n",
       " ('criminals.', 1246),\n",
       " ('support,', 1245),\n",
       " ('\"How', 1242),\n",
       " ('ideology.', 1236),\n",
       " ('(no', 1236),\n",
       " ('respect,', 1235),\n",
       " ('(2)', 1229),\n",
       " ('democracy,', 1228),\n",
       " ('work?', 1228),\n",
       " ('Therefore,', 1227),\n",
       " ('voters,', 1227),\n",
       " ('huh?', 1223),\n",
       " ('99%', 1222),\n",
       " ('Further,', 1220),\n",
       " ('another,', 1220),\n",
       " ('liar.', 1218),\n",
       " ('bias.', 1218),\n",
       " ('abuse,', 1216),\n",
       " ('it...', 1215),\n",
       " ('say?', 1215),\n",
       " ('Still,', 1215),\n",
       " ('presidency.', 1214),\n",
       " ('funds.', 1214),\n",
       " ('healthcare.', 1213),\n",
       " ('position,', 1211),\n",
       " ('program,', 1210),\n",
       " ('roads,', 1209),\n",
       " ('again!', 1208),\n",
       " ('reasons,', 1207),\n",
       " ('were,', 1203),\n",
       " ('thought,', 1203),\n",
       " ('stupid,', 1201),\n",
       " ('Remember,', 1199),\n",
       " ('regards,', 1199),\n",
       " ('anyway,', 1199),\n",
       " ('leaders.', 1197),\n",
       " ('child,', 1197),\n",
       " ('\"Why', 1195),\n",
       " ('honest,', 1195),\n",
       " ('Iraq,', 1193),\n",
       " ('(see', 1187),\n",
       " ('Yup,', 1184),\n",
       " ('Okay,', 1184),\n",
       " ('homeless.', 1183),\n",
       " ('employees,', 1183),\n",
       " ('What?', 1181),\n",
       " ('from,', 1181),\n",
       " ('legislation.', 1180),\n",
       " ('homes,', 1177),\n",
       " ('general,', 1177),\n",
       " ('\"When', 1176),\n",
       " ('\"They', 1176),\n",
       " ('Toronto,', 1173),\n",
       " ('statement,', 1173),\n",
       " ('abortion.', 1172),\n",
       " ('logic,', 1172),\n",
       " ('because,', 1170),\n",
       " ('IMO,', 1166),\n",
       " ('2015,', 1166),\n",
       " ('word,', 1163),\n",
       " ('think?', 1163),\n",
       " ('\"free', 1160),\n",
       " ('company,', 1158),\n",
       " ('homeless,', 1158),\n",
       " ('isn’t', 1157),\n",
       " ('fair,', 1157),\n",
       " ('elected,', 1157),\n",
       " ('corruption.', 1155),\n",
       " ('\"what', 1155),\n",
       " ('groups,', 1145),\n",
       " ('lying,', 1144),\n",
       " ('know?', 1143),\n",
       " ('ways,', 1139),\n",
       " ('event,', 1139),\n",
       " ('businesses.', 1138),\n",
       " ('rates,', 1135),\n",
       " ('credibility.', 1134),\n",
       " ('(it', 1134),\n",
       " ('dollars,', 1133),\n",
       " ('season,', 1131),\n",
       " ('Obviously,', 1129),\n",
       " ('use,', 1129),\n",
       " ('1,', 1128),\n",
       " ('Germany,', 1126),\n",
       " ('Apparently,', 1125),\n",
       " ('ever,', 1124),\n",
       " ('administration,', 1120),\n",
       " ('3%', 1118),\n",
       " ('writes:', 1117),\n",
       " ('Today,', 1117),\n",
       " ('doing,', 1117),\n",
       " ('policies,', 1117),\n",
       " ('candidates.', 1116),\n",
       " ('increase.', 1115),\n",
       " ('health,', 1114),\n",
       " ('Syria.', 1113),\n",
       " ('Nope,', 1113),\n",
       " ('yet?', 1113),\n",
       " ('it\"', 1112),\n",
       " ('all?', 1110),\n",
       " ('Brexit', 1109),\n",
       " ('question:', 1109),\n",
       " ('note,', 1108),\n",
       " ('article:', 1107),\n",
       " ('argument,', 1105),\n",
       " ('residents.', 1104),\n",
       " ('terrorism.', 1098),\n",
       " ('important,', 1095),\n",
       " ('values,', 1095),\n",
       " ('retirement.', 1094),\n",
       " ('up\"', 1094),\n",
       " ('cost,', 1093),\n",
       " ('behavior,', 1092),\n",
       " ('action,', 1091),\n",
       " ('abortion,', 1090),\n",
       " ('environment,', 1089),\n",
       " ('Muslims.', 1088),\n",
       " ('Obamacare.', 1087),\n",
       " ('That’s', 1084),\n",
       " ('healthcare,', 1083),\n",
       " ('positions.', 1081),\n",
       " ('leader,', 1080),\n",
       " ('claims.', 1080),\n",
       " ('million,', 1079),\n",
       " ('Washington,', 1078),\n",
       " ('above,', 1077),\n",
       " ('John,', 1074),\n",
       " ('spending,', 1074),\n",
       " ('2015.', 1073),\n",
       " ('Islam.', 1071),\n",
       " ('real,', 1070),\n",
       " ('happened,', 1070),\n",
       " ('illegal,', 1070),\n",
       " ('\"white', 1069),\n",
       " ('why,', 1068),\n",
       " ('wealth.', 1067),\n",
       " ('we,', 1067),\n",
       " ('book,', 1066),\n",
       " ('live,', 1066),\n",
       " ('ACA.', 1064),\n",
       " ('aside,', 1064),\n",
       " ('Secondly,', 1064),\n",
       " ('$50', 1063),\n",
       " ('well?', 1063),\n",
       " ('programs,', 1061),\n",
       " ('Muslims,', 1060),\n",
       " ('resources,', 1060),\n",
       " ('also,', 1060),\n",
       " ('narrative.', 1056),\n",
       " ('$$', 1053),\n",
       " ('has,', 1053),\n",
       " ('Party,', 1053),\n",
       " ('road,', 1049),\n",
       " ('you’re', 1047),\n",
       " ('Wrong.', 1046),\n",
       " ('so?', 1043),\n",
       " ('involved,', 1042),\n",
       " ('less,', 1042),\n",
       " ('Putin.', 1041),\n",
       " ('liberals.', 1039),\n",
       " ('importantly,', 1039),\n",
       " ('prices,', 1037),\n",
       " ('faith,', 1037),\n",
       " ('deficit.', 1035),\n",
       " ('made,', 1035),\n",
       " ('Re:', 1034),\n",
       " ('two,', 1031),\n",
       " ('protest.', 1030),\n",
       " ('thinking,', 1030),\n",
       " ('numbers,', 1028),\n",
       " ('thing?', 1026),\n",
       " ('Seriously?', 1026),\n",
       " ('PFD.', 1024),\n",
       " ('law?', 1024),\n",
       " ('Clearly,', 1024),\n",
       " ('otherwise,', 1023),\n",
       " ('welfare.', 1023),\n",
       " ('mean?', 1023),\n",
       " ('victim.', 1022),\n",
       " ('LOL!', 1017),\n",
       " ('Heck,', 1017),\n",
       " ('votes,', 1017),\n",
       " ('such,', 1017),\n",
       " ('gas,', 1016),\n",
       " ('Mexico,', 1016),\n",
       " ('Trump!', 1015),\n",
       " ('been,', 1015),\n",
       " ('charges.', 1015),\n",
       " ('blame.', 1014),\n",
       " ('place?', 1014),\n",
       " ('(e.g.', 1012),\n",
       " ('refugees.', 1010),\n",
       " ('everything,', 1010),\n",
       " ('stuff,', 1009),\n",
       " ('with?', 1009),\n",
       " ('short,', 1009),\n",
       " ('staff,', 1008),\n",
       " ('remember,', 1007),\n",
       " ('it:', 1006),\n",
       " ('Liberals,', 1004),\n",
       " ('\"all', 1003),\n",
       " ('Russians.', 1001),\n",
       " ('$2', 1001),\n",
       " ('statements.', 1000),\n",
       " ('government?', 1000),\n",
       " ('news\"', 999),\n",
       " ('we?', 995),\n",
       " ('me!', 995),\n",
       " ('candidate,', 994),\n",
       " ('members,', 994),\n",
       " ('simple,', 992),\n",
       " ('individuals.', 992),\n",
       " ('teachers,', 992),\n",
       " ('profits.', 991),\n",
       " ('high,', 991),\n",
       " ('Oregon,', 991),\n",
       " ('terrorists.', 991),\n",
       " ('on!', 989),\n",
       " ('criminal.', 988),\n",
       " ('a)', 987),\n",
       " ('governments.', 987),\n",
       " ('2017,', 985),\n",
       " ('4)', 985),\n",
       " ('some,', 983),\n",
       " ('sad,', 982),\n",
       " ('agenda,', 982),\n",
       " ('world?', 981),\n",
       " ('b)', 980),\n",
       " ('answer,', 979),\n",
       " ('run,', 979),\n",
       " ('way?', 979),\n",
       " ('Harper.', 978),\n",
       " ('security,', 978),\n",
       " ('love,', 978),\n",
       " ('friend,', 976),\n",
       " ('won’t', 976),\n",
       " ('hard,', 975),\n",
       " ('gone,', 974),\n",
       " ('lost,', 974),\n",
       " ('France,', 974),\n",
       " ('decision,', 974),\n",
       " ('True,', 973),\n",
       " ('\"As', 971),\n",
       " ('Um,', 971),\n",
       " ('\"That', 971),\n",
       " ('court,', 971),\n",
       " ('reporting.', 970),\n",
       " ('expense.', 970),\n",
       " ('infrastructure.', 970),\n",
       " ('w/', 970),\n",
       " ('language,', 970),\n",
       " ('Ok,', 970),\n",
       " ('column.', 969),\n",
       " ('elections,', 969),\n",
       " ('points,', 969),\n",
       " ('crimes,', 968),\n",
       " ('Eugene.', 968),\n",
       " ('Plus,', 967),\n",
       " ('Vancouver.', 966),\n",
       " ('(they', 966),\n",
       " ('2019.', 965),\n",
       " ('$5', 965),\n",
       " ('cities,', 965),\n",
       " ('Right,', 964),\n",
       " ('election?', 964),\n",
       " ('fire,', 964),\n",
       " ('Vancouver,', 962),\n",
       " ('now!', 962),\n",
       " ('once,', 961),\n",
       " ('research,', 958),\n",
       " ('interest,', 957),\n",
       " ('investigation,', 957),\n",
       " ('perspective,', 957),\n",
       " ('works,', 957),\n",
       " ('“We', 956),\n",
       " ('conservative.', 956),\n",
       " ('Act,', 955),\n",
       " ('U.S.,', 954),\n",
       " ('understand,', 954),\n",
       " ('areas,', 954),\n",
       " ('wife,', 953),\n",
       " ('questions,', 953),\n",
       " ('project,', 951),\n",
       " ('month,', 951),\n",
       " ('news?', 950),\n",
       " ('nations.', 949),\n",
       " ('today?', 949),\n",
       " ('Hopefully,', 948),\n",
       " ('justice,', 948),\n",
       " ('threat.', 948),\n",
       " ('warming.', 947),\n",
       " ('$500', 944),\n",
       " ('happens,', 943),\n",
       " ('Frankly,', 940),\n",
       " ('out!', 940),\n",
       " ('infrastructure,', 939),\n",
       " ('maybe,', 938),\n",
       " ('corruption,', 937),\n",
       " ('gun,', 935),\n",
       " ('(The', 934),\n",
       " ('\".', 934),\n",
       " ('constitution.', 933),\n",
       " ('businesses,', 933),\n",
       " ('corporations.', 929),\n",
       " ('Jesus,', 927),\n",
       " ('Iran,', 925),\n",
       " ('sex,', 923),\n",
       " ('right!', 921),\n",
       " ('liberal,', 921),\n",
       " ('ban.', 920),\n",
       " ('worry,', 917),\n",
       " ('parties,', 916),\n",
       " ('legislature.', 916),\n",
       " ('goes,', 916),\n",
       " ('marriage,', 915),\n",
       " ('hate,', 911),\n",
       " ('province.', 908),\n",
       " ('leaders,', 908),\n",
       " ('(except', 908),\n",
       " ('safe,', 908),\n",
       " ('returns.', 907),\n",
       " ('new,', 907),\n",
       " ('dead,', 907),\n",
       " ('supporters,', 907),\n",
       " ('journalism.', 906),\n",
       " ('fund.', 904),\n",
       " ('Uh,', 903),\n",
       " ('Alberta.', 901),\n",
       " ('it)', 901),\n",
       " ('75%', 901),\n",
       " ('(of', 900),\n",
       " ('Bush,', 900),\n",
       " ('fees,', 899),\n",
       " ('wrote:', 899),\n",
       " ('Anchorage,', 897),\n",
       " ('attack,', 896),\n",
       " ('(especially', 896),\n",
       " ('BC,', 895),\n",
       " ('rail,', 893),\n",
       " ('liar,', 893),\n",
       " ('care?', 893),\n",
       " ('knowledge,', 892),\n",
       " ('$15', 889),\n",
       " ('absurd.', 888),\n",
       " ('Senate,', 888),\n",
       " ('Drumpf', 887),\n",
       " ('Donald.', 885),\n",
       " ('Additionally,', 885),\n",
       " ('murder,', 884),\n",
       " ('generations.', 884),\n",
       " ('he’s', 884),\n",
       " ('authority.', 883),\n",
       " ('95%', 883),\n",
       " ('games,', 883),\n",
       " ('(you', 883),\n",
       " ('time!', 883),\n",
       " ('face,', 882),\n",
       " ('Paul,', 881),\n",
       " ('period,', 881),\n",
       " ('(from', 879),\n",
       " ('system?', 879),\n",
       " ('ago?', 879),\n",
       " ('Quebec.', 878),\n",
       " ('head,', 878),\n",
       " ('town,', 878),\n",
       " ('someone,', 877),\n",
       " ('report,', 877),\n",
       " ('really?', 876),\n",
       " ('wages,', 876),\n",
       " ('leadership,', 874),\n",
       " ('Moreover,', 873),\n",
       " ('You,', 873),\n",
       " ('result,', 873),\n",
       " ('doctors,', 871),\n",
       " ('POTUS.', 871),\n",
       " ('no?', 871),\n",
       " ('California,', 870),\n",
       " ('ignorant,', 870),\n",
       " ('order,', 869),\n",
       " ('(although', 869),\n",
       " ('taxes?', 867),\n",
       " ('small,', 867),\n",
       " ('conservative,', 866),\n",
       " ('evil,', 865),\n",
       " ('courts.', 865),\n",
       " ('corrupt,', 864),\n",
       " ('stated,', 863),\n",
       " ('solution,', 863),\n",
       " ('needed,', 862),\n",
       " ('need,', 862),\n",
       " ('slavery.', 861),\n",
       " ('rhetoric.', 860),\n",
       " ('anymore,', 860),\n",
       " ('ones,', 859),\n",
       " ('$3', 859),\n",
       " ('$200', 857),\n",
       " ('politician.', 857),\n",
       " ('funny,', 857),\n",
       " ('\"just', 857),\n",
       " ('century,', 854),\n",
       " ('data,', 853),\n",
       " ('\"to', 853),\n",
       " ('\"if', 851),\n",
       " ('(by', 851),\n",
       " ('wage.', 851),\n",
       " ('interests,', 849),\n",
       " ('promises.', 848),\n",
       " ('Thus,', 846),\n",
       " ('anything?', 844),\n",
       " ('therefore,', 842),\n",
       " ('teaching.', 841),\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_oov #output hidden "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that a number of the most common missing vocab words are contractions or words with a possessive apostrophie. Naturally there are also a number of miss-spellings. \n",
    "\n",
    "In addition, there appear to be a number of incorrect grammar examples, emojis, and also names. Interestingly with the names, we do not appear to be missing the name itself, i.e Trump or Obama, but rather missing the possessive case, e.g: Trump's or Obama's. We could therefore see if we can lemmatize/stem these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Emojis and other symbols\n",
    "Let us first deal with emojis and symbols. Emojis can contain useful information and we may have some contained in our text given this was taken from a common crawl. We will try and preserve emojis we have glove embeddings for and delete ones which we don't. We will do the same for other symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get all the characters from our vocabulary\n",
    "# we can use our build_vocab method from before \n",
    "clean_vocab = build_vocab(train_df['comment_text'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list comprehension to pull out characters\n",
    "# Instead of generating a large list, we append each character into a long string which allows us to view them easier, \n",
    "# We have added two spaces per character to spread these out.\n",
    "# The list comprehension simply takes a char for each char in our dict, if the length is 1 (i.e a char)\n",
    "clean_vocab_chars = ''.join([char for char in clean_vocab if len(char) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now make a filter to remove all regular letters and numbers\n",
    "\n",
    "# We can classify symbols as anything which are not latin letters. \n",
    "# The string package contains a list of common ascii characters and digits \n",
    "# https://docs.python.org/3/library/string.html\n",
    "\n",
    "# string.ascii_letters includes lowercase and upper case. We define a filter below \n",
    "non_symbols = string.digits + string.ascii_letters\n",
    "\n",
    "# We could also include common latin-based languages charahcters (western european).\n",
    "# This list was taken from Latin-1 charset table at: https://cs.stanford.edu/people/miles/iso8859.html#ISO\n",
    "# There are likely more that we could exclude\n",
    "latin_based_char = 'ÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ'\n",
    "\n",
    "# add this to filter\n",
    "non_symbols = non_symbols + latin_based_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new string of symbols which has been filtered\n",
    "clean_vocab_symbols = ''\n",
    "clean_vocab_symbols = ''.join([char for char in clean_vocab_chars if not char in non_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.—-,(:&@!;$=–🐵>\"?😑*…#\\ue014~/)•≠+\\uf04a\\'🐶“§%·τα☺\\uf0e0😜[]”|💖→❤_😂😄😀―{ᴀ★\\x96}😎😱🚌🌟😊`👍💩💯⛽🚄😖♡\\xad😈►◄💔½^\\u200b🙄✬\\ufeff\\x7f£±<😉🙂😃😐💙😳☆¾😘😞😓😍‘🙏😡🎶🌺😁🤔😔🙃😏😕💚😢😟\\\\😭😆🔗¬🚽😴🤗♫👮ι🐟💎♥\\x95⛲🍰ā›💛😠👎❧◞\\x13▂▃▄▅▆▇😵😨’☹➡⬅😅«»🆕👅👥👶💕👲👤👄🔛🙈⇒\\uf0b7⅓😋ġ⏺💰😲∙😇🙆👐†💫💤🍇\\ue613🏡¶🌞🎲🤓🍕©💋🙀💀🎄🏆😒™🗑İ💃¢👿😧༼つÀ💜￼🐝✔🎅🍺●🎵🌎😬🎉🤣😣Сви✘😝🤢╪▶☭✭🌸ℐ\\uf04c😺⏏🍔🐮🌹®🍀🔥\\u200f👻🙁🤑☎🖒✨✰❆☙ˌ🎓🚪⚲⚭⚆⬭⬯○‣∎💥✈🎼⅛¼🚀☠Яя𝗮📺🐋💓с\\x10😤👺😯🚶←𝙖🙌🤘ͦ☼⋆💞\\uf0a7🚢👏⊂🤡🚂⅔\\u200c‒Ｉ\\u200dд🐒\\x9f\\u202a⏩ﷻ😮🦄😩🚗✊🐳😥🤤▸⦁⛷🤧σ⇌¿𝒶🦊☐☑🐽😫🦆😷♾⚠🎁̶✓‐🤠잘🔫\\u202c☒▪↑😪𝖆𝕴∼☝💡🤥\\uf005🦁🤐𝒂😦♪😌🌏👽🍽🐾✌😰🐄−🔨😛¸€о🍭🛳👣🐸☻🐀👊🎈👀🍻🍩μ😗🏂👳🍗💪❄👌🍒⠀🐑⏰◦⛓♬💊🤙😶𝑰∵∴𝗜🍊⤏🔹🍎🐴🐎𝘢☞☜▲↴↳💘🏀⊘▫⬇Ā🚴☮🖤🥘➕🚫♀✋🐻🤦🎃－ａ🤞📉🔭𝒙\\uf10a🤯🐷💳🙇🔼'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_vocab_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of punctuation, emojis make up the majority of symbols in our dataset vocabulary. In addition to this we have a number of characters from different encodings. We can now compare this set with what we have available in the word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same as above \n",
    "# The below code iterates through the \n",
    "glove_char = ''.join([char for char in glove if len(char) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us filter out the symbols\n",
    "# Similar list_comprehension as above, but if statement to check c is  not in our filter \n",
    "glove_symbols = ''.join([char for char in glove_char if not char in non_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←§″′█½…“★”–●►−¢²¬░¡¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆¯♦¤▲¸¾⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤¹≤‡√◄━⇒▶º≥╝♡◊。✈≡☺✔↵≈✓♣☎℃◦└‟～！○◆№♠▌✿▸⁄□❖✦．｜À┃／￥╠↩✭▐☼µ☻┐├«∼┌℉☮฿≦♬✧〉－⌂✖･◕※‖◀‰\\x97↺∆œ┘┬╬،⌘š⊂ª＞〈⎙Å？☠⇐▫∗∈≠♀ƒ♔˚℗┗＊┼❀ı＆∩♂‿∑‣➜┛⇓☯⊖☀┳；∇⇑✰◇♯☞´ə↔┏｡◘∂✌♭┣┴┓✨ˈ˜❥┫℠✒ž［∫\\x93≧］\\x94∀♛\\x96∨◎ˑ↻⅓⇩＜≫✩ˆ✪♕؟₤☛╮␊＋┈ɡ％╋▽⇨┻⊗￡।▂✯▇＿➤₂✞＝▷△◙▅✝ﾟ∧␉☭┊╯☾➔∴\\x92▃↳＾׳➢╭➡＠⊙☢˝⅛∏ā„①๑∥❝Š☐▆Ÿ╱⋙๏☁⇔▔\\x91②➚◡╰٠♢˙۞✘✮☑⋆ℓⓘ❒☣✉ē⌊➠∣❑⅔◢ⓒ\\x80〒∕▮⦿✫✚⋯♩☂ˌ❞‗č܂☜ī‾✜╲∘⟩ō＼⟨·⅜✗Ă♚∅ⓔ◣͡‛❦⑨③◠✄❄１∃␣≪｢≅◯☽２İ∎｣⁰❧̅ǡⒶ↘⚓▣˘∪⇢✍ɛ⊥＃⅝⎯↠۩☰◥⊆✽ﬁ⚡↪ở❁☹ł◼☃◤❏Žⓢ⊱α➝̣✡∠｀▴┤Ȃ∝♏ⓐ✎;３④␤＇❣⅞✂✤ⓞ☪✴⌒˛♒＄ɪ✶▻Ⓔ◌◈۲Ʈ❚ʿ❂￦◉╜̃ťν✱╖❉₃ⓡℝ٤↗❶ʡ۰ˇⓣ♻➽۶₁ʃ׀✲Đʤ✬☉▉≒☥⌐♨✕ⓝ⊰❘＂⇧̵➪４▁βđ۱▏⊃ⓛ‚♰́✏⏑Œ̶٩Ⓢー⩾日￠❍≃⋰♋ɿ､̂ǿ❋✳ⓤ╤▕⌣✸℮⁺▨⑤╨Ⓥ♈❃☝Ā５✻⊇≻♘♞◂７✟Łū⌠✠☚✥ŋ❊ƂⒸŮ⌈❅Ⓡ♧Ⓞɑλ۵▭❱Ⓣ∟☕♺∵⍝ⓑɔ✵ŕ✣ℤ年ℕ٭♆Ⓘⅆ∶⚜◞்✹Ǥȡ➥ᴥ↕ɂ̳∷✋į➧∋̿ͧʘ┅⥤⬆ǀμ₄⋱ʔ☄↖⋮۔♌Ⓛ╕♓ـ⁴❯♍▋ă✺⭐６✾♊➣▿Ⓑ♉Ａ⏠◾▹⑥⩽в↦ż╥⍵⌋։➨и∮⇥ⓗⒹ⁻ʊć⎝⌥⌉◔◑ǂ✼♎ℂ♐╪ɨ⊚☒⇤θВⓜ⎠Ｏ◐ǰ⚠╞ﬂş◗⎕ⓨ☟Ｉⓟ♟❈↬ⓓŞ◻♮❙а♤∉؛⁂例ČⓃ־♑╫╓╳⬅☔πɒɹ߂Ō☸ɐʻ┄╧ʌ׃８ʒ⎢ġ❆⋄⚫ħ̏☏➞͂␙Ⓤ◟Ƥąʕ̊Ȥ⚐✙は↙̾ωΔ℘ﾞ✷⑦φ⍺❌⊢▵✅ｗ９ⓖ☨▰ʹŢ╡Ⓜő☤∽╘Ű˹↨ȿ♙⬇♱ś⌡Ω⠀╛❕┉Ⓟ̀Ǩ♖ⓚ┆⑧⎜Śǹ◜⚾⤴✇╟⎛☩➲➟ⓥⒽŘ⏝Ŀ◃０₀╢月↯✆ĶĢ˃⍴Ĥ❇ũ⚽╒Ｃɻɤ̸ʼ♜☓Ｔ➳⇄γ☬⚑✐⁵δȭ⌃◅▢ｓȸ❐ě∊☈ⅇℜ॥σ⎮ȣ▩のτεřＳŀு⊹‵␔☊➸̌☿⇉Ĺ➊⊳╙⁶ⓦ⇣｛̄↝ź⎟ęℳŹ▍❗ℑＭɾſｍŧĦ״Γ΄▞◁⛄⇝Ż⎪ˤ♁ｖ⇠☇ǻ✊位ℒạி｝๐⭕ĺ➘Ｂ❺ɸˡ⁀⑩ｃ⅕ŪƼ۳☙❛ɣ₆ŷƪ❓⟲Ʒ⇀≲Ｐ❷١ńŦŐⓕ⎥ＤсŔ\\u06ddǥͤ₋̱Ń̎♝Ľ≳▙ＲŤḥʹ➭ℰ܀ğʺȫⒼ⇛ˉ▊❸Ǻ号⇗̷Ŵ⇱℅Ⓧ⚛̐ːǴʱ̕⇌ŏＦｏ␀≌๖Ⓦ⊤̓ď☦ĭĳℚⒻɚ▜ʋʞ➙Ⓨ⌨◮☷ǁ◍ĮⓀ≔ų⏩⍳℞┋ɲ˻▚₅≺ْĒƱ▟➻Ďţ̪ŭ⏪̉❼⎞ρȥʂ┇⍟⇪ʧ▎⇦␝ＥỞľ⤷➀≖ＵＱ⟶♗ℵ̴♄⅙ͨ̈Ǭ❜̡ṣǫ▛✁➩ĸா˂↥Ī⏎⎷Ć̲➖ģĄʐɽ↲ℬℏ⩵̗Ｊ由η❢₈Ɣ≎➋ΣＬƖ❹Ĵǵ⚔⇇̑⊿ŬĞĚĥ۴⁸ᶘ̖ǾơΦ☍ȯ➹ĉ⥊⁁✢ａɯ回ĵ'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we actually have a number of word embeddings for symbols, and most importantly, we have word embeddings for punctuation marks including exclamation points. Recall that we mentioned earlier about how some punctuation may acutally help convey meaning. As GloVe comes with these we are able to keep the punctuation marks if needed. \n",
    "\n",
    "However, it is clear that we are likely going to have to drop a significant number of the emojis as the word-embedding set we have chosen has very few. Perhaps there are other word-embedding sets which have been trained including emojis. \n",
    "\n",
    "For the purposes of our neural network model, we will delete the symbols with no word-embeddings. Any word/symbol with no embedding gets automatically assignedd a OOV token which translates to a value of 0. As such we will not gain no information from including these symbols and instead will benefit from the fact that we have to process less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new string using same method above of dataset vocab symbols not in glove symbols\n",
    "drop_symbol = ''.join([char for char in clean_vocab_symbols if char not in glove_symbols])\n",
    "\n",
    "# create a keep list\n",
    "keep_list = ''.join([char for char in clean_vocab_symbols if char in glove_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(drop_symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we will be dropping 277 symbols from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keep_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.42 s, sys: 80.3 ms, total: 7.5 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# There are two methods of doing this, we can use string.replace and loop over the comments. However it would be faster \n",
    "# to use the string.translate() method as we use in our text cleaning pipeline earlier.\n",
    "\n",
    "symb_table = str.maketrans('', '', drop_symbol)\n",
    "train_df['comment_text_clean'] = train_df['comment_text'].apply(lambda x: x.translate(symb_table))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check what symbols remain\n",
    "clean_vocab2 = build_vocab(train_df['comment_text_clean'], verbose=False)\n",
    "clean_vocab_chars2 = ''.join([char for char in clean_vocab2 if len(char) == 1])\n",
    "clean_vocab_symbols2 = ''\n",
    "clean_vocab_symbols2 = ''.join([char for char in clean_vocab_chars2 if not char in non_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_symbol2 = ''.join([char for char in clean_vocab_symbols2 if char not in glove_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🏻🏼ɴᴛ🍾🏾🐕т👆🏽👉༽️𝙠ѕ👹𝙣𝘀𝘁👑💨🌝𝙡щ𝒕𝒇𝒎𝗳𝒔𝘴🐈👈🎨𝒏🎻𝗻👋Д'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_symbol2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above symbols remain in our vocabulary but not in the glove symbols. Lets apply another translate to remove these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add these extra symbols to the original list\n",
    "drop_symbol_final = drop_symbol2+drop_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new translation table and use the drop_symbol_final string\n",
    "symb_table = str.maketrans('', '', drop_symbol_final)\n",
    "train_df['comment_text_clean'] = train_df['comment_text_clean'].apply(lambda x: x.translate(symb_table))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with possessive apostrophes\n",
    "\n",
    "One point we noted earlier was that a number of OOV words were cases where we had a possessive apostrophe. We checked some of these and found that in many cases the base word was present in the embedding. If we therefore remove the posessive apostrophe, we should see a large increase in 'in-vocabulary' words. There is an argument that possessive apostrophe's contain information on sentence context, however if they are not in the word embeddings we are using then there is nothing to be lost by removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# We need to deal with cases where the word ends in s so is written \"James', Chris'\" e.t.c\n",
    "# We therefore use regex rather than simply replacing on \"'s'\"\n",
    "# the below regex uses ? to greedy match zero to one s, this way we will remove cases where the word ends in \"'\" and no s \n",
    "train_df['comment_text_clean'] = train_df['comment_text_clean'].apply(lambda x: re.sub(\"'s?\", \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making these changes, let us observe the impact on the in-vocabulary percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804874/1804874 [00:24<00:00, 73744.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 16.93% of vocab\n",
      "Found embeddings for  90.79% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(train_df['comment_text_clean'])\n",
    "glove_oov = check_coverage(vocab,glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have increased our percentage to 16.93%, however this remains low. Our next step will be to tokenize the sentences, we can see that a number of common OOV are ones with a punctuation mark at the end of the word. Tokenization will solve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "tokenizer = TweetTokenizer(reduce_len=True)\n",
    "train_df['comment_text_clean2'] = train_df['comment_text_clean'].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "#detokenizer = TreebankWordDetokenizer()\n",
    "#train_df['comment_text3'] = train_df['comment_text3'].apply(lambda x: detokenizer.detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but we can pass lists like tokenized text\n",
    "def build_vocab2(texts):\n",
    "    #sentences = texts.apply(lambda x: x.split()).values\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in texts:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 52.00% of vocab\n",
      "Found embeddings for  99.50% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab2 = build_vocab2(train_df['comment_text_clean2'])\n",
    "glove_oov2 = check_coverage(vocab2, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step we have achieved much greater accuracy of 52.00%, so over half of our vocabulary has a vector representation. It is important to remember at this stage that we are dealing with online comments so there is likely to be a large amount of slang, misspelling, and other quirks that mean getting a high accuracy will be difficult. Our word embeddings were taken from a common crawl of the internet so some of these may be accounted for, but unless we trained our own word embeddings on a similar set of data it will be hard to get very high accuracies. Let's check if there are any other low hanging fruit we can fix, else we will use this on our baseline model and revisit if we get poor accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('..', 43344),\n",
       " ('.\\n.', 10379),\n",
       " ('. . .', 5586),\n",
       " ('tRump', 2503),\n",
       " ('alt-right', 1959),\n",
       " ('.\\n\\n.', 1714),\n",
       " ('Brexit', 1665),\n",
       " ('. .', 1650),\n",
       " (');', 1568),\n",
       " ('):', 1528),\n",
       " ('. ...', 1469),\n",
       " ('anti-Trump', 1436),\n",
       " ('. \\n.', 1192),\n",
       " ('Drumpf', 1176),\n",
       " ('#MAGA', 1106),\n",
       " ('deplorables', 1020),\n",
       " (':(', 792),\n",
       " ('SB91', 778),\n",
       " ('alt-left', 641),\n",
       " ('Trumpcare', 567),\n",
       " ('...\\n\\n...', 553),\n",
       " ('. . . .', 543),\n",
       " ('Trumpism', 535),\n",
       " ('ᴅ', 499),\n",
       " (':-/', 493),\n",
       " ('bigly', 473),\n",
       " ('Klastri', 452),\n",
       " ('.  \\n.', 429),\n",
       " ('8:', 419),\n",
       " ('(8', 412),\n",
       " ('.\\n...', 401),\n",
       " ('...\\n...', 395),\n",
       " ('...\\n.', 387),\n",
       " ('.\\n.\\n.', 385),\n",
       " ('Auwe', 384),\n",
       " ('http://bit.ly/2gTbpns', 381),\n",
       " ('.\\n\\n...', 353),\n",
       " ('ʜᴇ', 351),\n",
       " ('Trumpian', 347),\n",
       " ('Trumpsters', 337),\n",
       " ('ᴜᴘ', 331),\n",
       " ('ʙʏ', 330),\n",
       " ('Yᴏᴜ', 330),\n",
       " ('.  ...', 323),\n",
       " ('Vinis', 321),\n",
       " (':/', 305),\n",
       " ('Saullie', 298),\n",
       " ('. ..', 296),\n",
       " ('shibai', 290),\n",
       " ('T-rump', 287),\n",
       " ('SJWs', 281),\n",
       " ('TFWs', 279),\n",
       " ('Koncerned', 275),\n",
       " ('pro-Trump', 265),\n",
       " ('RangerMC', 260),\n",
       " ('klastri', 250),\n",
       " ('BCLibs', 248),\n",
       " ('Trudope', 246),\n",
       " ('do:', 246),\n",
       " ('comᴵᴵᴵ', 245),\n",
       " ('garycrum', 245),\n",
       " ('Daesh', 241),\n",
       " ('.  .', 240),\n",
       " ('Alt-Right', 231),\n",
       " (':-(', 225),\n",
       " ('Donkel', 223),\n",
       " ('OBAMAcare', 222),\n",
       " ('Finicum', 219),\n",
       " ('http://www.cashapp24.com/', 218),\n",
       " ('Trumpkins', 218),\n",
       " ('Cheetolini', 216),\n",
       " ('wiliki', 214),\n",
       " ('Beyak', 209),\n",
       " ('Trudeaus', 208),\n",
       " ('Tridentinus', 203),\n",
       " ('Ontariowe', 202),\n",
       " ('bavius', 187),\n",
       " ('ᴏ', 185),\n",
       " ('ᴄʜᴇᴄᴋ', 184),\n",
       " ('ғᴏʀ', 184),\n",
       " ('Nageak', 184),\n",
       " ('ᴄᴏᴍᴘᴜᴇʀ', 183),\n",
       " ('Zupta', 182),\n",
       " ('Meggsy', 182),\n",
       " ('MUPTE', 182),\n",
       " ('... .', 182),\n",
       " ('Nurnie', 179),\n",
       " ('ᴍᴏʜ', 176),\n",
       " ('Kealohas', 175),\n",
       " ('ᴡᴏʀᴋɪɢ', 175),\n",
       " ('ᴊᴏʙ', 174),\n",
       " ('ᴏғ', 174),\n",
       " ('ʜᴏᴜʀʟʏ', 174),\n",
       " ('8)', 174),\n",
       " ('cash-for-access', 173),\n",
       " ('ʜɪs', 173),\n",
       " ('TrumpCare', 172),\n",
       " ('Anti-Trump', 172),\n",
       " ('Crapwell', 169),\n",
       " ('MAGAphants', 167),\n",
       " ('ᴡᴇᴇᴋ', 167),\n",
       " ('ʟɪᴋ', 167),\n",
       " ('ʜᴏᴍᴇ', 167),\n",
       " ('l2g', 167),\n",
       " ('ʜᴠᴇ', 166),\n",
       " ('ᴄ', 166),\n",
       " ('Putrumpski', 165),\n",
       " ('Deplorables', 165),\n",
       " ('ᴇᴅ', 165),\n",
       " ('ғɪʀs', 165),\n",
       " ('ʏᴏᴜʀ', 165),\n",
       " ('sɪɢɪɢ', 165),\n",
       " ('ʙᴏᴏᴍ', 165),\n",
       " ('ғᴏʟʟᴏᴡɪɢ', 165),\n",
       " ('Mᴋᴇ', 165),\n",
       " ('ᴄᴏᴇᴄɪᴏ', 165),\n",
       " ('ɪᴇʀᴇ', 165),\n",
       " ('ʀᴇʟɪʙʟᴇ', 165),\n",
       " ('ᴇᴇᴅ', 165),\n",
       " ('ᴏʟʏ', 165),\n",
       " ('ɪᴄᴏᴍᴇ', 165),\n",
       " ('ᴇxʀ', 165),\n",
       " ('ᴇᴇᴅɪɢ', 165),\n",
       " ('ʏᴏᴇ', 165),\n",
       " ('ᴏʀ', 165),\n",
       " ('ᴍᴏᴍs', 165),\n",
       " ('sʏ--ʜᴏᴍᴇ', 165),\n",
       " ('sᴜᴅᴇs', 165),\n",
       " ('Gʀᴇ', 165),\n",
       " ('ғʀᴏᴍ', 165),\n",
       " ('Sʀ', 165),\n",
       " ('do;', 162),\n",
       " ('SHOPO', 161),\n",
       " ('motleycrew', 159),\n",
       " ('Alt-Left', 158),\n",
       " ('\\u200e', 158),\n",
       " ('SW-SAZ', 156),\n",
       " ('Twitler', 154),\n",
       " ('YUGE', 152),\n",
       " ('tRUMP', 149),\n",
       " ('clickbait', 149),\n",
       " ('TheDonald', 147),\n",
       " ('post-truth', 145),\n",
       " ('907AK', 144),\n",
       " ('Trumpland', 143),\n",
       " ('8/', 143),\n",
       " ('hodad', 142),\n",
       " ('Mahawker', 141),\n",
       " ('Wiliki', 139),\n",
       " ('Catou', 136),\n",
       " ('Dejain', 136),\n",
       " ('Krookwell', 135),\n",
       " ('KABATA', 135),\n",
       " ('LesterP', 134),\n",
       " ('<3', 133),\n",
       " ('Trumpettes', 132),\n",
       " ('Lazeelink', 131),\n",
       " ('antifluoridationists', 130),\n",
       " ('Bozievich', 130),\n",
       " ('Moyane', 129),\n",
       " ('diverdave', 129),\n",
       " ('gubmit', 127),\n",
       " (';-}', 127),\n",
       " ('Pizzagate', 126),\n",
       " ('Alt-right', 126),\n",
       " ('flexpipe', 126),\n",
       " ('Trumpists', 126),\n",
       " ('Clodwell', 125),\n",
       " ('Exedus', 123),\n",
       " ('trumpy', 123),\n",
       " ('Ossoff', 122),\n",
       " ('whataboutism', 122),\n",
       " ('Dotard', 120),\n",
       " ('RadirD', 120),\n",
       " ('djou', 119),\n",
       " ('ITMFA', 118),\n",
       " ('Trump-Russia', 118),\n",
       " ('skyofblue', 117),\n",
       " ('click-bait', 117),\n",
       " ('covfefe', 115),\n",
       " ('Sarasi', 115),\n",
       " ('CO₂', 115),\n",
       " ('http://www.22moneybay.com', 115),\n",
       " ('muckamuck', 115),\n",
       " ('gubmut', 114),\n",
       " ('Kaanui', 113),\n",
       " ('Zuptas', 112),\n",
       " ('Anbang', 112),\n",
       " ('Colkoch', 110),\n",
       " ('Trumpty', 109),\n",
       " ('...\\n\\n.', 108),\n",
       " ('utilitas', 108),\n",
       " ('gofundme', 108),\n",
       " ('McWynnety', 107),\n",
       " ('Hoopili', 106),\n",
       " ('http://www.cash-spot.com', 106),\n",
       " ('minority-elect', 104),\n",
       " ('Patkotak', 104),\n",
       " ('::', 104),\n",
       " ('Bridich', 103),\n",
       " ('Lieberal', 103),\n",
       " ('antifluoridationist', 103),\n",
       " ('virtue-signalling', 101),\n",
       " ('AKLNG', 101),\n",
       " ('baby-in-chief', 100),\n",
       " ('Trumplethinskin', 100),\n",
       " ('AntiFa', 99),\n",
       " ('Trumpist', 99),\n",
       " ('▱', 98),\n",
       " ('TransMountain', 97),\n",
       " ('keaukaha', 96),\n",
       " ('Almandinger', 96),\n",
       " ('ponokeali', 96),\n",
       " ('Beegor', 95),\n",
       " ('onkeys', 94),\n",
       " ('keith_moon', 93),\n",
       " ('.\\n\\n\\n.', 93),\n",
       " ('Rmiller', 92),\n",
       " ('theDonald', 91),\n",
       " ('NPVIC', 91),\n",
       " ('Kizla', 90),\n",
       " ('PETROWS', 90),\n",
       " ('HitLIARy', 89),\n",
       " ('Trump-like', 89),\n",
       " ('NASWI', 88),\n",
       " ('Trumpo', 88),\n",
       " ('Whazzie', 87),\n",
       " ('AlwaysThere', 87),\n",
       " (')=', 87),\n",
       " ('https://docs.google.com/document/d/1DzOz3Y6D8g_MNXHNMJYAz1b41_cn535aU5UsN7Lj8X8/preview#',\n",
       "  86),\n",
       " ('hiLIARy', 86),\n",
       " ('MAGAphant', 85),\n",
       " ('DACAs', 85),\n",
       " ('LotL', 85),\n",
       " ('<--', 85),\n",
       " ('non-preschool', 84),\n",
       " ('Crimeans', 83),\n",
       " ('Dromig', 83),\n",
       " ('lieberal', 83),\n",
       " ('dtrumpview', 83),\n",
       " ('Gegonos', 83),\n",
       " ('Victimitis', 82),\n",
       " ('Trumpnuts', 82),\n",
       " ('Turdeau', 81),\n",
       " ('FakeNews.com', 80),\n",
       " ('xbt', 80),\n",
       " ('Tubania', 80),\n",
       " ('MMIW', 79),\n",
       " ('AnonAJ', 79),\n",
       " ('altrightpubs', 78),\n",
       " ('#PMPK', 78),\n",
       " ('BLOTUS', 78),\n",
       " ('yuuuge', 78),\n",
       " ('Caiside', 77),\n",
       " ('Mokantx', 77),\n",
       " ('Utqiagvik', 77),\n",
       " ('manini', 77),\n",
       " ('Veselnitskaya', 76),\n",
       " ('Petrows', 76),\n",
       " ('WesternPatriot', 75),\n",
       " ('brexit', 75),\n",
       " ('Drump', 75),\n",
       " ('virtue-signaling', 75),\n",
       " ('anti-trump', 75),\n",
       " ('StewartBrian', 74),\n",
       " ('Harperite', 74),\n",
       " ('ANGDA', 74),\n",
       " ('trumpster', 74),\n",
       " ('O77', 74),\n",
       " ('Graham-Cassidy', 73),\n",
       " ('Siemien', 73),\n",
       " ('8wop', 73),\n",
       " ('Osweiller', 72),\n",
       " ('etroit', 72),\n",
       " ('Agirl', 72),\n",
       " ('jgd', 71),\n",
       " ('Kadhr', 71),\n",
       " ('Trumpski', 71),\n",
       " ('ericnorstog', 71),\n",
       " ('[8', 71),\n",
       " ('onYouTube', 70),\n",
       " ('NDPee', 70),\n",
       " ('Artster', 70),\n",
       " ('BCFN', 70),\n",
       " ('Sniktaw', 70),\n",
       " ('.. .', 70),\n",
       " ('NFLGTV', 69),\n",
       " ('Wilson-Raybould', 69),\n",
       " ('Eugreen', 69),\n",
       " ('EuGreen', 68),\n",
       " ('Chumpty', 68),\n",
       " ('bsdetection', 68),\n",
       " ('Berig', 68),\n",
       " ('Alt-left', 68),\n",
       " ('anti-Trumpers', 68),\n",
       " ('Obergefell', 68),\n",
       " ('https://www.adn.com/politics/article/series-conoco-employees-help-ease-oil-tax-bill-through-legislature/2013/03/28/',\n",
       "  67),\n",
       " ('.   ...', 67),\n",
       " ('alayhis-Salaam', 67),\n",
       " ('como_estas', 66),\n",
       " ('hapaguy', 66),\n",
       " ('DB-TA', 66),\n",
       " ('Larsy', 65),\n",
       " ('LifeoftheLay', 65),\n",
       " ('OntariOWE', 65),\n",
       " ('Puana', 65),\n",
       " ('. . . . .', 65),\n",
       " ('POMV', 65),\n",
       " ('BCLib', 64),\n",
       " ('Kouvalis', 64),\n",
       " ('Bavius', 64),\n",
       " ('AnnieO', 64),\n",
       " ('wrong-wingers', 64),\n",
       " ('<-', 64),\n",
       " ('<---', 64),\n",
       " ('anti-Alberta', 63),\n",
       " ('/8', 63),\n",
       " ('Mythman', 62),\n",
       " ('BIGLY', 62),\n",
       " ('(=', 62),\n",
       " ('Sher-e-Hindustan', 61),\n",
       " ('invasive-species', 61),\n",
       " ('Kropar', 61),\n",
       " ('ricknro', 61),\n",
       " ('domestikgoddez', 61),\n",
       " ('Cliven', 61),\n",
       " ('jangm', 60),\n",
       " ('khadr', 60),\n",
       " ('Skeexix', 60),\n",
       " ('Just_plain_guest', 60),\n",
       " ('Trumpies', 60),\n",
       " ('#triggered', 60),\n",
       " ('Trumpanzees', 60),\n",
       " ('auwe', 60),\n",
       " ('skinut', 59),\n",
       " ('NP5491', 59),\n",
       " ('_3097', 59),\n",
       " ('#RAILFAIL', 58),\n",
       " ('http://www.jobpro22.com', 58),\n",
       " ('lespark', 58),\n",
       " ('Kahdr', 58),\n",
       " ('Notely', 58),\n",
       " ('Mr.Trump', 58),\n",
       " ('Mellerstig', 58),\n",
       " ('Covfefe', 57),\n",
       " ('KD48', 57),\n",
       " ('Manbaby', 57),\n",
       " ('SirJohn', 57),\n",
       " ('oldgit', 57),\n",
       " ('sirencall', 57),\n",
       " ('Polynesian-Hawaiian', 57),\n",
       " ('AOGA', 57),\n",
       " ('Truthbender', 56),\n",
       " ('🇺', 56),\n",
       " ('http://www.factoryofincome.com', 56),\n",
       " ('fake-news', 55),\n",
       " ('Bitebart', 55),\n",
       " ('Libranos', 55),\n",
       " ('TNRM', 55),\n",
       " ('heco', 55),\n",
       " ('StarAdvertiser', 55),\n",
       " ('Mmusi', 54),\n",
       " ('Ku-Ku-Klowns', 54),\n",
       " ('alt-Left', 54),\n",
       " ('Polynesian-Hawaiians', 54),\n",
       " ('Norstog', 54),\n",
       " ('Planktown', 54),\n",
       " ('Oilies', 54),\n",
       " ('JadedJade', 54),\n",
       " ('Tempmanoa', 53),\n",
       " ('beddah', 53),\n",
       " ('pizzagate', 53),\n",
       " ('mjmchale', 53),\n",
       " ('214Montreal', 53),\n",
       " ('Olrun', 53),\n",
       " ('Hanomansing', 52),\n",
       " ('Chinp', 52),\n",
       " ('Radir', 52),\n",
       " ('Malikane', 52),\n",
       " ('Khadrs', 52),\n",
       " ('Trumpenproletariat', 52),\n",
       " ('..\\n.', 52),\n",
       " ('Killery', 52),\n",
       " ('Mystrom', 52),\n",
       " ('#metoo', 51),\n",
       " ('ramaphosa', 51),\n",
       " ('antifas', 51),\n",
       " ('PoiDoggy', 51),\n",
       " ('https://www.youtube.com/watch?v=SyUDGfCNC-k', 51),\n",
       " ('SCoC', 51),\n",
       " ('lieberals', 51),\n",
       " ('dennism', 51),\n",
       " ('anti-F', 51),\n",
       " ('Kamisato', 51),\n",
       " ('. \\n\\n...', 51),\n",
       " ('Trumpites', 51),\n",
       " ('drumpf', 51),\n",
       " ('Tetpon', 51),\n",
       " ('Eugenean', 51),\n",
       " ('#10', 51),\n",
       " ('Hottubjoe', 50),\n",
       " ('Senzatela', 50),\n",
       " ('Oakbay', 50),\n",
       " ('.\\n.\\n.\\n.', 50),\n",
       " ('anti-vaxxer', 50),\n",
       " ('Boy-Roy', 50),\n",
       " ('YUUUGE', 50),\n",
       " ('Fiberals', 50),\n",
       " ('Tupola', 50),\n",
       " ('Iearned', 50),\n",
       " ('ZANC', 49),\n",
       " ('twittler', 49),\n",
       " ('fRANCIS', 49),\n",
       " ('Nurkic', 49),\n",
       " ('Tumpkin', 49),\n",
       " ('Zuptoid', 49),\n",
       " ('Tpubs', 49),\n",
       " ('SJSJ', 49),\n",
       " ('O-Care', 49),\n",
       " ('Gomeshi', 49),\n",
       " ('anti-Trudeau', 49),\n",
       " ('TRump', 49),\n",
       " ('Alamoana', 49),\n",
       " ('DRUGGIES', 49),\n",
       " ('QuietAndEffective', 49),\n",
       " ('LCFM', 49),\n",
       " ('Valleyisle', 48),\n",
       " ('BarryCare', 48),\n",
       " ('Chumpsters', 48),\n",
       " ('RolandX', 48),\n",
       " ('Balkisoon', 48),\n",
       " ('LIEberals', 48),\n",
       " ('notley', 48),\n",
       " ('anti-pipeline', 48),\n",
       " ('kupunas', 48),\n",
       " ('🇸', 48),\n",
       " ('ak.gov', 48),\n",
       " ('JetRx', 48),\n",
       " ('Muckamuck', 48),\n",
       " ('CriticalReader', 47),\n",
       " ('Russiagate', 47),\n",
       " ('kuroiwaj', 47),\n",
       " ('Elshikh', 47),\n",
       " ('alt-facts', 47),\n",
       " ('nurnie', 47),\n",
       " ('CleanupEugene', 47),\n",
       " ('anctuary', 47),\n",
       " ('HiLIARy', 47),\n",
       " ('Trump-haters', 47),\n",
       " ('courtview', 47),\n",
       " ('WWeek', 47),\n",
       " ('NanakuliBoss', 46),\n",
       " ('FAKEnews', 46),\n",
       " ('#45', 46),\n",
       " ('Trumpelthinskin', 46),\n",
       " ('lukecanada', 46),\n",
       " ('sub-sovereign', 46),\n",
       " ('.   \\n.', 46),\n",
       " ('community-vermin', 46),\n",
       " ('NCReporter', 46),\n",
       " ('DaBus', 46),\n",
       " ('CivilBot', 46),\n",
       " ('. \\n\\n.', 46),\n",
       " ('Trumpians', 46),\n",
       " ('OLive', 46),\n",
       " ('Ellmyer', 46),\n",
       " ('Warmbier', 45),\n",
       " ('#becauseit', 45),\n",
       " ('Cellodad', 45),\n",
       " ('#fakenews', 45),\n",
       " ('https://RiteBeyondRome.com', 45),\n",
       " ('ChuckT', 45),\n",
       " ('AkJen', 45),\n",
       " ('opili', 45),\n",
       " ('SB128', 45),\n",
       " ('ScienceDuck', 45),\n",
       " ('L2g', 45),\n",
       " ('Bananera', 44),\n",
       " ('NDP-Green', 44),\n",
       " ('Squiggs', 44),\n",
       " ('gorebull', 44),\n",
       " ('Duduzane', 44),\n",
       " ('WigAndFakeNose', 44),\n",
       " ('Aintie', 44),\n",
       " ('PCEPI', 44),\n",
       " ('OldBanister', 44),\n",
       " ('AWWU', 44),\n",
       " ('ammosexuals', 44),\n",
       " ('100,000+', 44),\n",
       " ('dbsb', 43),\n",
       " ('separated-from-roadway', 43),\n",
       " ('JZ783', 43),\n",
       " ('POTUSA', 43),\n",
       " ('Rationalthought', 43),\n",
       " ('Chinabot', 43),\n",
       " ('DiverDave', 43),\n",
       " ('Patwant', 43),\n",
       " ('Gruhcho', 43),\n",
       " ('.\\n\\n.\\n.', 43),\n",
       " ('eco-Marxist', 43),\n",
       " (':=', 43),\n",
       " ('];', 43),\n",
       " ('.   .', 43),\n",
       " ('freeheels', 43),\n",
       " ('Mkhwebane', 42),\n",
       " ('Gorkov', 42),\n",
       " ('weewili', 42),\n",
       " ('#WelcomeToCanada', 42),\n",
       " ('Keith_moon', 42),\n",
       " ('NDPeee', 42),\n",
       " ('905ers', 42),\n",
       " ('ahupua', 42),\n",
       " ('TajMahawker', 42),\n",
       " ('pre-ACA', 42),\n",
       " ('o_x', 42),\n",
       " ('guestsquared', 42),\n",
       " ('CoverOregon', 42),\n",
       " ('jack-a', 41),\n",
       " ('NOKO', 41),\n",
       " ('healthherbalclinic', 41),\n",
       " ('non-aboriginals', 41),\n",
       " ('localguy', 41),\n",
       " ('BROohthor', 41),\n",
       " ('Flexpipe', 41),\n",
       " ('http://akleg.gov/docs/pdf/2015TravelReport.pdf', 41),\n",
       " ('cat-hoarders', 41),\n",
       " ('BigOIl', 41),\n",
       " ('https://www.getcivil.com', 40),\n",
       " ('sloter', 40),\n",
       " ('DougCo', 40),\n",
       " ('peterpi', 40),\n",
       " ('𝒉𝒆', 40),\n",
       " ('liquefication', 40),\n",
       " ('THatcher', 40),\n",
       " ('JC12345', 40),\n",
       " ('former-Ukraine', 40),\n",
       " ('whitelash', 40),\n",
       " ('JeffSpooner', 40),\n",
       " ('cat-nutters', 40),\n",
       " ('alt-Right', 40),\n",
       " ('punahou', 40),\n",
       " ('Joseppi', 40),\n",
       " ('. \\n...', 40),\n",
       " ('https://www.youtube.com/watch?v=yCL2d8lLYXw', 40),\n",
       " ('lyft', 40),\n",
       " ('Motleycrew', 40),\n",
       " ('micro-aggression', 40),\n",
       " ('𝒐', 39),\n",
       " ('TIFTFY', 39),\n",
       " ('wealthcare', 39),\n",
       " ('trumpies', 39),\n",
       " ('Pahio', 39),\n",
       " ('Kaliphate', 39),\n",
       " ('Inzoli', 39),\n",
       " ('fakestream', 39),\n",
       " ('Tugley', 39),\n",
       " ('Ebes', 39),\n",
       " ('J.Bob', 39),\n",
       " ('nothing-burger', 39),\n",
       " ('x138', 39),\n",
       " ('#11', 39),\n",
       " ('Pikka', 39),\n",
       " ('commfish', 39),\n",
       " ('GCrum', 38),\n",
       " ('rangerMC', 38),\n",
       " ('Gunmerica', 38),\n",
       " ('Makhosi', 38),\n",
       " ('ILLEGALITY', 38),\n",
       " ('trumpcare', 38),\n",
       " ('Refublicans', 38),\n",
       " ('MoiLee', 38),\n",
       " ('Cladwall', 38),\n",
       " ('a_r', 38),\n",
       " ('CPIH', 38),\n",
       " ('z55man', 38),\n",
       " ('http://www.reuters.com/article/us-election-intelligence-commentary-idUSKCN10F1H5',\n",
       "  38),\n",
       " ('RMiller', 38),\n",
       " ('rickschlosser', 38),\n",
       " ('Trumptards', 38),\n",
       " ('trumpers', 38),\n",
       " ('matullahi', 38),\n",
       " ('t-rump', 38),\n",
       " ('25:31-', 38),\n",
       " ('... ...', 38),\n",
       " ('Prukop', 38),\n",
       " ('tompaine', 38),\n",
       " ('Timol', 37),\n",
       " ('BurtEarnest', 37),\n",
       " ('http://skytrainforsurrey.org/', 37),\n",
       " ('kiragirl', 37),\n",
       " ('Gregsask', 37),\n",
       " ('saullie', 37),\n",
       " ('Tormaine', 37),\n",
       " ('Canie', 37),\n",
       " ('Trump-lite', 37),\n",
       " ('niquab', 37),\n",
       " ('trumpsters', 37),\n",
       " ('https://nccs.net/1996-03-answers-to-americas-tax-problems', 37),\n",
       " ('SB138', 37),\n",
       " ('wrong-winger', 37),\n",
       " ('300,000+', 37),\n",
       " ('MaHawker', 37),\n",
       " ('Hlaudi', 36),\n",
       " ('HanabataDays', 36),\n",
       " ('tcare', 36),\n",
       " ('ElRey', 36),\n",
       " ('SirJohnSirJohn', 36),\n",
       " ('http://www.thecanadianencyclopedia.ca/en/article/petro-canada/', 36),\n",
       " ('anti-Russia', 36),\n",
       " ('MENCKEN', 36),\n",
       " ('Kukailimoku', 36),\n",
       " ('TradCath', 36),\n",
       " ('justmaybe', 36),\n",
       " ('Nslope', 36),\n",
       " ('cat-lickers', 36),\n",
       " ('Hokule', 36),\n",
       " ('1-800-', 36),\n",
       " ('Comeys', 36),\n",
       " ('.\\n..', 36),\n",
       " ('micro-aggressions', 36),\n",
       " ('Trumpish', 36),\n",
       " ('kakaako', 36),\n",
       " ('LIOs', 36),\n",
       " ('siemian', 35),\n",
       " ('Kushners', 35),\n",
       " ('Obamadon', 35),\n",
       " ('alt-reich', 35),\n",
       " ('BamaCare', 35),\n",
       " ('KofM', 35),\n",
       " ('anti-Trumps', 35),\n",
       " ('https://www.indivisibleguide.com/web', 35),\n",
       " ('Orangeness', 35),\n",
       " ('https://www.theguardian.com/technology/2011/mar/17/us-spy-operation-social-networks',\n",
       "  35),\n",
       " ('Shopo', 35),\n",
       " ('alt.right', 35),\n",
       " ('McGuilty', 35),\n",
       " ('Charamsa', 35),\n",
       " ('yjin', 35),\n",
       " ('1127A', 35),\n",
       " ('Trumpanzee', 35),\n",
       " ('DonInKansas', 35),\n",
       " ('p___y', 35),\n",
       " ('carfentanil', 35),\n",
       " ('Trump-bashing', 35),\n",
       " ('_not_', 35),\n",
       " ('=/', 35),\n",
       " ('2,4-', 35),\n",
       " ('Courtview', 35),\n",
       " ('cat-vectored', 35),\n",
       " ('Trumpie', 35),\n",
       " ('AMBBA', 35),\n",
       " ('HACSA', 35),\n",
       " ('SISs', 34),\n",
       " ('manafort', 34),\n",
       " ('lowlyrepub', 34),\n",
       " ('COVFEFE', 34),\n",
       " ('AW.UN', 34),\n",
       " ('SRV.UN', 34),\n",
       " ('denglish', 34),\n",
       " ('statscan', 34),\n",
       " ('https://www.theguardian.com/commentisfree/cifamerica/2011/mar/17/us-internet-morals-clumsy-spammer',\n",
       "  34),\n",
       " ('Budsie', 34),\n",
       " ('Mettrum', 34),\n",
       " ('#FakeNews', 34),\n",
       " ('Canada-U', 34),\n",
       " ('frightwing', 34),\n",
       " ('Wynnee', 34),\n",
       " ('trumpian', 34),\n",
       " ('Dilettantish', 34),\n",
       " ('BREXIT', 34),\n",
       " ('Kaneya', 34),\n",
       " ('JimJohnson', 34),\n",
       " ('ANSCA', 34),\n",
       " ('Palin-Americans', 34),\n",
       " ('jb49', 34),\n",
       " (']:', 34),\n",
       " ('Seimian', 33),\n",
       " ('Sambrailo', 33),\n",
       " ('guptas', 33),\n",
       " ('Jurbane', 33),\n",
       " ('anti-fa', 33),\n",
       " ('Bathabile', 33),\n",
       " ('http://www.fundinguniverse.com/company-histories/petro-canada-limited-history/',\n",
       "  33),\n",
       " ('duali', 33),\n",
       " ('TRUMPELTHINSKIN', 33),\n",
       " ('Shisaisama', 33),\n",
       " ('Goatie', 33),\n",
       " ('Kabib', 33),\n",
       " ('dividend-growth', 33),\n",
       " ('Ascalepius', 33),\n",
       " ('trumpkins', 33),\n",
       " ('Justerien', 33),\n",
       " ('Doltish', 33),\n",
       " ('4Me', 33),\n",
       " ('P:', 33),\n",
       " ('imua', 33),\n",
       " ('VPSOs', 33),\n",
       " ('.  .  .', 33),\n",
       " ('Yereth', 33),\n",
       " ('un-civil', 33),\n",
       " ('http://www.hc-sc.gc.ca/ahc-asc/pubs/_sites-lieux/insite/index-eng.php', 32),\n",
       " ('Binkleys', 32),\n",
       " ('Vectum', 32),\n",
       " ('u5a1a1', 32),\n",
       " ('windbourne', 32),\n",
       " ('Yonela', 32),\n",
       " ('Ranjeni', 32),\n",
       " ('Krooky', 32),\n",
       " ('making-available-for-export', 32),\n",
       " ('white-nationalist', 32),\n",
       " ('myclobutanil', 32),\n",
       " ('Librano', 32),\n",
       " ('DieterHH', 32),\n",
       " ('Bardish', 32),\n",
       " ('Poog', 32),\n",
       " ('in-my-view', 32),\n",
       " ('Jimmee', 32),\n",
       " ('PMJT', 32),\n",
       " ('CAELew', 32),\n",
       " ('CCPCs', 32),\n",
       " ('f--k', 32),\n",
       " ('clericalist', 32),\n",
       " ('Godlikeness', 32),\n",
       " ('Eco-Barbie', 32),\n",
       " ('ACfH', 32),\n",
       " ('uokalani', 32),\n",
       " ('Kaniela', 32),\n",
       " ('donʻt', 32),\n",
       " ('Mokrohisky', 32),\n",
       " ('dipnetting', 32),\n",
       " ('ᴵᴵᴵwww', 32),\n",
       " ('s--t', 32),\n",
       " ('9-1-', 32),\n",
       " ('jamesjohnson', 32),\n",
       " ('http://www.seattletimes.com/seattle-news/health/is-vancouvers-safe-drug-use-site-a-good-model-for-seattle/',\n",
       "  31),\n",
       " ('Sambraillo', 31),\n",
       " ('cANCer', 31),\n",
       " ('TigerEye', 31),\n",
       " ('Drumpfs', 31),\n",
       " ('DemoRats', 31),\n",
       " ('Ryancare', 31),\n",
       " ('McTrump', 31),\n",
       " ('11F350', 31),\n",
       " ('PHTest', 31),\n",
       " ('Trumpniks', 31),\n",
       " ('Organigram', 31),\n",
       " ('GLO-BULL', 31),\n",
       " ('rmiller', 31),\n",
       " ('Apuron', 31),\n",
       " ('wynnie', 31),\n",
       " ('KKKlown', 31),\n",
       " ('keptics', 31),\n",
       " ('UNFAO', 31),\n",
       " ('Maafala', 31),\n",
       " ('dTrump', 31),\n",
       " ('Kaahui', 31),\n",
       " ('Philando', 31),\n",
       " ('Hoffbeck', 31),\n",
       " ('http://www.adn.com/politics/article/series-conoco-employees-help-ease-oil-tax-bill-through-legislature/2013/03/28/',\n",
       "  31),\n",
       " ('railbelt', 31),\n",
       " ('Obooba', 31),\n",
       " ('obumma', 31),\n",
       " ('Peacehealth', 31),\n",
       " ('Demoski', 31),\n",
       " ('Mat-su', 31),\n",
       " ('ms.amylou', 31),\n",
       " ('ChickenLittle', 30),\n",
       " ('Heritagecare', 30),\n",
       " ('CarpaDM', 30),\n",
       " ('1-3-', 30),\n",
       " ('Sheppell', 30),\n",
       " ('Qbcoach', 30),\n",
       " ('globeandmail', 30),\n",
       " ('JimmyJ', 30),\n",
       " ('BCLiberals', 30),\n",
       " ('Fishkiller', 30),\n",
       " ('lugenpresse', 30),\n",
       " ('electrobuggies', 30),\n",
       " ('SteveVickRepoBoy', 30),\n",
       " ('ghgs', 30),\n",
       " ('Russophobic', 30),\n",
       " ('bankish', 30),\n",
       " ('BMracek', 30),\n",
       " ('slofstra', 30),\n",
       " ('unCanadian', 30),\n",
       " ('Malumba', 30),\n",
       " ('hyperloop', 30),\n",
       " ('Bumbardier', 30),\n",
       " ('1LittleBear', 30),\n",
       " ('Dennism', 30),\n",
       " ('left-lib', 30),\n",
       " ('Obummercare', 30),\n",
       " ('AUWE', 30),\n",
       " ('tRumps', 30),\n",
       " ('SpenAK', 30),\n",
       " ('no-cause', 30),\n",
       " ('.  \\n\\n.', 30),\n",
       " ('GFNC', 30),\n",
       " ('Kurdistanis', 29),\n",
       " ('Whataboutism', 29),\n",
       " ('Ocare', 29),\n",
       " ('na-na-da', 29),\n",
       " ('PNolan', 29),\n",
       " ('https://www.impeachdonaldtrumpnow.org', 29),\n",
       " ('Lysyk', 29),\n",
       " ('Dilbit', 29),\n",
       " ('whazz', 29),\n",
       " ('...\\n\\n. . .', 29),\n",
       " ('upanddown', 29),\n",
       " ('Trumpite', 29),\n",
       " ('pre-Vat', 29),\n",
       " ('Podestas', 29),\n",
       " ('another_reader', 29),\n",
       " ('neonics', 29),\n",
       " ('нe', 29),\n",
       " ('Owe-bama', 29),\n",
       " ('.  ..', 29),\n",
       " ('1.5-', 29),\n",
       " ('Conald', 29),\n",
       " ('19:3-', 29),\n",
       " ('governmint', 29),\n",
       " ('Pro-Trump', 29),\n",
       " ('cheechako', 29),\n",
       " ('Wolhforth', 29),\n",
       " ('Demopignicans', 29),\n",
       " ('sb21', 29),\n",
       " ('http://www.adn.com/sites/default/files/2015%20Legislative%20Travel%20Report.pdf',\n",
       "  29),\n",
       " ('thÈ', 28),\n",
       " ('Chappelle-Nadal', 28),\n",
       " ('JSwr', 28),\n",
       " ('deployees', 28),\n",
       " ('Muthambi', 28),\n",
       " ('Shibai-buster', 28),\n",
       " ('Kiragirl', 28),\n",
       " ('TDS-addled', 28),\n",
       " ('TheSA', 28),\n",
       " ('OCare', 28),\n",
       " ('🌮', 28),\n",
       " ('Fintrac', 28),\n",
       " ('ArchbRaymond', 28),\n",
       " ('Americanophobia', 28),\n",
       " ('Bingogate', 28),\n",
       " ('www.Nypost55.Com', 28),\n",
       " ('FakeNews', 28),\n",
       " ('HOT.UN', 28),\n",
       " ('Horgie', 28),\n",
       " ('pro-Canadian', 28),\n",
       " ('Phrogge', 28),\n",
       " ('TrumpVerified', 28),\n",
       " ('Hill-larious', 28),\n",
       " ('non-reg', 28),\n",
       " ('2.5-', 28),\n",
       " ('obooba', 28),\n",
       " ('islamaphobia', 28),\n",
       " ('.\\n\\n. . .', 28),\n",
       " ('Whitekeys', 28),\n",
       " ('ammosexual', 28),\n",
       " ('alayhimus-Salaam', 28),\n",
       " ('UNCIVIL', 28),\n",
       " ('Gray-Jackson', 28),\n",
       " ('non-Alaskans', 28),\n",
       " ('Iannarone', 28),\n",
       " ('500AF', 27),\n",
       " ('Proudpatriot', 27),\n",
       " ('Ugotta', 27),\n",
       " ('Canoutchie', 27),\n",
       " ('Bigfire', 27),\n",
       " ('Imbelli', 27),\n",
       " ('62,979', 27),\n",
       " ('steve_d', 27),\n",
       " ('Kevka', 27),\n",
       " ('SoCreds', 27),\n",
       " ('Malosi', 27),\n",
       " ('ForsterBarry', 27),\n",
       " ('AQN', 27),\n",
       " ('Oilberta', 27),\n",
       " ('NADDAWAY', 27),\n",
       " ('Reformacons', 27),\n",
       " ('Naddaway', 27),\n",
       " ('keala', 27),\n",
       " ('adscam', 27),\n",
       " ('SkinnerInDeed.com', 27),\n",
       " ('Wynnes', 27),\n",
       " ('LIEberal', 27),\n",
       " ('Haseko', 27),\n",
       " ('SuperGrid', 27),\n",
       " ('HilLIARy', 27),\n",
       " ('200,000+', 27),\n",
       " ('perdium', 27),\n",
       " ('DeMarban', 27),\n",
       " ('melanistically-challenged', 27),\n",
       " ('MaSalaam', 27),\n",
       " ('obomba', 27),\n",
       " ('Tajmahawker', 27),\n",
       " ('Drumph', 27),\n",
       " ('ocialism', 27),\n",
       " ('Trumpistas', 27),\n",
       " ('Eugeneans', 27),\n",
       " ('thiŚ', 26),\n",
       " ('Kroenkes', 26),\n",
       " ('blind-drunk', 26),\n",
       " ('OFSI', 26),\n",
       " ('Hanabooboo', 26),\n",
       " ('generationals', 26),\n",
       " ('AtheO', 26),\n",
       " ('anti-Islamophobia', 26),\n",
       " ('Howcum', 26),\n",
       " ('trumpie', 26),\n",
       " ('Christophbia', 26),\n",
       " ('https://www.youtube.com/watch?v=1o6-bi3jlxk', 26),\n",
       " ('Trumpistan', 26),\n",
       " ('PCan', 26),\n",
       " ('... \\n.', 26),\n",
       " ('formerlyrbfromcalgary', 26),\n",
       " ('Lugenpresse', 26),\n",
       " ('Ladaria', 26),\n",
       " ('Caffarra', 26),\n",
       " ('NcR', 26),\n",
       " ('Lügenpresse', 26),\n",
       " ('Reformacon', 26),\n",
       " ('it--and', 26),\n",
       " ('Trumpery', 26),\n",
       " ('Yamachika', 26),\n",
       " ('.. ..', 26),\n",
       " ('Trump-Putin', 26),\n",
       " ('Brexiters', 26),\n",
       " ('ThirdRock', 26),\n",
       " ('Murkowskis', 26),\n",
       " ('AKRR', 26),\n",
       " ('8(', 26),\n",
       " ('oilco', 26),\n",
       " ('HB44', 26),\n",
       " ('=:', 26),\n",
       " ('bump-stock', 25),\n",
       " ('Arapio', 25),\n",
       " ('SLOTER', 25),\n",
       " ('anti-protesters', 25),\n",
       " ('altrightpub', 25),\n",
       " ('FusionGPS', 25),\n",
       " ('Huckabee-Sanders', 25),\n",
       " ('ArchbFrancis', 25),\n",
       " ('Maitlin', 25),\n",
       " ('64hoo', 25),\n",
       " ('jusris', 25),\n",
       " ('Coccopalmerio', 25),\n",
       " ('HB111', 25),\n",
       " ('SidPens', 25),\n",
       " ('https://www.adn.com/business-economy/2017/02/05/energy-royalty-audits-and-adjustments-brought-alaska-an-additional-117-million-in-2016/',\n",
       "  25),\n",
       " ('jcpro', 25),\n",
       " ('CPI-common', 25),\n",
       " ('Riel_Canadian', 25),\n",
       " ('65,844', 25),\n",
       " ('alceste', 25),\n",
       " ('Murkan', 25),\n",
       " ('head-choppers', 25),\n",
       " ('pro-Liberal', 25),\n",
       " ('alt-reality', 25),\n",
       " ('Purgatrix', 25),\n",
       " ('First-Past-the-Post', 25),\n",
       " ('man-baby', 25),\n",
       " ('Archb', 25),\n",
       " ('cnpp', 25),\n",
       " ('trudeaus', 25),\n",
       " ('www.maths1951.wordpress.com', 25),\n",
       " ('Rosenlee', 25),\n",
       " ('Lebouthillier', 25),\n",
       " ('Trumpeteer', 25),\n",
       " ('Trump-supporting', 25),\n",
       " ('AToday', 25),\n",
       " ('lahui', 25),\n",
       " ('REDMAP', 25),\n",
       " ('olauloa', 25),\n",
       " ('post-Trump', 25),\n",
       " ('sb91', 25),\n",
       " ('ACFH', 25),\n",
       " ('PFER', 25),\n",
       " ('Spendowitz', 25),\n",
       " ('http://www.jobs.factoryofincome.com', 25),\n",
       " ('Repignicrats', 25),\n",
       " ('non-Trump', 25),\n",
       " ('Hillcorp', 25),\n",
       " ('Susitna-Watana', 25),\n",
       " ('wweek', 25),\n",
       " ('Rthur', 24),\n",
       " ('TomQuinn', 24),\n",
       " ('indubitablysnarky', 24),\n",
       " ('israheil', 24),\n",
       " ('DPEB', 24),\n",
       " ('Smeeagain', 24),\n",
       " ('Kizsla', 24),\n",
       " ('https://www.youtube.com/watch?v=SXxHfb66ZgM', 24),\n",
       " ('Hatefi', 24),\n",
       " ('Hanabata', 24),\n",
       " ('Republicare', 24),\n",
       " ('MillionMonkeys', 24),\n",
       " ('cajaybird', 24),\n",
       " ('dragoninwater', 24),\n",
       " ('DemLibs', 24),\n",
       " ('Con-quer', 24),\n",
       " ('Smarmo', 24),\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_oov2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexion of the out of vocabulary dictionary has changed significantly. Now we generally see out random collections of punctuation fill the upper ends of the list. In addition to this, we note many slang/irregular words that are often creations as part of the online comment environment such as 'SJW','Drumpf','Trumpian. The latter two are part of an interesting trend within the dictionary of more recent political developments such as 'Brexit' being OOV. It would be interesting to revist the word embeddings at a later stage to see if we can use/train a word embedding set that is more up to date to account for this.\n",
    "\n",
    "To summarize, for the data we will feed into the Neural Network we have:\n",
    " * Deleted OOV symbols\n",
    " * Removed possessive apostrophes\n",
    " * Tokenized words\n",
    " \n",
    "We will apply the exact same process to the test set and save down both files.\n",
    "We have one final set before saving the files, which is to drop the oclumns we won't be using and binarize (using boolean) the ones we will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess_nn(texts):\n",
    "    # Drop symbols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n",
    "for col in identity_columns + ['target']:\n",
    "    train_df[col] = np.where(train_df[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target'] = np.where(train_df['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>target_class</th>\n",
       "      <th>comment_text_clean</th>\n",
       "      <th>comment_text_clean2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>This is so cool. It  like,  would you want you...</td>\n",
       "      <td>[This, is, so, cool, ., It, like, ,, would, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>[Thank, you, !, !, This, would, make, my, life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>[This, is, such, an, urgent, design, problem, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Is this something I ll be able to install on m...</td>\n",
       "      <td>[Is, this, something, I, ll, be, able, to, ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>1</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>[haha, you, guys, are, a, bunch, of, losers, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  \\\n",
       "0  59848       0  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849       0  Thank you!! This would make my life a lot less...   \n",
       "2  59852       0  This is such an urgent design problem; kudos t...   \n",
       "3  59855       0  Is this something I'll be able to install on m...   \n",
       "4  59856       1               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0      0        0   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0      0        0   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0      0        0   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0      0        0   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0      0        0   \n",
       "\n",
       "   ...  wow  sad  likes  disagree  sexual_explicit  identity_annotator_count  \\\n",
       "0  ...    0    0      0         0              0.0                         0   \n",
       "1  ...    0    0      0         0              0.0                         0   \n",
       "2  ...    0    0      0         0              0.0                         0   \n",
       "3  ...    0    0      0         0              0.0                         0   \n",
       "4  ...    0    0      1         0              0.0                         4   \n",
       "\n",
       "   toxicity_annotator_count  target_class  \\\n",
       "0                         4             0   \n",
       "1                         4             0   \n",
       "2                         4             0   \n",
       "3                         4             0   \n",
       "4                        47             1   \n",
       "\n",
       "                                  comment_text_clean  \\\n",
       "0  This is so cool. It  like,  would you want you...   \n",
       "1  Thank you!! This would make my life a lot less...   \n",
       "2  This is such an urgent design problem; kudos t...   \n",
       "3  Is this something I ll be able to install on m...   \n",
       "4               haha you guys are a bunch of losers.   \n",
       "\n",
       "                                 comment_text_clean2  \n",
       "0  [This, is, so, cool, ., It, like, ,, would, yo...  \n",
       "1  [Thank, you, !, !, This, would, make, my, life...  \n",
       "2  [This, is, such, an, urgent, design, problem, ...  \n",
       "3  [Is, this, something, I, ll, be, able, to, ins...  \n",
       "4    [haha, you, guys, are, a, bunch, of, losers, .]  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our NN data we are going to drop uneeded columns. THe decision has been made to only keep the tokenized comment col\n",
    "# Ultimately given the size of the dataset, word embeddings, and the complexity of the model, we want to reduce memory\n",
    "# requirements whereever possible\n",
    "keep_cols = ['id','target', 'comment_text_clean2',\n",
    "             'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish','muslim', 'black', \n",
    "             'white', 'psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.loc[:,keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_df to file\n",
    "train_df.to_csv('data/train_for_nn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all the above steps to the test dataset\n",
    "\n",
    "# SYMBOL DROPPING\n",
    "clean_vocab = build_vocab(test_df['comment_text'], verbose=False)\n",
    "clean_vocab_chars = ''.join([char for char in clean_vocab if len(char) == 1])\n",
    "clean_vocab_symbols = ''\n",
    "clean_vocab_symbols = ''.join([char for char in clean_vocab_chars if not char in non_symbols])\n",
    "test_df['comment_text_clean'] = test_df['comment_text'].apply(lambda x: x.translate(symb_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSSESSIVE APOSTROPHE\n",
    "test_df['comment_text_clean'] = test_df['comment_text_clean'].apply(lambda x: re.sub(\"'s?\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE\n",
    "test_df['comment_text_clean2'] = test_df['comment_text_clean'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 52.00% of vocab\n",
      "Found embeddings for  99.50% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab_test = build_vocab2(test_df['comment_text_clean2'])\n",
    "glove_oov_test = check_coverage(vocab2, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the identity\n",
    "for col in identity_columns:\n",
    "    test_df[col] = np.where(test_df[col] >= 0.5, True, False)\n",
    "    \n",
    "# Binarize the target to 1,0\n",
    "test_df['toxicity'] = np.where(test_df['toxicity'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our NN data we are going to drop uneeded columns. THe decision has been made to only keep the tokenized comment col\n",
    "# Ultimately given the size of the dataset, word embeddings, and the complexity of the model, we want to reduce memory\n",
    "# requirements whereever possible\n",
    "keep_cols = ['id','toxicity' 'comment_text_clean2',\n",
    "             'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish','muslim', 'black', \n",
    "             'white', 'psychiatric_or_mental_illness']\n",
    "test_df = test_df.loc[:,keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename toxicity to target\n",
    "test_df.rename({'toxicity': 'target'},axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "test_df.to_csv('data/test_for_nn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
