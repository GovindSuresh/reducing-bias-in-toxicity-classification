{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "\n",
    "Preprocessing for NLP is a very task dependent process. The steps we take are dependent on the aims and methods we intend to apply at the modelling stage, as such there may be different paths to take.\n",
    "\n",
    "As a reminder, the task of our study is to be able to train models that are able to effectively classify text comments as Toxic, while minimising bias. Key to doing this will be to train a model that is able to understand contextual relationships between words, but also perhaps to understand the contextual impact of punctuation at a certain point. To help understand this, consider the impact of exclamation points to your view of the context of a sentence. \n",
    "\n",
    "Standard NLP preprocess workflows would generally remove punctuation given that classic methods of word-embedding such as TF-IDF do not gain meaning from these outside of their counts. However the use of pre-trained word-embeddings such as fasttext, GloVE, word2vec, and others, often contain vector representations of punctuation that has been learned by studying their relationships to other words. As a result, we could now skip the removal of punctuation.\n",
    "\n",
    "### Our Method:\n",
    "\n",
    "For this project we are following two paths. \n",
    "\n",
    "#### Traditional ML\n",
    "We will be training a series of classic ML classifiers such as SVM, Logistic Regression and Random Forest. We will embed the words using TF-IDF and therefore we will use a traditional NLP workflow.\n",
    "\n",
    " 1. Remove punctuation, convert to lower case\n",
    " 2. Tokenization\n",
    " 3. Removal of stop words\n",
    " 4. Stemming/Lemmatization\n",
    " \n",
    "We will use tools from NLTK/Spacy to achieve this, with adjustments for quirks in our data. \n",
    "\n",
    "#### Neural Networks with GloVe\n",
    "We will also be training a neural network model which takes advantage of glove word embeddings. As such we will not be carrying out the standard work flow. Instead, we will begin by comparing the number of tokens in our dataset vocabulary to the glove word embeddings. We will remove as many OOV words/symbols as we can. \n",
    "\n",
    "We will then attempt to correct issues such as misspellings that are causing words to not match with the word-embeddings. We will leave in punctuation that has a vector representation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gov/anaconda3/envs/capstone/lib/python3.7/site-packages/tqdm/std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import contractions\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the cleaned train and cleaned+concatenated test datasets. \n",
    "train_df = pd.read_csv('data/train_clean.csv')\n",
    "test_df = pd.read_csv('data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the first \n",
    "train_df = train_df.iloc[:,1:]\n",
    "test_df = test_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning for ML classifiers\n",
    "\n",
    "We need to explore the text to get a better idea of what we need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character count - pre any pre-processing\n",
    "len_list = []\n",
    "for i in train_df['comment_text']:\n",
    "    len_list.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot histogram\n",
    "plt.figure()\n",
    "plt.hist(len_list, bins=40)\n",
    "plt.xlabel('Frequency bins')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Character Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now lets look at the number of WORDS in each comment\n",
    "words = train_df['comment_text'].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\n",
    "train_df['words'] = words\n",
    "words = train_df.loc[train_df['words']]['words']\n",
    "plt.figure()\n",
    "plt.hist(words, bins=40)\n",
    "plt.xlabel('Frequency bins')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of characters, we see a somewhat bimodal distribution, with the majority of comments being very short, but another peak appearing around the 1000 charachter length. This may suggest that certain publications had a default max charachter limit set at 1000.\n",
    "\n",
    "In terms of actual word count, we see a more standard positive skew to the data.\n",
    "\n",
    "This is useful information to know given that we will likely need to restrict the length of our sequences before feeding them into models to account for memory constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " min: 1          25%: 94.0          50%: 202.0          75%: 414.0          max: 1906          mean: 297.2343266067327\n"
     ]
    }
   ],
   "source": [
    "#summary statistics for characters\n",
    "len_list = np.asarray(len_list)\n",
    "mean = len_list.mean()\n",
    "median = np.median(len_list)\n",
    "upper_quart = np.percentile(len_list, 75)\n",
    "lower_quart = np.percentile(len_list, 25)\n",
    "min_val = len_list.min()\n",
    "max_val = len_list.max()\n",
    "print(f' min: {min_val} \\\n",
    "         25%: {lower_quart} \\\n",
    "         50%: {median} \\\n",
    "         75%: {upper_quart} \\\n",
    "         max: {max_val} \\\n",
    "         mean: {mean}'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find comments with max length = 1000\n",
    "df = train_df\n",
    "# create new column with comment length\n",
    "df['comment_len'] = len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>target_class</th>\n",
       "      <th>words</th>\n",
       "      <th>comment_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>240627</td>\n",
       "      <td>0</td>\n",
       "      <td>BJ, your suggestion that low income residents ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>240648</td>\n",
       "      <td>0</td>\n",
       "      <td>Resolution 08-3945 presented to Metro council ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>240802</td>\n",
       "      <td>0</td>\n",
       "      <td>Quoting Joni Mitchell, \"Don't it always seem t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>241094</td>\n",
       "      <td>0</td>\n",
       "      <td>Is it not in the best interest of the OLCC to ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>241112</td>\n",
       "      <td>0</td>\n",
       "      <td>While I can't speak to the factual accuracy in...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803524</th>\n",
       "      <td>6331991</td>\n",
       "      <td>0</td>\n",
       "      <td>As I've said before, Island Air was operating ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803810</th>\n",
       "      <td>6332378</td>\n",
       "      <td>0</td>\n",
       "      <td>Oh I read it!\\n\\n\\nAnd again, teach Latin and ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804543</th>\n",
       "      <td>6333400</td>\n",
       "      <td>0</td>\n",
       "      <td>This idea that an old universe is at odds with...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804766</th>\n",
       "      <td>6333742</td>\n",
       "      <td>0</td>\n",
       "      <td>Paul:  The whole topic of graduation rates is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804850</th>\n",
       "      <td>6333907</td>\n",
       "      <td>0</td>\n",
       "      <td>Every time there are testimonies, like this, I...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10709 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  target                                       comment_text  \\\n",
       "516       240627       0  BJ, your suggestion that low income residents ...   \n",
       "529       240648       0  Resolution 08-3945 presented to Metro council ...   \n",
       "606       240802       0  Quoting Joni Mitchell, \"Don't it always seem t...   \n",
       "820       241094       0  Is it not in the best interest of the OLCC to ...   \n",
       "834       241112       0  While I can't speak to the factual accuracy in...   \n",
       "...          ...     ...                                                ...   \n",
       "1803524  6331991       0  As I've said before, Island Air was operating ...   \n",
       "1803810  6332378       0  Oh I read it!\\n\\n\\nAnd again, teach Latin and ...   \n",
       "1804543  6333400       0  This idea that an old universe is at odds with...   \n",
       "1804766  6333742       0  Paul:  The whole topic of graduation rates is ...   \n",
       "1804850  6333907       0  Every time there are testimonies, like this, I...   \n",
       "\n",
       "         severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "516                  0.0      0.0              0.0     0.0     0.0      0   \n",
       "529                  0.0      0.0              0.0     0.0     0.0      0   \n",
       "606                  0.0      0.0              0.0     0.0     0.0      0   \n",
       "820                  0.0      0.0              0.0     0.0     0.0      0   \n",
       "834                  0.0      0.0              0.0     0.0     0.0      0   \n",
       "...                  ...      ...              ...     ...     ...    ...   \n",
       "1803524              0.0      0.0              0.0     0.0     0.0      0   \n",
       "1803810              0.0      0.0              0.0     0.0     0.0      0   \n",
       "1804543              0.0      0.0              0.1     0.1     0.0      0   \n",
       "1804766              0.0      0.0              0.0     0.0     0.0      0   \n",
       "1804850              0.0      0.0              0.0     0.0     0.0      0   \n",
       "\n",
       "         atheist  ...  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "516            0  ...    0    0      0         0              0.0   \n",
       "529            0  ...    0    0      1         0              0.0   \n",
       "606            0  ...    0    0      0         0              0.0   \n",
       "820            0  ...    0    0      0         0              0.0   \n",
       "834            0  ...    0    0      4         0              0.0   \n",
       "...          ...  ...  ...  ...    ...       ...              ...   \n",
       "1803524        0  ...    0    0      0         0              0.0   \n",
       "1803810        0  ...    0    0      1         0              0.0   \n",
       "1804543        0  ...    0    0      0         0              0.0   \n",
       "1804766        0  ...    0    0      0         0              0.0   \n",
       "1804850        0  ...    0    0      0         0              0.0   \n",
       "\n",
       "         identity_annotator_count  toxicity_annotator_count  target_class  \\\n",
       "516                             4                         4             0   \n",
       "529                             0                         4             0   \n",
       "606                             0                         4             0   \n",
       "820                             0                         4             0   \n",
       "834                             0                         4             0   \n",
       "...                           ...                       ...           ...   \n",
       "1803524                         0                         4             0   \n",
       "1803810                         0                         4             0   \n",
       "1804543                         0                        10             0   \n",
       "1804766                         0                         4             0   \n",
       "1804850                        10                         4             0   \n",
       "\n",
       "         words  comment_len  \n",
       "516        185         1000  \n",
       "529        175         1000  \n",
       "606        155         1000  \n",
       "820        192         1000  \n",
       "834        175         1000  \n",
       "...        ...          ...  \n",
       "1803524    176         1000  \n",
       "1803810    171         1000  \n",
       "1804543    182         1000  \n",
       "1804766    160         1000  \n",
       "1804850    167         1000  \n",
       "\n",
       "[10709 rows x 47 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# boolean index for comments of 1000\n",
    "df[df['comment_len'] == 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21     2675\n",
       "54     2650\n",
       "13     1334\n",
       "53     1173\n",
       "102     810\n",
       "22      758\n",
       "55      494\n",
       "100     188\n",
       "105     149\n",
       "66      101\n",
       "43      100\n",
       "6        64\n",
       "93       33\n",
       "73       32\n",
       "87       28\n",
       "Name: publication_id, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the publication_id column and get value counts\n",
    "df[df['comment_len'] == 1000].loc[:,'publication_id'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54     627278\n",
       "21     377686\n",
       "102    213846\n",
       "13     156682\n",
       "55     133464\n",
       "53     129243\n",
       "22      54767\n",
       "105     38934\n",
       "100     23441\n",
       "43      10952\n",
       "6       10824\n",
       "66       8095\n",
       "93       3597\n",
       "52       2323\n",
       "87       1692\n",
       "Name: publication_id, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check total counts for publication id \n",
    "train_df['publication_id'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the total number of rows for the publication ids with the highest occurance of 1000 length comments, we can't say there appears to be a trend here. We may just have some form of default max length. In any case, we will most likely have to set a max length for these comments when training our neural network model due to memory constraints. We should still capture the majority of comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-processing:\n",
    "\n",
    "We will start with a baseline approach of:\n",
    "* Casing to lower\n",
    "* Expanding contractions\n",
    "* Tokenizing - Uses nltk.TweetTokenizer (with reduce_len = true)\n",
    "* Removing Stop words Uses nltk.English stop words\n",
    "* Lemmatizing - nltk. wordnet lemmatizer on adverbs, adjectives, verbs and nouns\n",
    "\n",
    "This is a standard workflow for pre-processing text. We will experiment with results if we remove some of these\n",
    "\n",
    "Ultimately this is a difficult task given these are online comments. We will likely see numerous misspellings that cause the tools we use to miss certain issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case.\n",
    "df['cleaned_comment'] = df['comment_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use contractions library to expand contractions\n",
    "# Contraction package code can be found here https://github.com/kootenpv/contractions, contractions list can be found below\n",
    "# https://github.com/kootenpv/contractions/blob/master/contractions/__init__.py\n",
    "# The package is very useful for expanding contractions and is good at doing so for slang \n",
    "\n",
    "# We could also implement this using a dictionary built from \n",
    "# https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "\n",
    "def contraction_expand(text):\n",
    "    return contractions.fix(text)\n",
    "#teststr = \"hi i'm the coolest cat yall, y'all, there's isn't\"\n",
    "\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: contraction_expand(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you!! This would make my life a lot less anxiety-inducing. Keep it up, and don't let anyone get in your way!\n",
      "thank you!! this would make my life a lot less anxiety-inducing. keep it up, and do not let anyone get in your way!\n",
      "\n",
      "And what are your qualifications as an expert in world monetary policies?  Not trying to insult you sir but your handle gives no clue of who you are or what are your talents.   I'm certain that Krugman circulates in elite company of many decorated economist- some who certainly challenge his theories, some who are supportive.   \n",
      "\n",
      "I find him interesting but still worry about what line might represent unsustainable debt.   *I'm strongly against a balanced budget amendment, no private institution would work under that constraint - but it does seem we need some debt limits.   I am no economist, but I'm not ignorant on all financial topics either.   Not a Macro guy at all\n",
      "\n",
      "Peace\n",
      "\n",
      "and what are your qualifications as an expert in world monetary policies?  not trying to insult you sir but your handle gives no clue of who you are or what are your talents.   I am certain that krugman circulates in elite company of many decorated economist- some who certainly challenge his theories, some who are supportive.   \n",
      "\n",
      "i find him interesting but still worry about what line might represent unsustainable debt.   *I am strongly against a balanced budget amendment, no private institution would work under that constraint - but it does seem we need some debt limits.   i am no economist, but I am not ignorant on all financial topics either.   not a macro guy at all\n",
      "\n",
      "peace\n",
      "\n",
      "Mmmm, only for short-term investors. The market's perception of value or price of shares may (or may not) increase, but the actual value of shares stems from the value of the underlying business which does not change one iota due to a stock split. The only instance where the \"value\" of a stock changes after a split is if it is traded. Why would you do that when you can realize further gains from holding a stock, particularly a bank stock, for long periods and defer taxes while doing so?\n",
      "\n",
      "mmmm, only for short-term investors. the market's perception of value or price of shares may (or may not) increase, but the actual value of shares stems from the value of the underlying business which does not change one iota due to a stock split. the only instance where the \"value\" of a stock changes after a split is if it is traded. why would you do that when you can realize further gains from holding a stock, particularly a bank stock, for long periods and defer taxes while doing so?\n"
     ]
    }
   ],
   "source": [
    "# Quick check of some comments\n",
    "print(df['comment_text'].loc[1])\n",
    "print(df['cleaned_comment'].loc[1])\n",
    "print('')\n",
    "print(df['comment_text'].loc[204000])\n",
    "print('')\n",
    "print(df['cleaned_comment'].loc[204000])\n",
    "\n",
    "print('')\n",
    "print(df['comment_text'].loc[565934])\n",
    "print('')\n",
    "print(df['cleaned_comment'].loc[565934])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we can see that we have successfuly converted the strings to lower case and appear to have expanded out contractions, certainly we have captured common cases such as \"I'm\". We will now progress to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZATION\n",
    "\n",
    "# We will use the TweetTokenizer from nltk.\n",
    "# Our data has been taken from online comments, which likely share very similar traits as a Tweet, such as hashtags and\n",
    "# additional punctuation for emphasis. This tokenizer should more effectively deal with them.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(reduce_len=True) # Set reducelen to true to somewhat reduce lengths of words like 'waaayyy'\n",
    "\n",
    "#apply tokenizer using lambda function \n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# remove stop words via lambda function and list comprehension\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Punctuation\n",
    "# We are taking advantage of the translate functionality of the string function. \n",
    "\n",
    "# first we make a translation table - what we are doing here is actually making an empty table. The third argument is for\n",
    "# charachters which will be mapped to None if found. As such by putting in string.punctuation we are creating a translate\n",
    "# table which will set punctuation to none \n",
    "punc_table = str.maketrans('', '', string.punctuation)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lambda x: [item.translate(punc_table) for item in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_text       Thank you!! This would make my life a lot less...\n",
      "cleaned_comment    [thank, , , would, make, life, lot, less, anxi...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# quick check\n",
    "print(df.loc[1,['comment_text', 'cleaned_comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 56s, sys: 6min 8s, total: 20min 4s\n",
      "Wall time: 31min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# lemmatize words using the WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Lemmatizer works on part of speech words, so we need to run this over the various pos,\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text_noun(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='n') for w in text]\n",
    "\n",
    "def lemmatize_text_verb(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='v') for w in text]\n",
    "def lemmatize_text_adj(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='a') for w in text]\n",
    "def lemmatize_text_adv(text):\n",
    "    return [lemmatizer.lemmatize(w, pos='r') for w in text]\n",
    "\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_noun)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_verb)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_adj)\n",
    "df['cleaned_comment'] = df['cleaned_comment'].apply(lemmatize_text_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now the end of our standard pipeline, we have put all the above steps together in a function below which we can apply to the text. THis will also allow us to ensure we treat the test set in the exact same way as we have done for the training set.\n",
    "\n",
    "Before we move on let's see our remaining vocabulary present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "df['cleaned_untoken'] = df['cleaned_comment'].apply(lambda x: detokenizer.detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define build vocab function\n",
    "def build_vocab(texts, verbose= True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that will iterate over the dataframe rows and builds a dictionary for the words as well as their count\n",
    "    Uses the tqdm library to create a progressbar given the size of task\n",
    "    \n",
    "    PARAMETERS:\n",
    "    texts: pandas series with comment to parse\n",
    "    verbose: Set to false for no progress bar\n",
    "    \n",
    "    RETURNS\n",
    "    vocab: Dictionary of words and their counts\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for i in tqdm(range(len(texts)), disable = (not verbose)):\n",
    "        for word in texts.loc[i]:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "        \n",
    "    \n",
    "    vocab = sorted(vocab.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['cleaned_comment'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary list at this point is 455,708, which remains a large number. We will try fit models with this vocabulary, but if this causes excessively long training times or other issues, we will consider dropping the rarest words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRE-PROCESS PIPELINE FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "def text_cleaner(df, col_name, clean_col_name):\n",
    "   \n",
    "\n",
    "    # Lemmatize functions to be called lated\n",
    "    # Lemmatize nouns\n",
    "    def lemmatize_text_noun(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='n') for w in text]\n",
    "    \n",
    "    # Lemmatize verbs\n",
    "    def lemmatize_text_verb(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='v') for w in text]\n",
    "    # Lemmatize adjectives\n",
    "    def lemmatize_text_adj(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='a') for w in text]\n",
    "\n",
    "    # Lemmatize adverbs\n",
    "    def lemmatize_text_adv(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='r') for w in text]\n",
    "    \n",
    "    # Expand contraction method\n",
    "    def contraction_expand(text):\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    # To lower case.\n",
    "    df[clean_col_name] = df[col_name].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Expand contractions\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: contraction_expand(x))\n",
    "    \n",
    "    #Tokenize:\n",
    "    tokenizer = TweetTokenizer(reduce_len=True)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: tokenizer.tokenize(x))\n",
    "   \n",
    "    \n",
    "    #Remove Stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    #Delete punctuation\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item.translate(punc_table) for item in x])\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_noun)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_verb)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adj)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adv)\n",
    "    \n",
    "    \n",
    "    return None\n",
    "\n",
    "def detokenizer(df, col_name, clean_col_name):\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    df[clean_col_name+'_detokenize'] = df[clean_col_name].apply(lambda x: detokenizer.detokenize(x))\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cleaner func on train data\n",
    "train_df2 = train_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3da4b3c2b876>\u001b[0m in \u001b[0;36mtext_cleaner\u001b[0;34m(df, col_name, clean_col_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#Tokenize:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_col_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_col_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4043\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4045\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3da4b3c2b876>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#Tokenize:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_col_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclean_col_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/nltk/tokenize/casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0msafe_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHANG_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\1\\1\\1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Tokenize:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWORD_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Possibly alter the case, but avoid changing emoticons like :D into :d:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_case\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the cleaner func on train data\n",
    "text_cleaner(train_df2, 'comment_text', 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [cool, , like, , , would, want, mother, read, ...\n",
       "1          [thank, , , would, make, life, lot, le, anxiet...\n",
       "2          [urgent, design, problem, , kudos, take, , imp...\n",
       "3           [something, I, able, install, site, , release, ]\n",
       "4                                [haha, guy, bunch, loser, ]\n",
       "                                 ...                        \n",
       "1804869    [maybe, tax, , thing, , would, collect, produc...\n",
       "1804870    [call, people, still, think, divine, role, cre...\n",
       "1804871    [thank, , , , right, wrong, , , , follow, advice]\n",
       "1804872    [anyone, quote, follow, exchange, , even, apoc...\n",
       "1804873    [student, define, ebd, legally, disable, eligi...\n",
       "Name: comment_text_clean, Length: 1804874, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2['comment_text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [cool, , like, , , would, want, mother, read, ...\n",
       "1          [thank, , , would, make, life, lot, le, anxiet...\n",
       "2          [urgent, design, problem, , kudos, take, , imp...\n",
       "3           [something, I, able, install, site, , release, ]\n",
       "4                                [haha, guy, bunch, loser, ]\n",
       "                                 ...                        \n",
       "1804869    [maybe, tax, , thing, , would, collect, produc...\n",
       "1804870    [call, people, still, think, divine, role, cre...\n",
       "1804871    [thank, , , , right, wrong, , , , follow, advice]\n",
       "1804872    [anyone, quote, follow, exchange, , even, apoc...\n",
       "1804873    [student, define, ebd, legally, disable, eligi...\n",
       "Name: cleaned_comment, Length: 1804874, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Both ways appear to have achieved the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text2(sentence):\n",
    "    \n",
    "    # create stopwords list and lemmatizer\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    #split words:\n",
    "    wordlist = sentence.split(' ')\n",
    "    #print(wordlist)\n",
    "    for word in wordlist:\n",
    "        # cast to lower\n",
    "        word = word.lower()\n",
    "        \n",
    "        \n",
    "        # Remove stop words\n",
    "        if word in stop_words:\n",
    "          \n",
    "            continue\n",
    "        \n",
    "        #lemmatize word\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos='n') #noun\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos='v') #verb\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos='a') #adj\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos='r') #adverd\n",
    "       \n",
    "        word.translate(punc_table)\n",
    "        \n",
    "        wordlist.append(lemmatized_word)\n",
    "        \n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69 µs, sys: 1e+03 ns, total: 70 µs\n",
      "Wall time: 76.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = df['comment_text']\n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(min_df=5, tokenizer=clean_text2)\n",
    "\n",
    "vect.fit(X_train)\n",
    "\n",
    "#X_train = vect.transform(X_train)\n",
    "#X_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank\n",
      "you!!\n",
      "this\n",
      "would\n",
      "make\n",
      "my\n",
      "life\n",
      "a\n",
      "lot\n",
      "less\n",
      "anxiety-inducing.\n",
      "keep\n",
      "it\n",
      "up,\n",
      "and\n",
      "don't\n",
      "let\n",
      "anyone\n",
      "get\n",
      "in\n",
      "your\n",
      "way!\n",
      "thank\n"
     ]
    }
   ],
   "source": [
    "for i in clean:\n",
    "    i = i.lower()\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank',\n",
       " 'you!!',\n",
       " 'This',\n",
       " 'would',\n",
       " 'make',\n",
       " 'my',\n",
       " 'life',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'less',\n",
       " 'anxiety-inducing.',\n",
       " 'Keep',\n",
       " 'it',\n",
       " 'up,',\n",
       " 'and',\n",
       " \"don't\",\n",
       " 'let',\n",
       " 'anyone',\n",
       " 'get',\n",
       " 'in',\n",
       " 'your',\n",
       " 'way!',\n",
       " 'thank']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean2 = clean_text2(X_train[1])\n",
    "clean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank',\n",
       " '',\n",
       " '',\n",
       " 'would',\n",
       " 'make',\n",
       " 'life',\n",
       " 'lot',\n",
       " 'le',\n",
       " 'anxietyinducing',\n",
       " '',\n",
       " 'keep',\n",
       " '',\n",
       " 'let',\n",
       " 'anyone',\n",
       " 'get',\n",
       " 'way',\n",
       " '']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2['comment_text_clean'].loc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(sentence):\n",
    "\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "    \n",
    "    for word in listofwords:\n",
    "        # Remove stopwords\n",
    "        if word in ENGLISH_STOP_WORDS: continue\n",
    "        # Stem words\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        for punctuation_mark in string.punctuation:\n",
    "            # Remove punctuation and set to lower case\n",
    "            stemmed_word = stemmed_word.replace(punctuation_mark, '').lower()\n",
    "        listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare function output to step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now check for pluralised/contracted words in case the lemmatizer failed to pick these up.\n",
    "i = range(0,100)\n",
    "newlist = []\n",
    "for i in range(0,100):\n",
    "    \n",
    "        \n",
    "    comment = df['cleaned_comment'].loc[i]\n",
    "    for i in comment:\n",
    "        #print(i)\n",
    "        t = i.find(\"'s\")\n",
    "        \n",
    "        if t == -1:\n",
    "            continue\n",
    "        else:\n",
    "            x = i\n",
    "            newlist.append(x)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# untokenize the cleaned_comment column\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "df['cleaned_untoken'] = df['cleaned_comment'].apply(lambda x: detokenizer.detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_untoken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for Glove word embeddings\n",
    "\n",
    "For our Neural Network models we will be using Glove pre-trained word-embeddings. Key to the success of the model will therefore be processing our text to ensure we have as high a vocabulary coverage as possible. This means we will have to correct as many miss-spellings and contractions as we can.\n",
    "\n",
    "We will use the Glove Common Crawl 840B 300dim vectors. We believe this will achieve superior accuracy over using the smaller 6B set which was trained on Wikipedia only. Remember, our comments come from various online sources so will likely use language differently than Wikipedia which is more formalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the word embeddings\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "import operator\n",
    "\n",
    "\n",
    "GLOVE_FILE = 'embeds/glove.840B.300d.txt'\n",
    "\n",
    "\n",
    "def load_embed(file):\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float16')[:1]\n",
    "    \n",
    "        \n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(file))\n",
    "        \n",
    "    return embedding_index\n",
    "\n",
    "def build_vocab(texts, verbose =  True):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print(f'Found embeddings for {len(a) / len(vocab):.2%} of vocab')\n",
    "    print(f'Found embeddings for  {k / (k + i):.2%} of all text')\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "\n",
    "def vocab_check_coverage(train, col_name):\n",
    "    #df = pd.concat([train, test]).reset_index(drop=True)\n",
    "    \n",
    "    vocab = build_vocab(df[col_name])\n",
    "    \n",
    "    oov_glove = check_coverage(vocab, glove)\n",
    "    oov_glove = {\"oov_rate\": len(oov_glove) / len(vocab), 'oov_words': oov_glove}\n",
    "    \n",
    "    \n",
    "    return oov_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "glove = load_embed(GLOVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Lets check the OOV for our pre-processed df and for df_train\n",
    "glove_oov = vocab_check_coverage(train_df, 'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "glove_oov_cleaned = vocab_check_coverage(df, 'cleaned_untoken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we were only able to find 15.82% of the words in our vocab within the embedding. After we applied some basic pre-processing to the text we were able to push this up to 24.76%, which is certainly an improvement, but is still pretty low. Let us investigate the glove_oov_cleaned dictionary to see what kind of words we are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_oov_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that a number of the most common missing vocab words are plurals, such as people's and who's. This is because we applied the lemmatizer with the pos tag as 'n' only, we should try this with the other POS tags. In addition, there appear to be a number of incorrect grammar examples, emojis, and also names. Interestingly with the names, we do not appear to be missing the name itself, i.e Trump or Obama, but rather missing the possessive case, e.g: Trump's or Obama's. We could therefore see if we can lemmatize/stem these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Emojis and other symbols\n",
    "Let us first deal with emojis and symbols. Emojis can contain useful information and we may have some contained in our text given this was taken from a common crawl. We will try and preserve emojis we have glove embeddings for and delete ones which we don't. We will do the same for other symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get all the characters from our vocabulary\n",
    "# we can use our build_vocab method from before \n",
    "clean_vocab = build_vocab(df['cleaned_untoken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use list comprehension to pull out characters\n",
    "# Instead of generating a large list, we append each character into a long string which allows us to view them easier, \n",
    "# We have added two spaces per character to spread these out.\n",
    "# The list comprehension simply takes a char for each char in our dict, if the length is 1 (i.e a char)\n",
    "clean_vocab_chars = '  '.join([char for char in clean_vocab if len(char) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now make a filter to remove all regular letters and numbers\n",
    "\n",
    "# We can classify symbols as anything which are not latin letters. \n",
    "# The string package contains a list of common ascii characters and digits \n",
    "# https://docs.python.org/3/library/string.html\n",
    "\n",
    "# string.ascii_letters includes lowercase and upper case. We define a filter below \n",
    "non_symbols = string.digits + string.ascii_letters\n",
    "\n",
    "# We could also include common latin-based languages charahcters (western european).\n",
    "# This list was taken from Latin-1 charset table at: https://cs.stanford.edu/people/miles/iso8859.html#ISO\n",
    "# There are likely more that we could exclude\n",
    "latin_based_char = 'ÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿ'\n",
    "\n",
    "# add this to filter\n",
    "non_symbols = non_symbols + latin_based_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new string of symbols which has been filtered\n",
    "clean_vocab_symbols = ''\n",
    "clean_vocab_symbols = ''.join([char for char in clean_vocab_chars if not char in non_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_vocab_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of punctuation, emojis make up the majority of symbols in our dataset vocabulary. In addition to this we have a number of characters from different encodings. We can now compare this set with what we have available in the word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same as above \n",
    "# The below code iterates through the \n",
    "glove_char = ' '.join([char for char in glove if len(char) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us filter out the symbols\n",
    "# Similar list_comprehension as above, but if statement to check c is  not in our filter \n",
    "glove_symbols = ''.join([char for char in glove_char if not char in non_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we actually have a number of word embeddings for symbols, and most importantly, we have word embeddings for punctuation marks including exclamation points. Recall that we mentioned earlier about how some punctuation may acutally help convey meaning. As GloVe comes with these we are able to keep the punctuation marks if needed. \n",
    "\n",
    "However, it is clear that we are likely going to have to drop a significant number of the emojis as the word-embedding set we have chosen has very few. Perhaps there are other word-embedding sets which have been trained including emojis. \n",
    "\n",
    "For the purposes of our neural network model, we will delete the symbols with no word-embeddings. Any word/symbol with no embedding gets automatically assignedd a OOV token which translates to a value of 0. As such we will not gain no information from including these symbols and instead will benefit from the fact that we have to process less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new string using same method above of dataset vocab symbols not in glove symbols\n",
    "\n",
    "drop_symbol = ' '.join([char for char in clean_vocab_symbols if char not in glove_symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we will be dropping 514 symbols from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
