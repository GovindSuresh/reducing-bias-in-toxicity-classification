{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for our work is to not only train a model to identify toxic comments, but to do so while reducing bias.\n",
    "Bias in this task can be viewed as the situation where certain identies such as 'Black', 'Muslim', 'Gay' e.t.c, begin triggering toxic classification for comments they are in, even when the comment is actually positive. This is a key issue in toxic comment classification. \n",
    "\n",
    "The goal of the [jigsaw unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) Kaggle challenge was to reduce this bias via a newly developed submetric which we have defined below.\n",
    "\n",
    "**Note: The goal of the model is simply to predict the toxicity score of a model.** \n",
    "\n",
    "The bias weighted ROC metric below is calulated by taking segmenting the the dataset into identity subgroups by using the provided identity labels and then calculating the subgroup metrics.\n",
    "\n",
    "In this notebook we have trained some baseline models which we will use as a baseline to compare our neural network against. \n",
    "\n",
    "### Metrics:\n",
    "\n",
    "In addition to accuracy we will observe the below metrics for our models\n",
    "\n",
    "#### Overall ROC-AUC:\n",
    "\n",
    "This is the standard ROC-AUC for the full evaluation set. In other words this is the area under the Reciever Operating Characteristic curve. It compares the true positive and false positive rates of a binary model.\n",
    "\n",
    "#### Subgroup ROC-AUC:\n",
    "\n",
    "Here, we restrict the data set to only the examples that mention the specific identity subgroup. A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.\n",
    "\n",
    "#### BPSN AUC:\n",
    "\n",
    "BPSN (Background Positive, Subgroup Negative) AUC: Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n",
    "\n",
    "#### BNSP AUC:\n",
    "\n",
    "BNSP (Background Negative, Subgroup Positive) AUC: Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.\n",
    "\n",
    "\n",
    "#### Generalized Mean of Bias AUCs\n",
    "To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
    "\n",
    "$M_p(m_s) = \\left(\\frac{1}{N} \\sum_{s=1}^{N} m_s^p\\right)^\\frac{1}{p}$\n",
    "\n",
    "Where:\n",
    "\n",
    "$M_p$ = the $p$th power-mean function\n",
    "\n",
    "$m_s$ = the bias metric $m$ calulated for subgroup $s$\n",
    "\n",
    "$N$ = number of identity subgroups\n",
    "\n",
    "For this competition, JigsawAI use a p value of -5 to encourage competitors to improve the model for the identity subgroups with the lowest model performance.\n",
    "\n",
    "### Final Metric\n",
    "We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score:\n",
    "\n",
    "$score = w_0 AUC_{overall} + \\sum_{a=1}^{A} w_a M_p(m_{s,a})$\n",
    "\n",
    "$A$ = number of submetrics (3)\n",
    "\n",
    "$m_{s,a}$ = bias metric for identity subgroup $s$ using submetric $a$\n",
    "\n",
    "$w_a$ = $a$ weighting for the relative importance of each submetric; all four $w$ values set to 0.25\n",
    "\n",
    "\n",
    "### Process:\n",
    "\n",
    "#### Classical ML models\n",
    "This is primarily an NLP task, our X feature matrix will be based off the text from online comments. We have defined a pre-processing pipeline in the 'preprocessing.ipynb' notebook to use for our our ML classifiers and a seperate pre-processing pipeline for the neural network models we are planning on training.\n",
    "\n",
    "From the classic ML classifer models, we intend to use the following models - our base word embedding technique will be TF-IDF: \n",
    "\n",
    "   * Logistic Regression\n",
    "   * SVM\n",
    "   * Random Forest\n",
    " \n",
    "   \n",
    "We will carry out hyperparameter optimization for each model and calculate the metrics for each.\n",
    "\n",
    "#### Neural Networks\n",
    "*see NN_model.ipynb*\n",
    "\n",
    "We will also train a neural network to answer this problem. We will start with a basic LSTM model which will be made of:\n",
    "    \n",
    "   * LSTM layers to read through the data\n",
    "   * Dense layers\n",
    "   * Output layer using sigmoid for the classes\n",
    "   \n",
    "We will then seek to improve this LSTM by creating a Bidrectional LSTM (BiLSTM) which we believe will improve accuracy by reading input sequences in both directions. If time allows we will also attempt to include a simple attention mechanism.\n",
    "\n",
    "The NN models will use Glove 840B 300d word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gov/anaconda3/envs/capstone/lib/python3.7/site-packages/tqdm/std.py:651: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import contractions\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "import operator\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Apply pre-processing to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train_data\n",
    "train_df = pd.read_csv('data/train_clean.csv')\n",
    "\n",
    "# Load in test data\n",
    "test_df = pd.read_csv('data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.rename({'toxicity':'target'}, axis=1, inplace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unneeded columns\n",
    "train_df = train_df.iloc[:,1:]\n",
    "test_df = test_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we define the function that pre-processes our text\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "def text_cleaner(df, col_name, clean_col_name):\n",
    "    '''\n",
    "    Text pre-processing pipeline, we lemmatize words, expand contractions, remove common stop words, apply lower case,\n",
    "    tokenize, and delete punctuation. All functions use apply and list comprehension for speed benefit.\n",
    "   \n",
    "    INPUT:\n",
    "    df = name of dataframe\n",
    "    col_name = name of column to pre-process\n",
    "    clean_col_name = name of new cleaned_column\n",
    "   \n",
    "    OUTPUT:\n",
    "    None - changes are made directly to dataframe\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Lemmatize helper functions\n",
    "    # Lemmatize nouns\n",
    "    def lemmatize_text_noun(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='n') for w in text]\n",
    "    \n",
    "    # Lemmatize verbs\n",
    "    def lemmatize_text_verb(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='v') for w in text]\n",
    "    # Lemmatize adjectives\n",
    "    def lemmatize_text_adj(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='a') for w in text]\n",
    "\n",
    "    # Lemmatize adverbs\n",
    "    def lemmatize_text_adv(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='r') for w in text]\n",
    "    \n",
    "    # Expand contraction method\n",
    "    def contraction_expand(text):\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    # To lower case.\n",
    "    df[clean_col_name] = df[col_name].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Expand contractions\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: contraction_expand(x))\n",
    "    \n",
    "    #Tokenize:\n",
    "    tokenizer = TweetTokenizer(reduce_len=True)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: tokenizer.tokenize(x))\n",
    "   \n",
    "    \n",
    "    #Remove Stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    #Delete punctuation\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item.translate(punc_table) for item in x])\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_noun)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_verb)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adj)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adv)\n",
    "    \n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def detokenizer(df, col_name):\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    df[col_name+'_detokenize'] = df[col_name].apply(lambda x: detokenizer.detokenize(x))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 18s, sys: 8.28 s, total: 19min 26s\n",
      "Wall time: 19min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the cleaner func on train data\n",
    "text_cleaner(train_df, 'comment_text', 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Run the same cleaner on the training data\n",
    "text_cleaner(test_df, 'comment_text', 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 34s, sys: 803 ms, total: 3min 35s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "detokenizer(train_df,'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 s, sys: 194 ms, total: 24.1 s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# detokenize the test data\n",
    "detokenizer(test_df, 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Modelling\n",
    "\n",
    "The data has been pre-processed for our models. We can now begin model training.\n",
    "\n",
    "We have a seperate test dataset that will be kept aside for testing only once we have an ideal model. For hyperparameter optimizing we will use Scikit-learn's GridSearchCV. \n",
    "\n",
    "After we have fitted a model and predicted results, we can then append the predictions to the dataframe and calculate the subgroup AUCs and the final weighted metric . \n",
    "\n",
    "#### Defining subgroup AUC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "\n",
    "# These calculations have been provided by Jigsaw AI for scoring based on the metrics of the kaggle competition\n",
    "# https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation\n",
    "\n",
    "# They work by filtering the relevant dataframe into specific subgroups and using the roc_auc_score metric from sklearn.\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df.loc[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df.loc[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df.loc[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df.loc[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset.loc[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These calculations have been provided by Jigsaw AI for scoring based on the metrics of the kaggle competition\n",
    "# https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation\n",
    "\n",
    "# They work by filtering the relevant dataframe into specific subgroups and using the roc_auc_score metric from sklearn.\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (1804874,)\n",
      "y_test shape: (194640,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We will use Grid Search Cross validation to optimize our hyperparameters on the training set. We will then fit the best estimator and then predict on the test set and calculate relevent metrics.\n",
    "\n",
    "Because the subgroup ROCs are calculated post prediction and require predictions to be appended to a dataframe, we cannot actually pass the metrics into the scoring function of scikit-learn's gridsearchCV(). Instead we will just test the models on accuracy and total ROC-AUC and then refitting the grid-search on the model with the best ROC-AUC as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 ms, sys: 3.6 ms, total: 5.25 ms\n",
      "Wall time: 8.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "#no scaling data is already transformed\n",
    "\n",
    "# Instantiate the tokenizer to use in the vectorizer\n",
    "tweet_tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "# Instantiate the vectorizer, we will pass in the TweetTokenizer() from nltk \n",
    "tfid_vec_2 = TfidfVectorizer(lowercase=False, tokenizer = tweet_tokenizer.tokenize)\n",
    "\n",
    "# define pipeline\n",
    "pipeline = Pipeline([('tf-idf', tfid_vec_2), \n",
    "                     ('model', SVC())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define scoring functions\n",
    "scorers = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "param_grid_log = [{'model': [LogisticRegression()], 'tf-idf': [tfid_vec_2], \n",
    "                   'model__penalty': ['l1', 'l2'],\n",
    "                   'model__C': [.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best cross-validated AUC score, i.e it finds the params that gives the best scores\n",
    "# then refits using the ones that give the best AUC. \n",
    "grid_log = GridSearchCV(pipeline, param_grid_log, cv=5, scoring=scorers, refit='AUC', \n",
    "                        return_train_score=True, n_jobs=-1)\n",
    "\n",
    "fittedgrid_log = grid_log.fit(train_df['comment_text_clean_detokenize'], train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Save best estimator to file\n",
    "joblib.dump(fittedgrid.best_estimator_, 'saved_models/best_log_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to load the model if required\n",
    "from sklearn.externals import joblib\n",
    "from joblib import load\n",
    "fittedgrid_log = load('saved_models/best_log_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores\n",
    "train_accuracy = fittedgrid_log.score(train_df['comment_text_clean_detokenize'], y_train)\n",
    "test_accuracy = fittedgrid_log.score(test_df['comment_text_clean_detokenize'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train and test values\n",
    "y_train_pred = fittedgrid_log.predict(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred = fittedgrid_log.predict(test_df['comment_text_clean_detokenize'])\n",
    "\n",
    "y_train_pred_prob = fittedgrid_log.predict_proba(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred_prob = fittedgrid_log.predict_proba(test_df['comment_text_clean_detokenize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    179192\n",
      "           1       0.76      0.50      0.60     15448\n",
      "\n",
      "    accuracy                           0.95    194640\n",
      "   macro avg       0.86      0.74      0.79    194640\n",
      "weighted avg       0.94      0.95      0.94    194640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix and Classification Report, we want to store the F1 Score.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# store the precision, recall, and f1 score for later and print the classification report\n",
    "log_precision = precision_score(y_test, y_test_pred)\n",
    "log_recall = recall_score(y_test, y_test_pred)\n",
    "log_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append preds and probabilities to train and valid dfs\n",
    "train_df['Prediction_log'] = y_train_pred\n",
    "train_df['Prediction_probability_log'] = y_train_pred_prob[:, 1]\n",
    "test_df['Prediction_log'] = y_test_pred\n",
    "test_df['Prediction_probability_log'] = y_test_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity columns used to calculate subgroup AUC\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in identity_columns + ['target']:\n",
    "    train_df[col] = np.where(train_df[col] >= 0.5, True, False)\n",
    "    test_df[col] = np.where(test_df[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the AUC metrics \n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "\n",
    "MODEL_NAME = 'Prediction_log'\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "log_bias_metrics_df_train = compute_bias_metrics_for_model(train_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "log_final_metric_train = get_final_metric(log_bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))\n",
    "\n",
    "log_bias_metrics_df_test = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "log_final_metric_test = get_final_metric(log_bias_metrics_df_test, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_bias_metrics_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train_accuracy:{train_accuracy}')\n",
    "print(f'train weighted subgroup AUC:{get_final_metric(bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))}')\n",
    "print(f'test_accuracy:{test_accuracy}')\n",
    "print(f'test weighted subgroup AUC::{get_final_metric(bias_metrics_df_val, calculate_overall_auc(test_df, MODEL_NAME))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation:\n",
    "\n",
    "The logistic regression performed well on pure accuracy, with a train accuracy of 95.38% and 94.96% test accuracy. What is also positive to see is that our hyperparameter optimization has led to a model which does not overfit excessively. \n",
    "\n",
    "However when we look at the weighted subgroup AUC metric, the 71.9% train score and 71.5% test score show that the model did have a tendency towards biased predictions for certain subgroup. In comparison, the benchmark CNN that was provided had a weighted AUC score of 88.35% albeit just on a validation set \n",
    "\n",
    "\n",
    "For example we can see that for the 'black' identity BPSN AUC was relatively low, suggesting the model is likely overweighting mentions of the 'black' identity with toxicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "We will now carry out a similar process with SVM to see if this performs appreciably different to logistic regression. \n",
    "\n",
    "#### Grid searching on a subset of data\n",
    "\n",
    "Given the length of time taken to grid search using SVM, we will run a gridsearch on a subset of our dataset by taking a new train test split. We will then use the results there as a proxy for the optimal parameters for our full dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create a new split of the training dataframe to use for this reduced test. \n",
    "# we will not need the remainder. We will take 1/4 of the total data set\n",
    "remainder, reduced_df = train_test_split(train_df, test_size=0.25, stratify=train_df['target'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Grid search applies tfid_vec_2 to all models. We try combinations of model loss function penalty strength and\n",
    "# model gamma  is a parameter for non linear hyperplanes. \n",
    "# The higher the gamma value it tries to exactly fit the training data set\n",
    "\n",
    "param_grid_svc = [{'model__kernel': ['rbf'],'tf-idf': [tfid_vec_2],\n",
    "              'model__C': [0.1, 1, 10,], 'model__gamma': [1, 10, 'scale'], 'model__probability': [True]}]\n",
    "\n",
    "reduced_grid_svm = GridSearchCV(pipeline, param_grid_svc, scoring=scorers, cv=3, refit='AUC',\n",
    "                       return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "reduced_grid_svm = reduced_grid_svm.fit(reduced_df['comment_text_clean_detokenize'], reduced_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file \n",
    "import joblib\n",
    "joblib.dump(reduced_grid_svm.best_estimator_, 'saved_models/reduced_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the model with the best AUC score\n",
    "reduced_grid_svm.best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives the model with the best AUC score\n",
    "reduced_grid_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Full dataset grid search\n",
    "\n",
    "param_grid_svc = [{'model__kernel': ['rbf'],'tf-idf': [tfid_vec_2],\n",
    "              'model__C': [0.1, 1, 10], 'model__probability': [True]}]\n",
    "\n",
    "grid_svc = GridSearchCV(pipeline, param_grid_svc, scoring=scorers, cv=5, refit='AUC',\n",
    "                       return_train_score=True, n_jobs=-1, verbose=10)\n",
    "\n",
    "fittedgrid_svc = grid_svc.fit(train_df['comment_text_clean_detokenize'], train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best estimator to file\n",
    "joblib.dump(fittedgrid_svc.best_estimator_, 'saved_models/best_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores\n",
    "train_accuracy = fittedgrid_svc.score(train_df['comment_text_clean_detokenize'], y_train)\n",
    "test_accuracy = fittedgrid_svc.score(test_df['comment_text_clean_detokenize'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train and test values\n",
    "y_train_pred = fittedgrid_svc.predict(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred = fittedgrid_svc.predict(test_df['comment_text_clean_detokenize'])\n",
    "\n",
    "y_train_pred_prob = fittedgrid_svc.predict_proba(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred_prob = fittedgrid_svc.predict_proba(test_df['comment_text_clean_detokenize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the precision, recall, and f1 score for later and print the classification report\n",
    "svm_precision = precision_score(y_test, y_test_pred)\n",
    "svm_recall = recall_score(y_test, y_test_pred)\n",
    "svm_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "display(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append preds and probabilities to train and valid dfs\n",
    "\n",
    "\n",
    "train_df['Prediction_svc'] = y_train_pred\n",
    "train_df['Prediction_probability_svc'] = y_train_pred_prob[:, 1]\n",
    "test_df['Prediction_svc'] = y_test_pred\n",
    "test_df['Prediction_probability_svc'] = y_test_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the AUC metrics \n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "\n",
    "MODEL_NAME = 'Prediction_svc'\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "svm_bias_metrics_df_train = compute_bias_metrics_for_model(train_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "svm_final_metric_train = get_final_metric(svm_bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))\n",
    "\n",
    "svm_bias_metrics_df_test = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "svm_final_metric_test get_final_metric(svm_bias_metrics_df_test, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "For our final model we will try the Random Forest Classifier which is an ensemble method. It is based on the Decision Tree model, only the Random Forest works by fitting on random sub samples of the data (with replacement), which is known as 'bagging'. A voting algorithm is then applied on the results of each of the trees to determine the final class of the data point. The aim of Random Forest is to train a series of overfit models and then average out the results via voting to get better results. The main hyperparemeter we will optimize for is the number of decision trees to use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the length of time taken to grid search on the Random Forest Classifier, we will run a gridsearch on a subset of our dataset by taking a new train test split. We will then use the results there as a proxy for the optimal parameters for our full dataset. We will use the same train_test split as used for the SVM subset search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  36 | elapsed: 78.8min remaining: 34.7min\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  36 | elapsed: 113.6min remaining: 27.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed: 132.4min remaining: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed: 165.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 44s, sys: 12.9 s, total: 50min 57s\n",
      "Wall time: 3h 35min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Note, most of the time here actually comes from the application of the tfid vectorizer to each fold of the data\n",
    "# Random forest itself takes ~5minutes to run on the vectorized data.\n",
    "\n",
    "# We control the model max depth \n",
    "param_grid_RF = {'model': [RandomForestClassifier()], 'tf-idf': [tfid_vec_2], \n",
    "                 'model__n_estimators': [10,50,100],\n",
    "                 'model__max_depth': [100, 500, 1000, 5000],\n",
    "             }\n",
    "reduced_grid_RF = GridSearchCV(pipeline, param_grid_RF, scoring=scorers, cv=3, refit='AUC',\n",
    "                       return_train_score=True, n_jobs=-1, verbose = 10)\n",
    "\n",
    "reduced_grid_RF = reduced_grid_RF.fit(reduced_df['comment_text_clean_detokenize'], reduced_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tf-idf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=1000,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the best estimator parameters\n",
    "reduced_grid_RF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230210872062684"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives the best AUC score\n",
    "reduced_grid_RF.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced_grid_RF.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test has suggested that in terms of hyperparameters, we should look at a model that has n_estimators around 100 or above, and max_depth of around 1000. We will use this as a guide for our grid search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search in the full dataset\n",
    "Now that we have a good idea of optimal parameters, we will run another grid search on a few parameters around the optimal figures provided by our test on the subset of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a grid-search on the full set of data \n",
    "param_grid_RF = {'model': [RandomForestClassifier()], 'tf-idf': [tfid_vec_2], \n",
    "                 'model__n_estimators': [100,150,200],\n",
    "                 'model__max_depth': [1000, 2000, 3000, 4000],\n",
    "             }\n",
    "grid_RF = GridSearchCV(pipeline, param_grid_RF, scoring=scorers, cv=3, refit='AUC',\n",
    "                       return_train_score=True, n_jobs=-1, verbose = 10)\n",
    "\n",
    "fittedgrid_RF = grid_RF.fit(train_df['comment_text_clean_detokenize'], train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=1000, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = tfid_vec_2.transform(train_df['comment_text_clean_detokenize'])\n",
    "X_test = tfid_vec_2.transform(test_df['comment_text_clean_detokenize'])\n",
    "RF_model = RandomForestClassifier(n_estimators = 100, max_depth = 1000)\n",
    "RF_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 3.61 s, total: 1min 24s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_test_accuracy = RF_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9417591450883682"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/best_RF.pkl']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save best estimator to file\n",
    "joblib.dump(RF_model, 'saved_models/best_RF.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train and test values\n",
    "y_train_pred = RF_model.predict(X_train)\n",
    "y_test_pred = RF_model.predict(X_test)\n",
    "\n",
    "y_train_pred_prob = RF_model.predict_proba(X_train)\n",
    "y_test_pred_prob = RF_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97    179192\n",
      "           1       0.83      0.34      0.48     15448\n",
      "\n",
      "    accuracy                           0.94    194640\n",
      "   macro avg       0.89      0.67      0.72    194640\n",
      "weighted avg       0.94      0.94      0.93    194640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# store the precision, recall, and f1 score for later and print the classification report\n",
    "rf_precision = precision_score(y_test, y_test_pred)\n",
    "rf_recall = recall_score(y_test, y_test_pred)\n",
    "rf_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity columns used to calculate subgroup AUC\n",
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in identity_columns + ['target']:\n",
    "    train_df[col] = np.where(train_df[col] >= 0.5, True, False)\n",
    "    test_df[col] = np.where(test_df[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append preds and probabilities to train and valid dfs\n",
    "train_df['Prediction_RF'] = y_train_pred\n",
    "train_df['Prediction_probability_RF'] = y_train_pred_prob[:, 1]\n",
    "test_df['Prediction_RF'] = y_test_pred\n",
    "test_df['Prediction_probability_RF'] = y_test_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the AUC metrics \n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "\n",
    "MODEL_NAME = 'Prediction_RF'\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "rf_bias_metrics_df_train = compute_bias_metrics_for_model(train_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "rf_final_metric_train = get_final_metric(rf_bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))\n",
    "\n",
    "rf_bias_metrics_df_test = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "rf_final_metric_test = get_final_metric(rf_bias_metrics_df_test, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1519</td>\n",
       "      <td>0.535805</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.534692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4226</td>\n",
       "      <td>0.538518</td>\n",
       "      <td>0.671294</td>\n",
       "      <td>0.536449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>835</td>\n",
       "      <td>0.542044</td>\n",
       "      <td>0.668454</td>\n",
       "      <td>0.540397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2040</td>\n",
       "      <td>0.542836</td>\n",
       "      <td>0.672161</td>\n",
       "      <td>0.540382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1065</td>\n",
       "      <td>0.546340</td>\n",
       "      <td>0.669129</td>\n",
       "      <td>0.545196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2452</td>\n",
       "      <td>0.556286</td>\n",
       "      <td>0.671977</td>\n",
       "      <td>0.555543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5155</td>\n",
       "      <td>0.573352</td>\n",
       "      <td>0.671357</td>\n",
       "      <td>0.572037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>511</td>\n",
       "      <td>0.584684</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.586552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4386</td>\n",
       "      <td>0.585907</td>\n",
       "      <td>0.670266</td>\n",
       "      <td>0.584965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "6                          black           1519      0.535805  0.671200   \n",
       "3                      christian           4226      0.538518  0.671294   \n",
       "4                         jewish            835      0.542044  0.668454   \n",
       "5                         muslim           2040      0.542836  0.672161   \n",
       "2      homosexual_gay_or_lesbian           1065      0.546340  0.669129   \n",
       "7                          white           2452      0.556286  0.671977   \n",
       "1                         female           5155      0.573352  0.671357   \n",
       "8  psychiatric_or_mental_illness            511      0.584684  0.664368   \n",
       "0                           male           4386      0.585907  0.670266   \n",
       "\n",
       "   bnsp_auc  \n",
       "6  0.534692  \n",
       "3  0.536449  \n",
       "4  0.540397  \n",
       "5  0.540382  \n",
       "2  0.545196  \n",
       "7  0.555543  \n",
       "1  0.572037  \n",
       "8  0.586552  \n",
       "0  0.584965  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_bias_metrics_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108032567341172"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_final_metric_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train_accuracy:{train_accuracy}')\n",
    "print(f'train weighted AUC:{rf_final_metric_train}')\n",
    "print(f'test_accuracy:{test_accuracy}')\n",
    "print(f'test weighted AUC::{rf_final_metric_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the classical models the Random Forest was by far the weakest in terms of the final weighted AUC metric. It generally did poorly accross all the identity subgroups. In addition to this, it had very poor recall on the test set, implying that the model was failing to identify many cases of toxic commentary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance\n",
    "\n",
    "Now that we have trained three seperate models which have been shown to deliver strong performance in text classification tasks in the past let us take the time to compare them side-by-side and also discuss their short-comings in terms of reducing bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the subgroup bias metrics tables for each model and then the final metrics for each model for test only\n",
    "display(log_bias_metrics_df_test)\n",
    "display(svm_bias_metrics_df_test)\n",
    "display(rf_bias_metrics_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final metrics for each model for test only.\n",
    "print(f' Final metric for Logistic Regression: {log_final_metric_test}')\n",
    "print(f' Final metric for SVM: {svm_final_metric_test}')\n",
    "print(f' Final metric for Random Forest: {rf_final_metric_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy': [log_accuracy, svm_accuracy, rf_accuracy], 'Precision': [log_precision, svm_precision, rf_precision],\n",
    "       'Recall': [log_recall, svm_recall, rf_recall], 'F1': [log_f1, svm_f1, rf_f1],\n",
    "       'Final Bias Metric': [log_bias_metrics_df_test, svm_bias_metrics_df_test, rf_bias_metrics_df_test ]}\n",
    "\n",
    "results_df = pd.DataFrame(data, index=['Logistic', 'Precision', 'Recall', 'Final Bias Metric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the results of our ML models using eli5\n",
    "\n",
    "Below we use the eli5 package to help interpret our results. We will do this using the logistic model we fit due to only certain sklearn classifiers being compatible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.lime import TextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# We need to fit a standard Logistic Model using the same parametrs as our fitted grid as eli5 does not take in grids\n",
    "eli5_log_model = LogisticRegression(C=1, penalty='l1')\n",
    "# use the train and test transformed arrays from earlier\n",
    "eli5_log_model.fit(X_train,y_train)\n",
    "\n",
    "#score to check accuracy similar\n",
    "accuracy = eli5_log_model.score(X_test, y_test)\n",
    "#predict the test \n",
    "preds = eli5_log_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['eli5_log'] = preds[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['eli5_log'] = np.where(test_df['eli5_log']>=0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1804874x181709 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42012254 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(eli5_log_model, test_df.loc[23566,'comment_text_clean_detokenize'], vec=tfid_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=Non-toxic\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.875</b>, score <b>-1.947</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +4.068\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 98.01%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.151\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        preference\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.26%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.037\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        less\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.30%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.034\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        wonderful\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.39%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.028\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        part\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.41%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.027\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        typo\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.43%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.025\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        feel\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.45%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.024\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        question\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.51%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.020\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        condemnation\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.52%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.020\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        actually\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.52%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.020\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        personal\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.017\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        business\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.61%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.015\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        play\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.61%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.014\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        think\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.014\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        do\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.65%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.012\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        life\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.68%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.011\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        fact\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.011\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        possible\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.009\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        family\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.72%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.009\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        know\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.82%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.005\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        support\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        young\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.85%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.004\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        really\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 99.89%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.002\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        people\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.76%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.008\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        care\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.68%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.011\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        absolutely\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.62%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.014\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        person\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.57%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.017\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        I\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.38%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.029\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        old\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.32%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.032\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        guy\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.26%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.037\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        men\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 99.06%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.052\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        wife\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 98.45%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.105\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        heterosexual\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.39%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.222\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        sex\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 94.93%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.573\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        sexual\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 89.90%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -1.533\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        homosexual\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Explanation(estimator=\"LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\\n                   multi_class='warn', n_jobs=None, penalty='l1',\\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\\n                   warm_start=False)\", description=None, error=None, method='linear model', is_regression=False, targets=[TargetExplanation(target='Non-toxic', feature_weights=FeatureWeights(pos=[FeatureWeight(feature='<BIAS>', weight=4.067819526352757, std=None, value=1.0), FeatureWeight(feature='preference', weight=0.15050612342487557, std=None, value=0.09848561374752673), FeatureWeight(feature='less', weight=0.036944886978590434, std=None, value=0.0500007178968633), FeatureWeight(feature='wonderful', weight=0.03366097949275505, std=None, value=0.04248781514865893), FeatureWeight(feature='part', weight=0.028071741398437572, std=None, value=0.028695163706261836), FeatureWeight(feature='typo', weight=0.026525265257097065, std=None, value=0.05552247344761501), FeatureWeight(feature='feel', weight=0.02510549275237288, std=None, value=0.030215840943073273), FeatureWeight(feature='question', weight=0.023841653295555622, std=None, value=0.02989906224280827), FeatureWeight(feature='condemnation', weight=0.02026188992118554, std=None, value=0.05234319218098195), FeatureWeight(feature='actually', weight=0.019820321467975, std=None, value=0.027909609531955874), FeatureWeight(feature='personal', weight=0.019710444406929213, std=None, value=0.03441866191382622), FeatureWeight(feature='business', weight=0.016951853666838285, std=None, value=0.02949374721939931), FeatureWeight(feature='play', weight=0.01457930155185926, std=None, value=0.031171737086051646), FeatureWeight(feature='think', weight=0.01441062144290144, std=None, value=0.02056907354815411), FeatureWeight(feature='do', weight=0.014166183433322727, std=None, value=0.029319737402930382), FeatureWeight(feature='life', weight=0.012448280707995136, std=None, value=0.027174048980096552), FeatureWeight(feature='fact', weight=0.010839712371861526, std=None, value=0.026469590531643414), FeatureWeight(feature='possible', weight=0.010553022429058853, std=None, value=0.035006343821040904), FeatureWeight(feature='family', weight=0.0093540006619206, std=None, value=0.029651813188166162), FeatureWeight(feature='know', weight=0.009237308450812094, std=None, value=0.02141555522935561), FeatureWeight(feature='support', weight=0.004999720265117958, std=None, value=0.027467486358471824), FeatureWeight(feature='young', weight=0.0036654504265455556, std=None, value=0.033846188512670905), FeatureWeight(feature='really', weight=0.00362754966135804, std=None, value=0.025857875671579034), FeatureWeight(feature='people', weight=0.0022696096266836482, std=None, value=0.01903367763501745)], neg=[FeatureWeight(feature='homosexual', weight=-1.5333567146966083, std=None, value=0.24161180220043157), FeatureWeight(feature='sexual', weight=-0.573117504249419, std=None, value=0.11510645269997512), FeatureWeight(feature='sex', weight=-0.22189934896406804, std=None, value=0.03863307178797715), FeatureWeight(feature='heterosexual', weight=-0.10525988881695267, std=None, value=0.05552247344761501), FeatureWeight(feature='wife', weight=-0.051797752776036335, std=None, value=0.03866720671384609), FeatureWeight(feature='men', weight=-0.03651617378313542, std=None, value=0.03383374101463431), FeatureWeight(feature='guy', weight=-0.03225704785716684, std=None, value=0.03004123887762275), FeatureWeight(feature='old', weight=-0.02868211338270907, std=None, value=0.029844398374451343), FeatureWeight(feature='I', weight=-0.016989373098195867, std=None, value=0.11251272723185926), FeatureWeight(feature='person', weight=-0.014201923335239518, std=None, value=0.02935669962375227), FeatureWeight(feature='absolutely', weight=-0.011204155808376842, std=None, value=0.03651218283971557), FeatureWeight(feature='care', weight=-0.007510241722540822, std=None, value=0.028050857768798794)], pos_remaining=0, neg_remaining=0), proba=0.8750731045334196, score=-1.9465787009543574, weighted_spans=None, heatmap=None)], feature_importances=None, decision_tree=None, highlight_spaces=None, transition_features=None, image=None)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = test_df.loc[92316,'comment_text']\n",
    "text\n",
    "eli5.explain_prediction(eli5_log_model, text, \n",
    "                        target_names=['Non-toxic', 'Toxic'], vec=tfid_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Racist out comes don't have to motivated by a since of white superiority. Simple ignorance is all it takes. I worked in the public sector for 30 years in Eugene, often being the only black male in the office. I would not consider the vast majority of white folks I worked with to be conscious racists. However, the majority of them were ignorant about their own bias'. For example, they may say they hire people they feel comfortable with. However, since most of them never had a close black friend, they some how end up hiring only white folks. Of course, if they crew up in Idaho, how would they get to know any black people well enough to make friends? For example, I am the only black man to be a manager in the 100 year history of EWEB. For every one on those years EWEB was less than a mile from the UO. No one in management at EWEB think there is anything wrong or unusual with that reality. I would not be surprised if all public offices in Oregon have some degree of that blindness.\""
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = test_df.loc[166392,'comment_text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'racist come motivated since white superiority  simple ignorance take  worked public sector 30 year eugene  often black male office  would consider vast majority white folk worked conscious racist  however  majority ignorant bias   example  may say hire people feel comfortable  however  since never close black friend  end hiring white folk  course  crew idaho  would get know black people well enough make friend  example  black man manager 100 year history eweb  every one year eweb le mile uo  one management eweb think anything wrong unusual reality  would surprised public office oregon degree blindness'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = test_df.loc[23566,'comment_text_clean_detokenize']\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>comment_text_clean</th>\n",
       "      <th>comment_text_clean_detokenize</th>\n",
       "      <th>Prediction_svc</th>\n",
       "      <th>Prediction_probability_svc</th>\n",
       "      <th>Prediction_log</th>\n",
       "      <th>Prediction_probability_log</th>\n",
       "      <th>eli5_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>7000878</td>\n",
       "      <td>Oh man..., no one is trying to \"wear\" anyone d...</td>\n",
       "      <td>2017-08-28 16:42:44.817441+00</td>\n",
       "      <td>22</td>\n",
       "      <td>370137</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[oh, man, , , one, trying, , wear, , anyone, l...</td>\n",
       "      <td>oh man   one trying  wear  anyone left  ask bl...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.604803</td>\n",
       "      <td>1</td>\n",
       "      <td>0.604803</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>7001090</td>\n",
       "      <td>Interesting that the DP failed to tell the ent...</td>\n",
       "      <td>2017-08-30 03:35:00.945664+00</td>\n",
       "      <td>102</td>\n",
       "      <td>372110</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[interesting, dp, failed, tell, entire, story,...</td>\n",
       "      <td>interesting dp failed tell entire story  sayin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>7001329</td>\n",
       "      <td>\"Democrats renounced racism. \"\\r\\n\\r\\nSo  Tim ...</td>\n",
       "      <td>2017-09-03 15:26:54.188281+00</td>\n",
       "      <td>13</td>\n",
       "      <td>373393</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[, democrat, renounced, racism, , , tim, agree...</td>\n",
       "      <td>democrat renounced racism   tim agrees democra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427517</td>\n",
       "      <td>0</td>\n",
       "      <td>0.427517</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>7001686</td>\n",
       "      <td>You can't make this stuff up ..\\r\\n.A white ra...</td>\n",
       "      <td>2017-10-12 02:26:36.040025+00</td>\n",
       "      <td>105</td>\n",
       "      <td>387829</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, stuff, \\r\\n, white, rapper, rant, playe...</td>\n",
       "      <td>make stuff \\r\\n white rapper rant played black...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921078</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>7003562</td>\n",
       "      <td>What annoys me is the way these politically co...</td>\n",
       "      <td>2016-12-08 16:31:29.779533+00</td>\n",
       "      <td>54</td>\n",
       "      <td>155349</td>\n",
       "      <td>approved</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[annoys, way, politically, correct, term, gone...</td>\n",
       "      <td>annoys way politically correct term gone full ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>7003599</td>\n",
       "      <td>You don't have a clue of what it's like to be ...</td>\n",
       "      <td>2017-02-25 15:16:06.873103+00</td>\n",
       "      <td>53</td>\n",
       "      <td>314382</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[clue, like, , grew, among, black, , worked, b...</td>\n",
       "      <td>clue like  grew among black  worked black  pla...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560420</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>7003602</td>\n",
       "      <td>Well personally, as a white guy I was accused ...</td>\n",
       "      <td>2017-11-06 21:52:38.604522+00</td>\n",
       "      <td>102</td>\n",
       "      <td>397590</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[well, personally, , white, guy, accused, raci...</td>\n",
       "      <td>well personally  white guy accused racism furi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>7004204</td>\n",
       "      <td>Yes. \\r\\n\\r\\nUnlike women, gays, lesbians, tra...</td>\n",
       "      <td>2017-05-22 18:12:29.957359+00</td>\n",
       "      <td>13</td>\n",
       "      <td>336018</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[yes, , unlike, woman, , gay, , lesbian, , tra...</td>\n",
       "      <td>yes  unlike woman  gay  lesbian  transgenders ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.860850</td>\n",
       "      <td>1</td>\n",
       "      <td>0.860850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>7004826</td>\n",
       "      <td>I'm talking about the eugenics practiced in Or...</td>\n",
       "      <td>2016-04-19 16:39:05.823989+00</td>\n",
       "      <td>13</td>\n",
       "      <td>60528</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, talking, eugenics, practiced, oregon, invo...</td>\n",
       "      <td>I talking eugenics practiced oregon involuntar...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.796792</td>\n",
       "      <td>1</td>\n",
       "      <td>0.796792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5844</th>\n",
       "      <td>7005844</td>\n",
       "      <td>It's simple.\\r\\nWorking means getting up and g...</td>\n",
       "      <td>2017-09-10 14:49:42.415101+00</td>\n",
       "      <td>102</td>\n",
       "      <td>376135</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[simple, , working, mean, getting, going, work...</td>\n",
       "      <td>simple  working mean getting going work  buyin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.699297</td>\n",
       "      <td>1</td>\n",
       "      <td>0.699297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7722</th>\n",
       "      <td>7007722</td>\n",
       "      <td>Walter:  I'm sure you deny feeling \"white man'...</td>\n",
       "      <td>2017-09-03 18:36:12.576890+00</td>\n",
       "      <td>13</td>\n",
       "      <td>373393</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[walter, , I, sure, deny, feeling, , white, ma...</td>\n",
       "      <td>walter  I sure deny feeling  white man guilt  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582941</td>\n",
       "      <td>1</td>\n",
       "      <td>0.582941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059</th>\n",
       "      <td>7010059</td>\n",
       "      <td>I am a retired Army colonel. This PC, left win...</td>\n",
       "      <td>2017-11-08 16:31:48.801952+00</td>\n",
       "      <td>105</td>\n",
       "      <td>398116</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[retired, army, colonel, , pc, , left, wing, j...</td>\n",
       "      <td>retired army colonel  pc  left wing jerk tryin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548858</td>\n",
       "      <td>1</td>\n",
       "      <td>0.548858</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10831</th>\n",
       "      <td>7010831</td>\n",
       "      <td>Yes, breathing while black.</td>\n",
       "      <td>2017-09-02 21:59:40.362604+00</td>\n",
       "      <td>53</td>\n",
       "      <td>373093</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[yes, , breathing, black, ]</td>\n",
       "      <td>yes  breathing black</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718893</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12581</th>\n",
       "      <td>7012581</td>\n",
       "      <td>Forget about the Catholic component. It's very...</td>\n",
       "      <td>2016-10-14 15:55:45.727706+00</td>\n",
       "      <td>53</td>\n",
       "      <td>148503</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[forget, catholic, component, , clear, race, i...</td>\n",
       "      <td>forget catholic component  clear race importan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684445</td>\n",
       "      <td>1</td>\n",
       "      <td>0.684445</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13444</th>\n",
       "      <td>7013444</td>\n",
       "      <td>I'm a moderate and don't get caught up in eith...</td>\n",
       "      <td>2017-08-16 18:12:31.914734+00</td>\n",
       "      <td>102</td>\n",
       "      <td>367233</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, moderate, get, caught, either, side, polit...</td>\n",
       "      <td>I moderate get caught either side politics  th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485873</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13680</th>\n",
       "      <td>7013680</td>\n",
       "      <td>Are you kidding? 99% of the chronic thieves bu...</td>\n",
       "      <td>2017-08-24 22:06:27.097897+00</td>\n",
       "      <td>21</td>\n",
       "      <td>370264</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[kidding, , 99, , chronic, thief, busted, plac...</td>\n",
       "      <td>kidding  99  chronic thief busted place work w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774574</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070</th>\n",
       "      <td>7014070</td>\n",
       "      <td>This country and the NFL has made more black m...</td>\n",
       "      <td>2017-09-23 19:37:12.534970+00</td>\n",
       "      <td>55</td>\n",
       "      <td>381076</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[country, nfl, made, black, millionaire, anyth...</td>\n",
       "      <td>country nfl made black millionaire anything el...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.653236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.653236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16036</th>\n",
       "      <td>7016036</td>\n",
       "      <td>You are wrong. Racism can be active or passive...</td>\n",
       "      <td>2017-09-03 18:35:16.925135+00</td>\n",
       "      <td>13</td>\n",
       "      <td>373393</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[wrong, , racism, active, passive, , attention...</td>\n",
       "      <td>wrong  racism active passive  attention need i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882464</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21363</th>\n",
       "      <td>7021363</td>\n",
       "      <td>\"Yeah, thats not the point: you removed statu...</td>\n",
       "      <td>2017-08-28 17:15:58.989283+00</td>\n",
       "      <td>22</td>\n",
       "      <td>370137</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[, yeah, , point, , removed, statue, mary, bab...</td>\n",
       "      <td>yeah  point  removed statue mary baby jesus  c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.550947</td>\n",
       "      <td>1</td>\n",
       "      <td>0.550947</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21626</th>\n",
       "      <td>7021626</td>\n",
       "      <td>Let me\\r\\n\\r\\nFirst, the percentage of whites ...</td>\n",
       "      <td>2017-01-23 17:12:19.417798+00</td>\n",
       "      <td>54</td>\n",
       "      <td>163496</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[let, first, , percentage, white, indian, citi...</td>\n",
       "      <td>let first  percentage white indian citizen pra...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.817162</td>\n",
       "      <td>1</td>\n",
       "      <td>0.817162</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22993</th>\n",
       "      <td>7022993</td>\n",
       "      <td>Eastman votes against honoring Hmong and Laoti...</td>\n",
       "      <td>2017-05-06 11:42:18.537246+00</td>\n",
       "      <td>21</td>\n",
       "      <td>332432</td>\n",
       "      <td>approved</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[eastman, vote, honoring, hmong, laotian, vete...</td>\n",
       "      <td>eastman vote honoring hmong laotian veteran  v...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.551769</td>\n",
       "      <td>1</td>\n",
       "      <td>0.551769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23102</th>\n",
       "      <td>7023102</td>\n",
       "      <td>I think you're saying the being born rich is p...</td>\n",
       "      <td>2017-08-16 22:47:54.024159+00</td>\n",
       "      <td>13</td>\n",
       "      <td>366931</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[think, saying, born, rich, probably, factor, ...</td>\n",
       "      <td>think saying born rich probably factor trump b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.670123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.670123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23566</th>\n",
       "      <td>7023566</td>\n",
       "      <td>Racist out comes don't have to motivated by a ...</td>\n",
       "      <td>2017-09-18 17:01:22.386060+00</td>\n",
       "      <td>13</td>\n",
       "      <td>378947</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[racist, come, motivated, since, white, superi...</td>\n",
       "      <td>racist come motivated since white superiority ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.813732</td>\n",
       "      <td>1</td>\n",
       "      <td>0.813732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29236</th>\n",
       "      <td>7029236</td>\n",
       "      <td>Chris in Ottawa,\\r\\n\\r\\nAre you KIDDING me?\\r\\...</td>\n",
       "      <td>2017-07-30 15:52:16.305318+00</td>\n",
       "      <td>54</td>\n",
       "      <td>360964</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[chris, ottawa, , kidding, , mean, , kidding, ...</td>\n",
       "      <td>chris ottawa  kidding  mean  kidding  nonchine...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.671653</td>\n",
       "      <td>1</td>\n",
       "      <td>0.671653</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32443</th>\n",
       "      <td>7032443</td>\n",
       "      <td>A Nazi would be more inclined, ideologically, ...</td>\n",
       "      <td>2017-08-18 14:15:38.899097+00</td>\n",
       "      <td>102</td>\n",
       "      <td>367562</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[nazi, would, inclined, , ideologically, , vot...</td>\n",
       "      <td>nazi would inclined  ideologically  vote berni...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700639</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700639</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>7034296</td>\n",
       "      <td>garycrum - Clearly, the country has been divid...</td>\n",
       "      <td>2016-03-17 16:26:41.527452+00</td>\n",
       "      <td>13</td>\n",
       "      <td>49679</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[garycrum, , clearly, , country, divided, time...</td>\n",
       "      <td>garycrum  clearly  country divided time  persp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>1</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36050</th>\n",
       "      <td>7036050</td>\n",
       "      <td>And do it in the 50s when public schools for b...</td>\n",
       "      <td>2017-10-16 21:28:48.500213+00</td>\n",
       "      <td>102</td>\n",
       "      <td>389721</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[50, public, school, black, horrible, , white,...</td>\n",
       "      <td>50 public school black horrible  white get leg</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.893342</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36368</th>\n",
       "      <td>7036368</td>\n",
       "      <td>Jim, I don't think you can handle the truth ab...</td>\n",
       "      <td>2017-06-08 22:50:12.371396+00</td>\n",
       "      <td>13</td>\n",
       "      <td>341755</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[jim, , think, handle, truth, role, white, rac...</td>\n",
       "      <td>jim  think handle truth role white racism hist...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.586750</td>\n",
       "      <td>1</td>\n",
       "      <td>0.586750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36449</th>\n",
       "      <td>7036449</td>\n",
       "      <td>I'm having a hard time believing that you feel...</td>\n",
       "      <td>2016-10-24 20:11:23.056474+00</td>\n",
       "      <td>53</td>\n",
       "      <td>148708</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[I, hard, time, believing, feel, bad, black, p...</td>\n",
       "      <td>I hard time believing feel bad black populatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543354</td>\n",
       "      <td>1</td>\n",
       "      <td>0.543354</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40972</th>\n",
       "      <td>7040972</td>\n",
       "      <td>So, Muslims are not \"our people\"? Let's say th...</td>\n",
       "      <td>2016-11-30 21:20:47.109316+00</td>\n",
       "      <td>53</td>\n",
       "      <td>153390</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[, muslim, , people, , , let, u, say, number, ...</td>\n",
       "      <td>muslim  people   let u say number black violen...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530568</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530568</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows  51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                       comment_text  \\\n",
       "878    7000878  Oh man..., no one is trying to \"wear\" anyone d...   \n",
       "1090   7001090  Interesting that the DP failed to tell the ent...   \n",
       "1329   7001329  \"Democrats renounced racism. \"\\r\\n\\r\\nSo  Tim ...   \n",
       "1686   7001686  You can't make this stuff up ..\\r\\n.A white ra...   \n",
       "3562   7003562  What annoys me is the way these politically co...   \n",
       "3599   7003599  You don't have a clue of what it's like to be ...   \n",
       "3602   7003602  Well personally, as a white guy I was accused ...   \n",
       "4204   7004204  Yes. \\r\\n\\r\\nUnlike women, gays, lesbians, tra...   \n",
       "4826   7004826  I'm talking about the eugenics practiced in Or...   \n",
       "5844   7005844  It's simple.\\r\\nWorking means getting up and g...   \n",
       "7722   7007722  Walter:  I'm sure you deny feeling \"white man'...   \n",
       "10059  7010059  I am a retired Army colonel. This PC, left win...   \n",
       "10831  7010831                        Yes, breathing while black.   \n",
       "12581  7012581  Forget about the Catholic component. It's very...   \n",
       "13444  7013444  I'm a moderate and don't get caught up in eith...   \n",
       "13680  7013680  Are you kidding? 99% of the chronic thieves bu...   \n",
       "14070  7014070  This country and the NFL has made more black m...   \n",
       "16036  7016036  You are wrong. Racism can be active or passive...   \n",
       "21363  7021363  \"Yeah, thats not the point: you removed statu...   \n",
       "21626  7021626  Let me\\r\\n\\r\\nFirst, the percentage of whites ...   \n",
       "22993  7022993  Eastman votes against honoring Hmong and Laoti...   \n",
       "23102  7023102  I think you're saying the being born rich is p...   \n",
       "23566  7023566  Racist out comes don't have to motivated by a ...   \n",
       "29236  7029236  Chris in Ottawa,\\r\\n\\r\\nAre you KIDDING me?\\r\\...   \n",
       "32443  7032443  A Nazi would be more inclined, ideologically, ...   \n",
       "34296  7034296  garycrum - Clearly, the country has been divid...   \n",
       "36050  7036050  And do it in the 50s when public schools for b...   \n",
       "36368  7036368  Jim, I don't think you can handle the truth ab...   \n",
       "36449  7036449  I'm having a hard time believing that you feel...   \n",
       "40972  7040972  So, Muslims are not \"our people\"? Let's say th...   \n",
       "\n",
       "                        created_date  publication_id  article_id    rating  \\\n",
       "878    2017-08-28 16:42:44.817441+00              22      370137  approved   \n",
       "1090   2017-08-30 03:35:00.945664+00             102      372110  approved   \n",
       "1329   2017-09-03 15:26:54.188281+00              13      373393  approved   \n",
       "1686   2017-10-12 02:26:36.040025+00             105      387829  approved   \n",
       "3562   2016-12-08 16:31:29.779533+00              54      155349  approved   \n",
       "3599   2017-02-25 15:16:06.873103+00              53      314382  approved   \n",
       "3602   2017-11-06 21:52:38.604522+00             102      397590  approved   \n",
       "4204   2017-05-22 18:12:29.957359+00              13      336018  approved   \n",
       "4826   2016-04-19 16:39:05.823989+00              13       60528  approved   \n",
       "5844   2017-09-10 14:49:42.415101+00             102      376135  approved   \n",
       "7722   2017-09-03 18:36:12.576890+00              13      373393  approved   \n",
       "10059  2017-11-08 16:31:48.801952+00             105      398116  approved   \n",
       "10831  2017-09-02 21:59:40.362604+00              53      373093  approved   \n",
       "12581  2016-10-14 15:55:45.727706+00              53      148503  approved   \n",
       "13444  2017-08-16 18:12:31.914734+00             102      367233  approved   \n",
       "13680  2017-08-24 22:06:27.097897+00              21      370264  approved   \n",
       "14070  2017-09-23 19:37:12.534970+00              55      381076  approved   \n",
       "16036  2017-09-03 18:35:16.925135+00              13      373393  approved   \n",
       "21363  2017-08-28 17:15:58.989283+00              22      370137  approved   \n",
       "21626  2017-01-23 17:12:19.417798+00              54      163496  approved   \n",
       "22993  2017-05-06 11:42:18.537246+00              21      332432  approved   \n",
       "23102  2017-08-16 22:47:54.024159+00              13      366931  approved   \n",
       "23566  2017-09-18 17:01:22.386060+00              13      378947  approved   \n",
       "29236  2017-07-30 15:52:16.305318+00              54      360964  approved   \n",
       "32443  2017-08-18 14:15:38.899097+00             102      367562  approved   \n",
       "34296  2016-03-17 16:26:41.527452+00              13       49679  approved   \n",
       "36050  2017-10-16 21:28:48.500213+00             102      389721  approved   \n",
       "36368  2017-06-08 22:50:12.371396+00              13      341755  approved   \n",
       "36449  2016-10-24 20:11:23.056474+00              53      148708  approved   \n",
       "40972  2016-11-30 21:20:47.109316+00              53      153390  approved   \n",
       "\n",
       "       funny  wow  sad  likes  ...  intellectual_or_learning_disability  \\\n",
       "878        0    0    0      0  ...                                    0   \n",
       "1090       0    0    0     17  ...                                    0   \n",
       "1329       0    0    0      0  ...                                    0   \n",
       "1686       0    0    0      1  ...                                    0   \n",
       "3562       2    0    0      2  ...                                    0   \n",
       "3599       0    0    0      0  ...                                    0   \n",
       "3602       0    0    0      0  ...                                    0   \n",
       "4204       1    0    0      2  ...                                    0   \n",
       "4826       0    0    0      0  ...                                    0   \n",
       "5844       1    0    1      3  ...                                    0   \n",
       "7722       1    0    0      3  ...                                    0   \n",
       "10059      0    0    0      1  ...                                    0   \n",
       "10831      0    0    0      1  ...                                    0   \n",
       "12581      0    0    0     22  ...                                    0   \n",
       "13444      1    0    0      1  ...                                    0   \n",
       "13680      0    0    0      9  ...                                    0   \n",
       "14070      0    0    1      2  ...                                    0   \n",
       "16036      1    0    0      2  ...                                    0   \n",
       "21363      0    0    0      0  ...                                    0   \n",
       "21626      0    0    0      0  ...                                    0   \n",
       "22993      2    0    1     55  ...                                    0   \n",
       "23102      1    0    0      2  ...                                    0   \n",
       "23566      0    0    0      0  ...                                    0   \n",
       "29236      0    0    0      5  ...                                    0   \n",
       "32443      0    0    1      0  ...                                    0   \n",
       "34296      0    0    0      0  ...                                    0   \n",
       "36050      0    0    0      1  ...                                    0   \n",
       "36368      0    0    0      0  ...                                    0   \n",
       "36449      0    0    0      1  ...                                    0   \n",
       "40972      0    0    0      0  ...                                    0   \n",
       "\n",
       "       psychiatric_or_mental_illness  other_disability  \\\n",
       "878                                0                 0   \n",
       "1090                               0                 0   \n",
       "1329                               0                 0   \n",
       "1686                               0                 0   \n",
       "3562                               0                 0   \n",
       "3599                               0                 0   \n",
       "3602                               0                 0   \n",
       "4204                               0                 0   \n",
       "4826                               0                 0   \n",
       "5844                               0                 0   \n",
       "7722                               0                 0   \n",
       "10059                              0                 0   \n",
       "10831                              0                 0   \n",
       "12581                              0                 0   \n",
       "13444                              0                 0   \n",
       "13680                              0                 0   \n",
       "14070                              0                 0   \n",
       "16036                              0                 0   \n",
       "21363                              0                 0   \n",
       "21626                              0                 0   \n",
       "22993                              0                 0   \n",
       "23102                              0                 0   \n",
       "23566                              0                 0   \n",
       "29236                              0                 0   \n",
       "32443                              0                 0   \n",
       "34296                              0                 0   \n",
       "36050                              0                 0   \n",
       "36368                              0                 0   \n",
       "36449                              0                 0   \n",
       "40972                              0                 0   \n",
       "\n",
       "                                      comment_text_clean  \\\n",
       "878    [oh, man, , , one, trying, , wear, , anyone, l...   \n",
       "1090   [interesting, dp, failed, tell, entire, story,...   \n",
       "1329   [, democrat, renounced, racism, , , tim, agree...   \n",
       "1686   [make, stuff, \\r\\n, white, rapper, rant, playe...   \n",
       "3562   [annoys, way, politically, correct, term, gone...   \n",
       "3599   [clue, like, , grew, among, black, , worked, b...   \n",
       "3602   [well, personally, , white, guy, accused, raci...   \n",
       "4204   [yes, , unlike, woman, , gay, , lesbian, , tra...   \n",
       "4826   [I, talking, eugenics, practiced, oregon, invo...   \n",
       "5844   [simple, , working, mean, getting, going, work...   \n",
       "7722   [walter, , I, sure, deny, feeling, , white, ma...   \n",
       "10059  [retired, army, colonel, , pc, , left, wing, j...   \n",
       "10831                        [yes, , breathing, black, ]   \n",
       "12581  [forget, catholic, component, , clear, race, i...   \n",
       "13444  [I, moderate, get, caught, either, side, polit...   \n",
       "13680  [kidding, , 99, , chronic, thief, busted, plac...   \n",
       "14070  [country, nfl, made, black, millionaire, anyth...   \n",
       "16036  [wrong, , racism, active, passive, , attention...   \n",
       "21363  [, yeah, , point, , removed, statue, mary, bab...   \n",
       "21626  [let, first, , percentage, white, indian, citi...   \n",
       "22993  [eastman, vote, honoring, hmong, laotian, vete...   \n",
       "23102  [think, saying, born, rich, probably, factor, ...   \n",
       "23566  [racist, come, motivated, since, white, superi...   \n",
       "29236  [chris, ottawa, , kidding, , mean, , kidding, ...   \n",
       "32443  [nazi, would, inclined, , ideologically, , vot...   \n",
       "34296  [garycrum, , clearly, , country, divided, time...   \n",
       "36050  [50, public, school, black, horrible, , white,...   \n",
       "36368  [jim, , think, handle, truth, role, white, rac...   \n",
       "36449  [I, hard, time, believing, feel, bad, black, p...   \n",
       "40972  [, muslim, , people, , , let, u, say, number, ...   \n",
       "\n",
       "                           comment_text_clean_detokenize  Prediction_svc  \\\n",
       "878    oh man   one trying  wear  anyone left  ask bl...               1   \n",
       "1090   interesting dp failed tell entire story  sayin...               0   \n",
       "1329   democrat renounced racism   tim agrees democra...               0   \n",
       "1686   make stuff \\r\\n white rapper rant played black...               1   \n",
       "3562   annoys way politically correct term gone full ...               1   \n",
       "3599   clue like  grew among black  worked black  pla...               1   \n",
       "3602   well personally  white guy accused racism furi...               1   \n",
       "4204   yes  unlike woman  gay  lesbian  transgenders ...               1   \n",
       "4826   I talking eugenics practiced oregon involuntar...               1   \n",
       "5844   simple  working mean getting going work  buyin...               1   \n",
       "7722   walter  I sure deny feeling  white man guilt  ...               1   \n",
       "10059  retired army colonel  pc  left wing jerk tryin...               1   \n",
       "10831                               yes  breathing black               1   \n",
       "12581  forget catholic component  clear race importan...               1   \n",
       "13444  I moderate get caught either side politics  th...               0   \n",
       "13680  kidding  99  chronic thief busted place work w...               1   \n",
       "14070  country nfl made black millionaire anything el...               1   \n",
       "16036  wrong  racism active passive  attention need i...               1   \n",
       "21363  yeah  point  removed statue mary baby jesus  c...               1   \n",
       "21626  let first  percentage white indian citizen pra...               1   \n",
       "22993  eastman vote honoring hmong laotian veteran  v...               1   \n",
       "23102  think saying born rich probably factor trump b...               1   \n",
       "23566  racist come motivated since white superiority ...               1   \n",
       "29236  chris ottawa  kidding  mean  kidding  nonchine...               1   \n",
       "32443  nazi would inclined  ideologically  vote berni...               1   \n",
       "34296  garycrum  clearly  country divided time  persp...               1   \n",
       "36050     50 public school black horrible  white get leg               1   \n",
       "36368  jim  think handle truth role white racism hist...               1   \n",
       "36449  I hard time believing feel bad black populatio...               1   \n",
       "40972  muslim  people   let u say number black violen...               1   \n",
       "\n",
       "       Prediction_probability_svc  Prediction_log  Prediction_probability_log  \\\n",
       "878                      0.604803               1                    0.604803   \n",
       "1090                     0.468953               0                    0.468953   \n",
       "1329                     0.427517               0                    0.427517   \n",
       "1686                     0.921078               1                    0.921078   \n",
       "3562                     0.812175               1                    0.812175   \n",
       "3599                     0.560420               1                    0.560420   \n",
       "3602                     0.939019               1                    0.939019   \n",
       "4204                     0.860850               1                    0.860850   \n",
       "4826                     0.796792               1                    0.796792   \n",
       "5844                     0.699297               1                    0.699297   \n",
       "7722                     0.582941               1                    0.582941   \n",
       "10059                    0.548858               1                    0.548858   \n",
       "10831                    0.718893               1                    0.718893   \n",
       "12581                    0.684445               1                    0.684445   \n",
       "13444                    0.485873               0                    0.485873   \n",
       "13680                    0.774574               1                    0.774574   \n",
       "14070                    0.653236               1                    0.653236   \n",
       "16036                    0.882464               1                    0.882464   \n",
       "21363                    0.550947               1                    0.550947   \n",
       "21626                    0.817162               1                    0.817162   \n",
       "22993                    0.551769               1                    0.551769   \n",
       "23102                    0.670123               1                    0.670123   \n",
       "23566                    0.813732               1                    0.813732   \n",
       "29236                    0.671653               1                    0.671653   \n",
       "32443                    0.700639               1                    0.700639   \n",
       "34296                    0.567251               1                    0.567251   \n",
       "36050                    0.893342               1                    0.893342   \n",
       "36368                    0.586750               1                    0.586750   \n",
       "36449                    0.543354               1                    0.543354   \n",
       "40972                    0.530568               1                    0.530568   \n",
       "\n",
       "       eli5_log  \n",
       "878           1  \n",
       "1090          1  \n",
       "1329          1  \n",
       "1686          1  \n",
       "3562          1  \n",
       "3599          1  \n",
       "3602          1  \n",
       "4204          1  \n",
       "4826          1  \n",
       "5844          1  \n",
       "7722          1  \n",
       "10059         1  \n",
       "10831         1  \n",
       "12581         1  \n",
       "13444         1  \n",
       "13680         1  \n",
       "14070         1  \n",
       "16036         1  \n",
       "21363         1  \n",
       "21626         1  \n",
       "22993         1  \n",
       "23102         1  \n",
       "23566         1  \n",
       "29236         1  \n",
       "32443         1  \n",
       "34296         1  \n",
       "36050         1  \n",
       "36368         1  \n",
       "36449         1  \n",
       "40972         1  \n",
       "\n",
       "[30 rows x 51 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[(test_df['target'] == 0) & (test_df['eli5_log'] == 1) & \n",
    "        (test_df['black']==1)].iloc[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in log model\n",
    "joblib.dump(fittedgrid.best_estimator_, 'saved_models/best_log_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=Toxic\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.509</b>, score <b>0.035</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.821\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.59%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.787\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 69.36%); opacity: 0.94\" title=\"1.153\">muslim</span><span style=\"opacity: 0.80\">  </span><span style=\"background-color: hsl(0, 100.00%, 90.35%); opacity: 0.83\" title=\"-0.221\">people</span><span style=\"opacity: 0.80\">   </span><span style=\"background-color: hsl(120, 100.00%, 90.59%); opacity: 0.83\" title=\"0.214\">let</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 97.58%); opacity: 0.80\" title=\"-0.031\">u</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.44%); opacity: 0.81\" title=\"-0.076\">say</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.65%); opacity: 0.85\" title=\"-0.390\">number</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"1.687\">black</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.79%); opacity: 0.83\" title=\"-0.274\">violence</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.61%); opacity: 0.81\" title=\"-0.123\">equal</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 66.64%); opacity: 0.95\" title=\"1.302\">muslim</span><span style=\"opacity: 0.80\">  </span><span style=\"background-color: hsl(0, 100.00%, 93.46%); opacity: 0.82\" title=\"-0.127\">become</span><span style=\"opacity: 0.80\">  </span><span style=\"background-color: hsl(0, 100.00%, 91.93%); opacity: 0.82\" title=\"-0.171\">people</span><span style=\"opacity: 0.80\">  </span><span style=\"background-color: hsl(120, 100.00%, 68.95%); opacity: 0.94\" title=\"1.175\">white</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.01%); opacity: 0.84\" title=\"-0.376\">violence</span><span style=\"opacity: 0.80\">   </span><span style=\"background-color: hsl(0, 100.00%, 92.01%); opacity: 0.82\" title=\"-0.169\">become</span><span style=\"opacity: 0.80\">  </span><span style=\"background-color: hsl(0, 100.00%, 91.77%); opacity: 0.82\" title=\"-0.176\">people</span><span style=\"opacity: 0.80\">   </span><span style=\"background-color: hsl(0, 100.00%, 93.16%); opacity: 0.82\" title=\"-0.135\">believe</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.30%); opacity: 0.81\" title=\"0.056\">spelled</span><span style=\"opacity: 0.80\">   </span><span style=\"background-color: hsl(0, 100.00%, 81.67%); opacity: 0.87\" title=\"-0.554\">habeas</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the lime TextExplainer model\n",
    "text_model_log = TextExplainer(random_state=1)\n",
    "\n",
    "#select a comment to investigate\n",
    "text = test_df.loc[40972,'comment_text_clean_detokenize']\n",
    "\n",
    "# Fit the Textexplainer using the logistic model predicted probabilites\n",
    "text_model_log.fit(text, fittedgrid_log.predict_proba)\n",
    "text_model_log.show_prediction(target_names=['Non-toxic', 'Toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#92136 lgbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No it won't . That's just wishful thinking on democrats fault .   For the 100 th time , Walker cited the cost of drug users treatment as being lost with Obamacare .  I laugh every time I hear a liberal claim republicans want to hurt people , and that's why they dumped Obamacare.\""
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['comment_text'].values[2:3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=Non-toxic\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.984</b>, score <b>-4.104</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.525\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 94.35%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.580\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 94.18%); opacity: 0.81\" title=\"0.053\">so</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 97.33%); opacity: 0.80\" title=\"-0.017\">muslims</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.08%); opacity: 0.84\" title=\"-0.165\">are</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.25%); opacity: 0.82\" title=\"-0.080\">not</span><span style=\"opacity: 0.80\"> &quot;</span><span style=\"background-color: hsl(120, 100.00%, 94.98%); opacity: 0.81\" title=\"0.043\">our</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.05%); opacity: 0.82\" title=\"0.083\">people</span><span style=\"opacity: 0.80\">&quot;? </span><span style=\"background-color: hsl(120, 100.00%, 87.93%); opacity: 0.84\" title=\"0.150\">let</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 86.32%); opacity: 0.84\" title=\"-0.179\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.61%); opacity: 0.82\" title=\"-0.089\">say</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.34%); opacity: 0.84\" title=\"-0.179\">the</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 99.09%); opacity: 0.80\" title=\"-0.004\">number</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.86%); opacity: 0.82\" title=\"-0.101\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.79%); opacity: 0.83\" title=\"-0.135\">black</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.78%); opacity: 0.83\" title=\"0.135\">violence</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.18%); opacity: 0.84\" title=\"-0.164\">equals</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.36%); opacity: 0.83\" title=\"-0.143\">that</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.10%); opacity: 0.81\" title=\"-0.030\">of</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.52%); opacity: 0.81\" title=\"0.025\">muslims</span><span style=\"opacity: 0.80\">; </span><span style=\"background-color: hsl(0, 100.00%, 92.19%); opacity: 0.82\" title=\"-0.081\">do</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.01%); opacity: 0.80\" title=\"0.020\">they</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.68%); opacity: 0.81\" title=\"-0.047\">then</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.10%); opacity: 0.81\" title=\"0.041\">become</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.28%); opacity: 0.82\" title=\"0.094\">not</span><span style=\"opacity: 0.80\"> &quot;</span><span style=\"background-color: hsl(120, 100.00%, 94.98%); opacity: 0.81\" title=\"0.043\">our</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.11%); opacity: 0.84\" title=\"0.165\">people</span><span style=\"opacity: 0.80\">? </span><span style=\"background-color: hsl(120, 100.00%, 95.16%); opacity: 0.81\" title=\"0.041\">when</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 60.00%); opacity: 1.00\" title=\"-0.832\">white</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.02%); opacity: 0.84\" title=\"-0.167\">violence</span><span style=\"opacity: 0.80\">, </span><span style=\"background-color: hsl(0, 100.00%, 93.68%); opacity: 0.81\" title=\"-0.060\">does</span><span style=\"opacity: 0.80\">; </span><span style=\"background-color: hsl(120, 100.00%, 95.17%); opacity: 0.81\" title=\"0.041\">do</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 97.01%); opacity: 0.80\" title=\"0.020\">they</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.68%); opacity: 0.81\" title=\"-0.047\">then</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.10%); opacity: 0.81\" title=\"0.041\">become</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.28%); opacity: 0.82\" title=\"0.094\">not</span><span style=\"opacity: 0.80\"> &quot;</span><span style=\"background-color: hsl(120, 100.00%, 94.98%); opacity: 0.81\" title=\"0.043\">our</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.01%); opacity: 0.82\" title=\"0.099\">people</span><span style=\"opacity: 0.80\">&quot;?\r\n",
       "</span><span style=\"background-color: hsl(0, 100.00%, 91.30%); opacity: 0.82\" title=\"-0.094\">i</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.76%); opacity: 0.82\" title=\"-0.087\">believe</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.54%); opacity: 0.83\" title=\"0.140\">it</span><span style=\"opacity: 0.80\">&#x27;</span><span style=\"background-color: hsl(0, 100.00%, 79.93%); opacity: 0.87\" title=\"-0.311\">s</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 79.06%); opacity: 0.88\" title=\"-0.330\">spelled</span><span style=\"opacity: 0.80\">: &quot;</span><span style=\"background-color: hsl(0, 100.00%, 97.85%); opacity: 0.80\" title=\"-0.013\">habeas</span><span style=\"opacity: 0.80\">&quot;</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the lime TextExplainer model\n",
    "text_model_log = TextExplainer(random_state=1)\n",
    "\n",
    "#select a comment to investigate\n",
    "text = test_df.loc[40972,'comment_text']\n",
    "\n",
    "# Fit the Textexplainer using the logistic model predicted probabilites\n",
    "text_model_log.fit(text,fittedgrid_log.predict_proba )\n",
    "text_model_log.show_prediction(target_names=['Non-toxic', 'Toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m a liberal and I don\\'t deny any of that. I despise Nathan Bedford Forrest. I despise Woodrow Wilson and his fawning over Birth of a Nation, which revitalized the KKK in the 1920s, including right here in the city Denver and the state of Colorado. I despise that Wilson waged a war to allegedly make the world \"Safe for Democracy\" while black Americans were being segregated and being terrorized by white supremacist rioters and lynch mobs at home. I despise the racist DixieCrats who threw a tantrum over Fanny Lou Hamer\\'s Mississippi Freedom Democratic Party.\\r\\n\\r\\nThe difference between me and you is that I acknowledge these to purge them from my current party. You do it to deflect from the segregationist president and his white nationalist buddies that you cape for here every day.'"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['comment_text'].values[87444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = test_df.loc[92316,'comment_text']\n",
    "text2\n",
    "text3 = text2.replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"\\\\\", '' )\n",
    "text3 = \" No, CE, I am not a homosexual.  I'm a monogamous heterosexual and have been married to my \\\n",
    "lively (was going to type 'lovely' and made a typo,  actually that 'lively' works too) wife, Sally, for 54 years. \\\n",
    "I have personal friends who ARE homosexual and have had those friendships for forty years or more.  \\\n",
    "They are absolutely wonderful people and the sexual preference doesn't play any role in our friendship. \\\n",
    "I really couldn't care less about anyone's sexual preference or sexual relationships.  \\\n",
    "I'm an 'old fashioned' guy and feel your sex life and romantic relationships are YOUR business. \\\n",
    "I do know of instances where the condemnation of family has destroyed the lives of \\\n",
    "young homosexual men and women, and it saddens me.  \\\n",
    "IMO, it's possible to support the rights and welfare of homosexual persons without being a homosexual yourself.  \\\n",
    "In fact, I think it's part of being a decent person.Hope that answers your question...Gary Cru \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'More news outlets are deleting their commentary sites; the great liberal bastion the Toronto Star and CNN dropped theirs ages ago as they could not take criticism. Other outlets increasingly  only allow comments on safe topics. Others censor or ban anyone who upsets the apple cart by talking about sacred cows or elephants in the room: such as homosexuals and aids, or black crime, or overpaid civil servants,  Canadian Jews and Israel, or anything deemed politically incorrect. Of course it is okay to bash white Anglo saxon men or Muslims. But it is not okay to point out the  striking similarities between Orthodox Jews and Muslims and how they treat their women, or how a lot of Muslim men chase Christian and Jewish girls but we cant go near their girls. And it is not okay to talk about how only lower middle class Christian boys fight and die for Canada in te middle east wars while so many others hide out in elite schools. I wish some academics would address these issues objectively.'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4 = test_df.loc[96458,'comment_text']\n",
    "text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>...</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>comment_text_clean</th>\n",
       "      <th>comment_text_clean_detokenize</th>\n",
       "      <th>Prediction_svc</th>\n",
       "      <th>Prediction_probability_svc</th>\n",
       "      <th>Prediction_log</th>\n",
       "      <th>Prediction_probability_log</th>\n",
       "      <th>eli5_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91787</th>\n",
       "      <td>7091787</td>\n",
       "      <td>set race relations back by governing while bla...</td>\n",
       "      <td>2017-01-03 00:20:51.418615+00</td>\n",
       "      <td>54</td>\n",
       "      <td>159664</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[set, race, relation, back, governing, black, ...</td>\n",
       "      <td>set race relation back governing black  wonder...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785751</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91923</th>\n",
       "      <td>7091923</td>\n",
       "      <td>I don't know the background to this case excep...</td>\n",
       "      <td>2017-08-18 18:56:37.782689+00</td>\n",
       "      <td>100</td>\n",
       "      <td>367936</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[know, background, case, except, white, extrem...</td>\n",
       "      <td>know background case except white extremist ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909707</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909707</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92740</th>\n",
       "      <td>7092740</td>\n",
       "      <td>&gt;I am not defending the DUP\\r\\nLiar ! You STAR...</td>\n",
       "      <td>2017-06-10 06:40:45.143685+00</td>\n",
       "      <td>100</td>\n",
       "      <td>342372</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[, defending, dup, liar, , started, thread, de...</td>\n",
       "      <td>defending dup liar  started thread defending d...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524313</td>\n",
       "      <td>1</td>\n",
       "      <td>0.524313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93236</th>\n",
       "      <td>7093236</td>\n",
       "      <td>We bring flowers to the dead be they school ki...</td>\n",
       "      <td>2016-06-24 20:09:07.725714+00</td>\n",
       "      <td>22</td>\n",
       "      <td>139610</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[bring, flower, dead, school, kid, , black, bi...</td>\n",
       "      <td>bring flower dead school kid  black bible stud...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514338</td>\n",
       "      <td>1</td>\n",
       "      <td>0.514338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96103</th>\n",
       "      <td>7096103</td>\n",
       "      <td>Mr. Harris is not a liberal. He is just an edu...</td>\n",
       "      <td>2017-06-08 21:25:51.347801+00</td>\n",
       "      <td>13</td>\n",
       "      <td>341755</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[mr, , harris, liberal, , educated, black, man...</td>\n",
       "      <td>mr  harris liberal  educated black man  suspec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.516916</td>\n",
       "      <td>1</td>\n",
       "      <td>0.516916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96458</th>\n",
       "      <td>7096458</td>\n",
       "      <td>More news outlets are deleting their commentar...</td>\n",
       "      <td>2017-04-27 13:48:21.733011+00</td>\n",
       "      <td>54</td>\n",
       "      <td>329864</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[news, outlet, deleting, commentary, site, , g...</td>\n",
       "      <td>news outlet deleting commentary site  great li...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.538284</td>\n",
       "      <td>1</td>\n",
       "      <td>0.538284</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97108</th>\n",
       "      <td>7097108</td>\n",
       "      <td>\"Buckwheat\" was a Black character in the Littl...</td>\n",
       "      <td>2016-10-28 20:02:04.557455+00</td>\n",
       "      <td>21</td>\n",
       "      <td>149818</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[, buckwheat, , black, character, little, rasc...</td>\n",
       "      <td>buckwheat  black character little rascal movie...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652036</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97291</th>\n",
       "      <td>7097291</td>\n",
       "      <td>There's opinions and there's lies . Shannyn pr...</td>\n",
       "      <td>2016-10-16 17:10:27.898600+00</td>\n",
       "      <td>21</td>\n",
       "      <td>148649</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[opinion, lie, , shannyn, probably, know, diff...</td>\n",
       "      <td>opinion lie  shannyn probably know difference ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746986</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98787</th>\n",
       "      <td>7098787</td>\n",
       "      <td>Why because police show up after the crime? Or...</td>\n",
       "      <td>2017-11-10 02:55:35.648452+00</td>\n",
       "      <td>102</td>\n",
       "      <td>399117</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[police, show, crime, , troll, poor, neighborh...</td>\n",
       "      <td>police show crime  troll poor neighborhood har...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575684</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103100</th>\n",
       "      <td>7103100</td>\n",
       "      <td>3/ &lt;&lt; From African-American activist Angela Da...</td>\n",
       "      <td>2017-04-28 01:32:45.850482+00</td>\n",
       "      <td>53</td>\n",
       "      <td>328923</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[3, , , , africanamerican, activist, angela, d...</td>\n",
       "      <td>3    africanamerican activist angela davis lef...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806735</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                       comment_text  \\\n",
       "91787   7091787  set race relations back by governing while bla...   \n",
       "91923   7091923  I don't know the background to this case excep...   \n",
       "92740   7092740  >I am not defending the DUP\\r\\nLiar ! You STAR...   \n",
       "93236   7093236  We bring flowers to the dead be they school ki...   \n",
       "96103   7096103  Mr. Harris is not a liberal. He is just an edu...   \n",
       "96458   7096458  More news outlets are deleting their commentar...   \n",
       "97108   7097108  \"Buckwheat\" was a Black character in the Littl...   \n",
       "97291   7097291  There's opinions and there's lies . Shannyn pr...   \n",
       "98787   7098787  Why because police show up after the crime? Or...   \n",
       "103100  7103100  3/ << From African-American activist Angela Da...   \n",
       "\n",
       "                         created_date  publication_id  article_id    rating  \\\n",
       "91787   2017-01-03 00:20:51.418615+00              54      159664  approved   \n",
       "91923   2017-08-18 18:56:37.782689+00             100      367936  approved   \n",
       "92740   2017-06-10 06:40:45.143685+00             100      342372  rejected   \n",
       "93236   2016-06-24 20:09:07.725714+00              22      139610  approved   \n",
       "96103   2017-06-08 21:25:51.347801+00              13      341755  approved   \n",
       "96458   2017-04-27 13:48:21.733011+00              54      329864  approved   \n",
       "97108   2016-10-28 20:02:04.557455+00              21      149818  approved   \n",
       "97291   2016-10-16 17:10:27.898600+00              21      148649  approved   \n",
       "98787   2017-11-10 02:55:35.648452+00             102      399117  approved   \n",
       "103100  2017-04-28 01:32:45.850482+00              53      328923  approved   \n",
       "\n",
       "        funny  wow  sad  likes  ...  intellectual_or_learning_disability  \\\n",
       "91787       0    0    0      2  ...                                    0   \n",
       "91923       0    0    0      0  ...                                    0   \n",
       "92740       0    0    0      0  ...                                    0   \n",
       "93236       0    0    0      1  ...                                    0   \n",
       "96103       0    0    0      1  ...                                    0   \n",
       "96458       0    0    0      6  ...                                    0   \n",
       "97108       0    0    0      2  ...                                    0   \n",
       "97291       0    0    0      2  ...                                    0   \n",
       "98787       0    1    0      1  ...                                    0   \n",
       "103100      0    0    0      0  ...                                    0   \n",
       "\n",
       "        psychiatric_or_mental_illness  other_disability  \\\n",
       "91787                               0                 0   \n",
       "91923                               0                 0   \n",
       "92740                               0                 0   \n",
       "93236                               0                 0   \n",
       "96103                               0                 0   \n",
       "96458                               0                 0   \n",
       "97108                               0                 0   \n",
       "97291                               0                 0   \n",
       "98787                               0                 0   \n",
       "103100                              0                 0   \n",
       "\n",
       "                                       comment_text_clean  \\\n",
       "91787   [set, race, relation, back, governing, black, ...   \n",
       "91923   [know, background, case, except, white, extrem...   \n",
       "92740   [, defending, dup, liar, , started, thread, de...   \n",
       "93236   [bring, flower, dead, school, kid, , black, bi...   \n",
       "96103   [mr, , harris, liberal, , educated, black, man...   \n",
       "96458   [news, outlet, deleting, commentary, site, , g...   \n",
       "97108   [, buckwheat, , black, character, little, rasc...   \n",
       "97291   [opinion, lie, , shannyn, probably, know, diff...   \n",
       "98787   [police, show, crime, , troll, poor, neighborh...   \n",
       "103100  [3, , , , africanamerican, activist, angela, d...   \n",
       "\n",
       "                            comment_text_clean_detokenize  Prediction_svc  \\\n",
       "91787   set race relation back governing black  wonder...               1   \n",
       "91923   know background case except white extremist ki...               1   \n",
       "92740   defending dup liar  started thread defending d...               1   \n",
       "93236   bring flower dead school kid  black bible stud...               1   \n",
       "96103   mr  harris liberal  educated black man  suspec...               1   \n",
       "96458   news outlet deleting commentary site  great li...               1   \n",
       "97108   buckwheat  black character little rascal movie...               1   \n",
       "97291   opinion lie  shannyn probably know difference ...               1   \n",
       "98787   police show crime  troll poor neighborhood har...               1   \n",
       "103100  3    africanamerican activist angela davis lef...               1   \n",
       "\n",
       "        Prediction_probability_svc  Prediction_log  \\\n",
       "91787                     0.785751               1   \n",
       "91923                     0.909707               1   \n",
       "92740                     0.524313               1   \n",
       "93236                     0.514338               1   \n",
       "96103                     0.516916               1   \n",
       "96458                     0.538284               1   \n",
       "97108                     0.652036               1   \n",
       "97291                     0.746986               1   \n",
       "98787                     0.575684               1   \n",
       "103100                    0.806735               1   \n",
       "\n",
       "        Prediction_probability_log  eli5_log  \n",
       "91787                     0.785751         1  \n",
       "91923                     0.909707         1  \n",
       "92740                     0.524313         1  \n",
       "93236                     0.514338         1  \n",
       "96103                     0.516916         1  \n",
       "96458                     0.538284         1  \n",
       "97108                     0.652036         1  \n",
       "97291                     0.746986         1  \n",
       "98787                     0.575684         1  \n",
       "103100                    0.806735         1  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[(test_df['target'] == 0) & (test_df['Prediction_log'] == 1) & (test_df['black']==1)].iloc[60:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
