{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for our work is to not only train a model to identify toxic comments, but to do so while reducing bias.\n",
    "Bias in this task can be viewed as the situation where certain identies such as 'Black', 'Muslim', 'Gay' e.t.c, begin triggering toxic classification for comments they are in, even when the comment is actually positive. This is a key issue in toxic comment classification. \n",
    "\n",
    "The goal of the [jigsaw unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) Kaggle challenge was to reduce this bias via a newly developed submetric which we have defined below.\n",
    "\n",
    "**Note: The goal of the model is simply to predict the toxicity score of a model.** \n",
    "\n",
    "The bias weighted ROC metric below is calulated by taking segmenting the the dataset into identity subgroups by using the provided identity labels and then calculating the subgroup metrics. \n",
    "\n",
    "### Metrics:\n",
    "\n",
    "In addition to accuracy we will observe the below metrics for our models\n",
    "\n",
    "#### Overall ROC-AUC:\n",
    "\n",
    "This is the standard ROC-AUC for the full evaluation set. In other words this is the area under the Reciever Operating Characteristic curve. It compares the true positive and false positive rates of a binary model.\n",
    "\n",
    "#### Subgroup ROC-AUC:\n",
    "\n",
    "Here, we restrict the data set to only the examples that mention the specific identity subgroup. A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.\n",
    "\n",
    "#### BPSN AUC:\n",
    "\n",
    "BPSN (Background Positive, Subgroup Negative) AUC: Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n",
    "\n",
    "#### BNSP AUC:\n",
    "\n",
    "BNSP (Background Negative, Subgroup Positive) AUC: Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.\n",
    "\n",
    "\n",
    "#### Generalized Mean of Bias AUCs\n",
    "To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
    "\n",
    "$M_p(m_s) = \\left(\\frac{1}{N} \\sum_{s=1}^{N} m_s^p\\right)^\\frac{1}{p}$\n",
    "\n",
    "Where:\n",
    "\n",
    "$M_p$ = the $p$th power-mean function\n",
    "\n",
    "$m_s$ = the bias metric $m$ calulated for subgroup $s$\n",
    "\n",
    "$N$ = number of identity subgroups\n",
    "\n",
    "For this competition, JigsawAI use a p value of -5 to encourage competitors to improve the model for the identity subgroups with the lowest model performance.\n",
    "\n",
    "### Final Metric\n",
    "We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score:\n",
    "\n",
    "$score = w_0 AUC_{overall} + \\sum_{a=1}^{A} w_a M_p(m_{s,a})$\n",
    "\n",
    "$A$ = number of submetrics (3)\n",
    "\n",
    "$m_{s,a}$ = bias metric for identity subgroup $s$ using submetric $a$\n",
    "\n",
    "$w_a$ = $a$ weighting for the relative importance of each submetric; all four $w$ values set to 0.25\n",
    "\n",
    "\n",
    "### Process:\n",
    "\n",
    "#### Classical ML models\n",
    "This is primarily an NLP task, our X feature matrix will be based off the text from online comments. We have defined a pre-processing pipeline in the 'preprocessing.ipynb' notebook to use for our our ML classifiers and a seperate pre-processing pipeline for the neural network models we are planning on training.\n",
    "\n",
    "From the classic ML classifer models, we intend to use the following models - our base word embedding technique will be TF-IDF: \n",
    "\n",
    "   * Logistic Regression\n",
    "   * SVM\n",
    "   * Random Forest\n",
    " \n",
    "   \n",
    "We will carry out hyperparameter optimization for each model and calculate the metrics for each.\n",
    "\n",
    "#### Neural Networks\n",
    "\n",
    "We will also train a neural network to answer this problem. We will start with a basic LSTM model which will be made of:\n",
    "    \n",
    "   * Two LSTM layers to read through the data\n",
    "   * Two Dense layers w/ 4 nodes\n",
    "   * Output layer using sigmoid for the classes\n",
    "   \n",
    "We will then seek to improve this LSTM by creating a Bidrectional LSTM (BiLSTM) which we believe will improve accuracy by reading input sequences in both directions. If time allows we will also attempt to include a simple attention mechanism.\n",
    "\n",
    "The NN models will use Glove 840B 300d word embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import contractions\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "import operator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Apply pre-processing to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train_data\n",
    "train_df = pd.read_csv('data/train_clean.csv')\n",
    "\n",
    "# Load in test data\n",
    "test_df = pd.read_csv('data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.rename({'toxicity':'target'}, axis=1, inplace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         1\n",
       "         ..\n",
       "194635    0\n",
       "194636    0\n",
       "194637    0\n",
       "194638    0\n",
       "194639    0\n",
       "Name: target, Length: 194640, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop uneeded columns as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[:,1:]\n",
    "test_df = test_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "def text_cleaner(df, col_name, clean_col_name):\n",
    "   \n",
    "\n",
    "    # Lemmatize functions to be called lated\n",
    "    # Lemmatize nouns\n",
    "    def lemmatize_text_noun(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='n') for w in text]\n",
    "    \n",
    "    # Lemmatize verbs\n",
    "    def lemmatize_text_verb(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='v') for w in text]\n",
    "    # Lemmatize adjectives\n",
    "    def lemmatize_text_adj(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='a') for w in text]\n",
    "\n",
    "    # Lemmatize adverbs\n",
    "    def lemmatize_text_adv(text):\n",
    "        return [lemmatizer.lemmatize(w, pos='r') for w in text]\n",
    "    \n",
    "    # Expand contraction method\n",
    "    def contraction_expand(text):\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    # To lower case.\n",
    "    df[clean_col_name] = df[col_name].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Expand contractions\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: contraction_expand(x))\n",
    "    \n",
    "    #Tokenize:\n",
    "    tokenizer = TweetTokenizer(reduce_len=True)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: tokenizer.tokenize(x))\n",
    "   \n",
    "    \n",
    "    #Remove Stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    #Delete punctuation\n",
    "    punc_table = str.maketrans('', '', string.punctuation)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lambda x: [item.translate(punc_table) for item in x])\n",
    "    \n",
    "    # LEMMATIZATION\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_noun)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_verb)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adj)\n",
    "    df[clean_col_name] = df[clean_col_name].apply(lemmatize_text_adv)\n",
    "    \n",
    "    \n",
    "    return None\n",
    "def detokenizer(df, col_name):\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    df[col_name+'_detokenize'] = df[col_name].apply(lambda x: detokenizer.detokenize(x))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 8s, sys: 24.1 s, total: 21min 32s\n",
      "Wall time: 25min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the cleaner func on train data\n",
    "text_cleaner(train_df, 'comment_text', 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 15s, sys: 21.8 s, total: 2min 37s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Run the same cleaner on the training data\n",
    "text_cleaner(test_df, 'comment_text', 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 35s, sys: 454 ms, total: 3min 35s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "detokenizer(train_df,'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.1 s, sys: 143 ms, total: 23.2 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# detokenize the test data\n",
    "detokenizer(test_df, 'comment_text_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          cool  like   would want mother read    really ...\n",
       "1          thank   would make life lot le anxietyinducing...\n",
       "2              urgent design problem  kudos take  impressive\n",
       "3                     something I able install site  release\n",
       "4                                       haha guy bunch loser\n",
       "                                 ...                        \n",
       "1804869    maybe tax  thing  would collect product import...\n",
       "1804870         call people still think divine role creation\n",
       "1804871                thank    right wrong    follow advice\n",
       "1804872    anyone quote follow exchange  even apocryphal ...\n",
       "1804873    student define ebd legally disable eligible sp...\n",
       "Name: comment_text_clean_detokenize, Length: 1804874, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['comment_text_clean_detokenize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         jeff session another one trump orwellian choic...\n",
       "1         actually inspect infrastructure grand chief st...\n",
       "2         wishful think democrat fault  100 th time  wal...\n",
       "3         instead wring hand nibble periphery issue  fac...\n",
       "4         many commenters garbage pile high yard  bald t...\n",
       "                                ...                        \n",
       "194635      lose job promote misinformation harmful student\n",
       "194636    thin project mean low fire danger improve wild...\n",
       "194637            hope millennials happy put airhead charge\n",
       "194638    I think kellyanne conway   k   trump whisperer...\n",
       "194639    still figure pizza ak cost pizza washington  i...\n",
       "Name: comment_text_clean_detokenize, Length: 194640, dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['comment_text_clean_detokenize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Modelling\n",
    "\n",
    "The data has been pre-processed for our models. We can now begin model training.\n",
    "\n",
    "One thing to note is that the usual train-validation split is different for this case. In order to calculate the sub-group AUC metrics, we need the identity label data that is contained in the dataframe. So, what we will do here is create a training dataframe and a validation data frame using scikit-learn's *train_test_split function*, we will then create the *X* feature matrix (which will be the comments) and the *y* target matrix (the toxicity label) from these two dataframes.\n",
    "\n",
    "After we have fitted a model and predicted results, we can then append the predictions to the dataframe and calculate the ROCs. \n",
    "\n",
    "#### Defining subgroup ROC metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def compute_subgroup_auc(df, subgroup, label, model_name):\n",
    "    subgroup_examples = df[df[subgroup]]\n",
    "    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n",
    "\n",
    "def compute_bpsn_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n",
    "    subgroup_negative_examples = df.loc[df[subgroup] & ~df[label]]\n",
    "    non_subgroup_positive_examples = df.loc[~df[subgroup] & df[label]]\n",
    "    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bnsp_auc(df, subgroup, label, model_name):\n",
    "    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n",
    "    subgroup_positive_examples = df.loc[df[subgroup] & df[label]]\n",
    "    non_subgroup_negative_examples = df.loc[~df[subgroup] & ~df[label]]\n",
    "    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n",
    "    return compute_auc(examples[label], examples[model_name])\n",
    "\n",
    "def compute_bias_metrics_for_model(dataset,\n",
    "                                   subgroups,\n",
    "                                   model,\n",
    "                                   label_col,\n",
    "                                   include_asegs=False):\n",
    "    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n",
    "    records = []\n",
    "    for subgroup in subgroups:\n",
    "        record = {\n",
    "            'subgroup': subgroup,\n",
    "            'subgroup_size': len(dataset.loc[dataset[subgroup]])\n",
    "        }\n",
    "        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n",
    "        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n",
    "        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n",
    "        records.append(record)\n",
    "    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def power_mean(series, p):\n",
    "    total = sum(np.power(series, p))\n",
    "    return np.power(total / len(series), 1 / p)\n",
    "\n",
    "def get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n",
    "    bias_score = np.average([\n",
    "        power_mean(bias_df[SUBGROUP_AUC], POWER),\n",
    "        power_mean(bias_df[BPSN_AUC], POWER),\n",
    "        power_mean(bias_df[BNSP_AUC], POWER)\n",
    "    ])\n",
    "    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (1804874,)\n",
      "y_test shape: (194640,)\n"
     ]
    }
   ],
   "source": [
    "# set the y values\n",
    "y_train = train_df['target']\n",
    "y_test = test_df['target']\n",
    "\n",
    "# we will directly choose the comment feature column in the tfid vectorizer and the models\n",
    "\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We will use Grid Search Cross validation to optimize our hyperparameters on the training set. We will then fit the best estimator and then predict on the test set and calculate relevent metrics.\n",
    "\n",
    "Because the subgroup ROCs are calculated post prediction and require predictions to be appended to a dataframe, we cannot actually pass the metrics into the scoring function of scikit-learn's gridsearchCV(). Instead we will just test the models on accuracy and total ROC-AUC and then refitting the grid-search on the model with the best ROC-AUC as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#no scaling data is already transformed\n",
    "\n",
    "# Instantiate the vectorizer, we will pass in the TweetTokenizer() from nltk \n",
    "tfid_vec_2 = TfidfVectorizer(lowercase=False, tokenizer = tweet_tokenizer.tokenize)\n",
    "\n",
    "# define pipeline\n",
    "pipeline = Pipeline([('tf-idf', tfid_vec_2), \n",
    "                     ('model', SVC())])\n",
    "\n",
    "# Define scoring functions\n",
    "scorers = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "# define parameters\n",
    "param_grid_log = [{'model': [LogisticRegression()], 'tf-idf': [tfid_vec_2], \n",
    "                   'model__penalty': ['l1', 'l2'],\n",
    "                   'model__C': [.001, 0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best cross-validated AUC score, i.e it finds the params that gives the best scores\n",
    "# then refits using the ones that give the best AUC. \n",
    "grid_log = GridSearchCV(pipeline, param_grid_log, cv=5, scoring=scorers, refit='AUC', \n",
    "                        return_train_score=True, n_jobs=-1)\n",
    "\n",
    "fittedgrid = grid_log.fit(train_df['comment_text_clean_detokenize'], train_df['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# Save best estimator to file\n",
    "joblib.dump(fittedgrid.best_estimator_, 'saved_models/best_log_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores\n",
    "train_accuracy = fittedgrid.score(train_df['comment_text_clean_detokenize'], y_train)\n",
    "test_accuracy = fittedgrid.score(test_df['comment_text_clean_detokenize'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train and test values\n",
    "y_train_pred = fittedgrid.predict(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred = fittedgrid.predict(test_df['comment_text_clean_detokenize'])\n",
    "\n",
    "y_train_pred_prob = fittedgrid.predict_proba(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred_prob = fittedgrid.predict_proba(test_df['comment_text_clean_detokenize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append preds and probabilities to train and valid dfs\n",
    "\n",
    "\n",
    "train_df['Prediction_log'] = y_train_pred\n",
    "train_df['Prediction_probability_log'] = y_train_pred_prob[:, 1]\n",
    "test_df['Prediction_log'] = y_test_pred\n",
    "test_df['Prediction_probability_log'] = y_test_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_columns = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in identity_columns + ['target']:\n",
    "    train_df[col] = np.where(train_df[col] >= 0.5, True, False)\n",
    "    test_df[col] = np.where(test_df[col] >= 0.5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7150250269924768"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the AUC metrics \n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "\n",
    "MODEL_NAME = 'Prediction_log'\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "log_bias_metrics_df_train = compute_bias_metrics_for_model(train_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "log_final_metric_train = get_final_metric(log_bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))\n",
    "\n",
    "log_bias_metrics_df_test = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "log_final_metric_test = get_final_metric(log_bias_metrics_df_test, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>21006</td>\n",
       "      <td>0.677591</td>\n",
       "      <td>0.721540</td>\n",
       "      <td>0.700603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>40423</td>\n",
       "      <td>0.678946</td>\n",
       "      <td>0.741414</td>\n",
       "      <td>0.682061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>10997</td>\n",
       "      <td>0.683245</td>\n",
       "      <td>0.701611</td>\n",
       "      <td>0.725101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>7651</td>\n",
       "      <td>0.685615</td>\n",
       "      <td>0.729084</td>\n",
       "      <td>0.699830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>14901</td>\n",
       "      <td>0.694250</td>\n",
       "      <td>0.697453</td>\n",
       "      <td>0.740054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>25082</td>\n",
       "      <td>0.697999</td>\n",
       "      <td>0.707779</td>\n",
       "      <td>0.733958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>53429</td>\n",
       "      <td>0.719074</td>\n",
       "      <td>0.735354</td>\n",
       "      <td>0.727654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>44484</td>\n",
       "      <td>0.722769</td>\n",
       "      <td>0.731714</td>\n",
       "      <td>0.734609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>4889</td>\n",
       "      <td>0.723606</td>\n",
       "      <td>0.729101</td>\n",
       "      <td>0.737448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "5                         muslim          21006      0.677591  0.721540   \n",
       "3                      christian          40423      0.678946  0.741414   \n",
       "2      homosexual_gay_or_lesbian          10997      0.683245  0.701611   \n",
       "4                         jewish           7651      0.685615  0.729084   \n",
       "6                          black          14901      0.694250  0.697453   \n",
       "7                          white          25082      0.697999  0.707779   \n",
       "1                         female          53429      0.719074  0.735354   \n",
       "0                           male          44484      0.722769  0.731714   \n",
       "8  psychiatric_or_mental_illness           4889      0.723606  0.729101   \n",
       "\n",
       "   bnsp_auc  \n",
       "5  0.700603  \n",
       "3  0.682061  \n",
       "2  0.725101  \n",
       "4  0.699830  \n",
       "6  0.740054  \n",
       "7  0.733958  \n",
       "1  0.727654  \n",
       "0  0.734609  \n",
       "8  0.737448  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_metrics_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>subgroup_size</th>\n",
       "      <th>subgroup_auc</th>\n",
       "      <th>bpsn_auc</th>\n",
       "      <th>bnsp_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1065</td>\n",
       "      <td>0.657019</td>\n",
       "      <td>0.695304</td>\n",
       "      <td>0.707064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>christian</td>\n",
       "      <td>4226</td>\n",
       "      <td>0.667762</td>\n",
       "      <td>0.742481</td>\n",
       "      <td>0.671798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2040</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.720456</td>\n",
       "      <td>0.697212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jewish</td>\n",
       "      <td>835</td>\n",
       "      <td>0.677827</td>\n",
       "      <td>0.723069</td>\n",
       "      <td>0.699662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>black</td>\n",
       "      <td>1519</td>\n",
       "      <td>0.678529</td>\n",
       "      <td>0.691751</td>\n",
       "      <td>0.731926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>white</td>\n",
       "      <td>2452</td>\n",
       "      <td>0.688773</td>\n",
       "      <td>0.705066</td>\n",
       "      <td>0.729291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>4386</td>\n",
       "      <td>0.712445</td>\n",
       "      <td>0.732588</td>\n",
       "      <td>0.725413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>5155</td>\n",
       "      <td>0.714756</td>\n",
       "      <td>0.737360</td>\n",
       "      <td>0.723030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>511</td>\n",
       "      <td>0.720370</td>\n",
       "      <td>0.721739</td>\n",
       "      <td>0.743106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subgroup  subgroup_size  subgroup_auc  bpsn_auc  \\\n",
       "2      homosexual_gay_or_lesbian           1065      0.657019  0.695304   \n",
       "3                      christian           4226      0.667762  0.742481   \n",
       "5                         muslim           2040      0.671500  0.720456   \n",
       "4                         jewish            835      0.677827  0.723069   \n",
       "6                          black           1519      0.678529  0.691751   \n",
       "7                          white           2452      0.688773  0.705066   \n",
       "0                           male           4386      0.712445  0.732588   \n",
       "1                         female           5155      0.714756  0.737360   \n",
       "8  psychiatric_or_mental_illness            511      0.720370  0.721739   \n",
       "\n",
       "   bnsp_auc  \n",
       "2  0.707064  \n",
       "3  0.671798  \n",
       "5  0.697212  \n",
       "4  0.699662  \n",
       "6  0.731926  \n",
       "7  0.729291  \n",
       "0  0.725413  \n",
       "1  0.723030  \n",
       "8  0.743106  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_metrics_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy:0.9538283994514587\n",
      "train weighted subgroup AUC:0.7197198017038468\n",
      "test_accuracy:0.9496323269141005\n",
      "test weighted subgroup AUC::0.7150250269924768\n"
     ]
    }
   ],
   "source": [
    "print(f'train_accuracy:{train_accuracy}')\n",
    "print(f'train weighted subgroup AUC:{get_final_metric(bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))}')\n",
    "print(f'test_accuracy:{test_accuracy}')\n",
    "print(f'test weighted subgroup AUC::{get_final_metric(bias_metrics_df_val, calculate_overall_auc(test_df, MODEL_NAME))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation:\n",
    "\n",
    "The logistic regression performed well on pure accuracy, with a train accuracy of 95.38% and 94.96% test accuracy. What is also positive to see is that our hyperparameter optimization has led to a model which does not overfit excessively. \n",
    "\n",
    "However when we look at the weighted subgroup AUC metric, the 71.9% train score and 71.5% test score show that the model did have a tendency towards biased predictions for certain subgroup. In comparison, the benchmark CNN that was provided had a weighted AUC score of 88.35% albeit just on a validation set \n",
    "\n",
    "\n",
    "For example we can see that for the 'black' identity BPSN AUC was relatively low, suggesting the model is likely overweighting mentions of the 'black' identity with toxicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "We will now carry out a similar process with SVM to see if this performs appreciably different to logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/capstone/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#NOW DO FOR SVC\n",
    "\n",
    "param_grid_svc = [{'model__kernel': ['rbf'],'tf-idf': [tfid_vec_2],\n",
    "              'model__C': [0.01, 0.1, 1, 10], 'model__probability': [True]}]\n",
    "\n",
    "grid_svc = GridSearchCV(pipeline, param_grid_svc, scoring=scorers, cv=5, refit='AUC',\n",
    "                       return_train_score=True, n_jobs=-1, verbose=1)\n",
    "\n",
    "fittedgrid_svc = grid_svc.fit(train_df['comment_text_clean_detokenize'], train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fittedgrid_svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-ef0dd650ec8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save best estimator to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfittedgrid_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/best_svm.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fittedgrid_svc' is not defined"
     ]
    }
   ],
   "source": [
    "# Save best estimator to file\n",
    "joblib.dump(fittedgrid_svc.best_estimator_, 'saved_models/best_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores\n",
    "train_accuracy = fittedgrid_svc.score(train_df['comment_text_clean_detokenize'], y_train)\n",
    "test_accuracy = fittedgrid_svc.score(test_df['comment_text_clean_detokenize'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train and test values\n",
    "y_train_pred = fittedgrid_svc.predict(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred = fittedgrid_svc.predict(test_df['comment_text_clean_detokenize'])\n",
    "\n",
    "y_train_pred_prob = fittedgrid_svc.predict_proba(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred_prob = fittedgrid_svc.predict_proba(test_df['comment_text_clean_detokenize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append preds and probabilities to train and valid dfs\n",
    "\n",
    "\n",
    "train_df['Prediction_svc'] = y_train_pred\n",
    "train_df['Prediction_probability_svc'] = y_train_pred_prob[:, 1]\n",
    "test_df['Prediction_svc'] = y_test_pred\n",
    "test_df['Prediction_probability_svc'] = y_test_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the AUC metrics \n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "\n",
    "MODEL_NAME = 'Prediction_svc'\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "svm_bias_metrics_df_train = compute_bias_metrics_for_model(train_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "svm_final_metric_train = get_final_metric(svm_bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))\n",
    "\n",
    "svm_bias_metrics_df_test = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "svm_final_metric_test get_final_metric(svm_bias_metrics_df_test, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "For our final model we will try the Random Forest Classifier which is an ensemble method. It is based on the Decision Tree model, only the Random Forest works by fitting on random sub samples of the data (with replacement), which is known as 'bagging'. A voting algorithm is then applied on the results of each of the trees to determine the final class of the data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# We control the model max depth and try a few different min_ leaf_counts\n",
    "param_grid_RF = {'model': [RandomForestClassifier()], 'tf-idf': [tfid_vec_2]\n",
    "              'model__max_depth': [10,15,20,30,50,100,None], 'model__min_samples_leaf': [2,5,10]\n",
    "             }\n",
    "grid_RF = GridSearchCV(pipeline, param_grid_RF, scoring=scorers, cv=5, refit='AUC',\n",
    "                       return_train_score=True, n_jobs=-1)\n",
    "\n",
    "fittedgrid_RF = grid_RF.fit(train_df['comment_text_clean_detokenize'], train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best estimator to file\n",
    "joblib.dump(fittedgrid_RF.best_estimator_, 'saved_models/best_rf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores\n",
    "train_accuracy = fittedgrid_RF.score(train_df['comment_text_clean_detokenize'], y_train)\n",
    "test_accuracy = fittedgrid_RF.score(test_df['comment_text_clean_detokenize'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train and test values\n",
    "y_train_pred = fittedgrid_RF.predict(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred = fittedgrid_RF.predict(test_df['comment_text_clean_detokenize'])\n",
    "\n",
    "y_train_pred_prob = fittedgrid_RF.predict_proba(train_df['comment_text_clean_detokenize'])\n",
    "y_test_pred_prob = fittedgrid_RF.predict_proba(test_df['comment_text_clean_detokenize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append preds and probabilities to train and valid dfs\n",
    "\n",
    "\n",
    "train_df['Prediction_RF'] = y_train_pred\n",
    "train_df['Prediction_probability_RF'] = y_train_pred_prob[:, 1]\n",
    "test_df['Prediction_RF'] = y_test_pred\n",
    "test_df['Prediction_probability_RF'] = y_test_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the AUC metrics \n",
    "\n",
    "SUBGROUP_AUC = 'subgroup_auc'\n",
    "BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n",
    "BNSP_AUC = 'bnsp_auc'\n",
    "\n",
    "MODEL_NAME = 'Prediction_svc'\n",
    "TOXICITY_COLUMN = 'target'\n",
    "\n",
    "rf_bias_metrics_df_train = compute_bias_metrics_for_model(train_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "rf_final_metric_test = get_final_metric(rf_bias_metrics_df_train, calculate_overall_auc(train_df, MODEL_NAME))\n",
    "\n",
    "rf_bias_metrics_df_test = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\n",
    "rf_final_metric_test = get_final_metric(rf_bias_metrics_df_test, calculate_overall_auc(test_df, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asses rf performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance\n",
    "\n",
    "Now that we have trained three seperate models which have been shown to deliver strong performance in text classification tasks in the past let us take the time to compare them side-by-side and also discuss their short-comings in terms of reducing bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the subgroup bias metrics tables for each model and then the final metrics for each model for test only\n",
    "display(log_bias_metrics_df_test)\n",
    "display(svm_bias_metrics_df_test)\n",
    "display(rf_bias_metrics_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final metrics for each model for test only.\n",
    "print(f' Final metric for Logistic Regression: {log_final_metric_test}')\n",
    "print(f' Final metric for SVM: {svm_final_metric_test}')\n",
    "print(f' Final metric for Random Forest: {rf_final_metric_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
